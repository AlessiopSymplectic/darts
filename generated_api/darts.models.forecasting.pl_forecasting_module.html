
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>&lt;no title&gt; &#8212; darts  documentation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="shortcut icon" href="../_static/docs-favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Facebook Prophet" href="darts.models.forecasting.prophet_model.html" />
    <link rel="prev" title="N-BEATS" href="darts.models.forecasting.nbeats.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/darts-logo-trim.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../README.html">
  Home
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../quickstart/00-quickstart.html">
  Quickstart
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../userguide.html">
  User Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="darts.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../examples.html">
  Examples
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/unit8co/darts" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/unit8co" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="darts.dataprocessing.html">
   Data Processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="darts.dataprocessing.transformers.html">
     Data Transformers
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.dataprocessing.transformers.base_data_transformer.html">
       Data Transformer Base Class
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.dataprocessing.transformers.boxcox.html">
       Box-Cox Transformer
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.dataprocessing.transformers.fittable_data_transformer.html">
       Fittable Data Transformer Base Class
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.dataprocessing.transformers.invertible_data_transformer.html">
       Invertible Data Transformer Base Class
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.dataprocessing.transformers.mappers.html">
       Mapper and InvertibleMapper
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.dataprocessing.transformers.missing_values_filler.html">
       Missing Values Filler
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.dataprocessing.transformers.scaler.html">
       Scaler
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.dataprocessing.pipeline.html">
     Pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="darts.datasets.html">
   Datasets
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="darts.metrics.html">
   Metrics
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.metrics.metrics.html">
     Metrics
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="darts.models.html">
   Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="darts.models.filtering.html">
     Filtering Models
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.filtering.gaussian_process_filter.html">
       Gaussian Processes
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.filtering.kalman_filter.html">
       Kalman Filter
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.filtering.moving_average.html">
       Moving Average
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="darts.models.forecasting.html">
     Forecasting Models
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.arima.html">
       ARIMA
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.auto_arima.html">
       Auto-ARIMA
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.baselines.html">
       Baseline Models
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.block_rnn_model.html">
       Block Recurrent Neural Networks
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.exponential_smoothing.html">
       Exponential Smoothing
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.fft.html">
       Fast Fourier Transform
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.gradient_boosted_model.html">
       LightGBM Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.kalman_forecaster.html">
       Kalman Filter Forecaster
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.linear_regression_model.html">
       Linear Regression model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.nbeats.html">
       N-BEATS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.prophet_model.html">
       Facebook Prophet
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.random_forest.html">
       Random Forest
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.regression_ensemble_model.html">
       Regression ensemble model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.regression_model.html">
       Regression Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.rnn_model.html">
       Recurrent Neural Networks
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.tbats.html">
       BATS and TBATS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.tcn_model.html">
       Temporal Convolutional Network
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.tft_model.html">
       Temporal Fusion Transformer (TFT)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.theta.html">
       Theta Method
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.transformer_model.html">
       Transformer Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.models.forecasting.varima.html">
       VARIMA
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="darts.utils.html">
   Utils
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="darts.utils.data.html">
     TimeSeries Datasets
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.utils.data.encoder_base.html">
       Encoder Base Classes
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.utils.data.encoders.html">
       Time Axes Encoders
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.utils.data.horizon_based_dataset.html">
       Horizon-Based Training Dataset
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.utils.data.inference_dataset.html">
       Inference Dataset
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.utils.data.sequential_dataset.html">
       Sequential Training Dataset
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.utils.data.shifted_dataset.html">
       Shifted Training Dataset
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="darts.utils.data.training_dataset.html">
       Training Datasets Base Classes
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.utils.likelihood_models.html">
     Likelihood Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.utils.losses.html">
     PyTorch Loss Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.utils.missing_values.html">
     Utils for filling missing values
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.utils.model_selection.html">
     Model selection utilities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.utils.statistics.html">
     Time Series Statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.utils.timeseries_generation.html">
     Utils for time series generation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.utils.torch.html">
     Utils for Pytorch and its usage
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="darts.utils.utils.html">
     Additional util functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="darts.timeseries.html">
   Timeseries
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="simple visible nav section-nav flex-column">
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <span class="target" id="module-darts.models.forecasting.pl_forecasting_module"></span><p>This file contains abstract classes for deterministic and probabilistic PyTorch Lightning Modules</p>
<dl class="py class">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">darts.models.forecasting.pl_forecasting_module.</span></span><span class="sig-name descname"><span class="pre">PLDualCovariatesModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn=MSELoss()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_cls=&lt;class</span> <span class="pre">'torch.optim.adam.Adam'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_cls=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_kwargs=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLDualCovariatesModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">darts.models.forecasting.pl_forecasting_module.PLForecastingModule</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>PyTorch Lightning-based Forecasting Module.</p>
<p>This class is meant to be inherited to create a new PyTorch Lightning-based forecasting module.
When subclassing this class, please make sure to add the following methods with the given signatures:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.__init__()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.forward()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._produce_train_output()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._get_batch_prediction()</span></code></p></li>
</ul>
</div></blockquote>
<p>In subclass <cite>MyModel</cite>’s <code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code> function call <code class="docutils literal notranslate"><span class="pre">super(MyModel,</span> <span class="pre">self).__init__(**kwargs)</span></code> where
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> are the parameters of <code class="xref py py-class docutils literal notranslate"><span class="pre">PLTorchForecastingModel</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of input past time steps per chunk.</p></li>
<li><p><strong>output_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of output time steps per chunk.</p></li>
<li><p><strong>loss_fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">_Loss</span></code>) – PyTorch loss function used for training.
This parameter will be ignored for probabilistic models if the <code class="docutils literal notranslate"><span class="pre">likelihood</span></code> parameter is specified.
Default: <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss()</span></code>.</p></li>
<li><p><strong>likelihood</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-class docutils literal notranslate"><span class="pre">Likelihood</span></code></a>]) – One of Darts’ <a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Likelihood</span></code></a> models to be used for
probabilistic forecasts. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>optimizer_cls</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The PyTorch optimizer class to be used. Default: <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam</span></code>.</p></li>
<li><p><strong>optimizer_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch optimizer (e.g., <code class="docutils literal notranslate"><span class="pre">{'lr':</span> <span class="pre">1e-3}</span></code>
for specifying a learning rate). Otherwise the default values of the selected <code class="docutils literal notranslate"><span class="pre">optimizer_cls</span></code>
will be used. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_cls</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code>]) – Optionally, the PyTorch learning rate scheduler class to be used. Specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> corresponds
to using a constant learning rate. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.automatic_optimization" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.automatic_optimization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">automatic_optimization</span></code></a></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.current_epoch" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.current_epoch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_epoch</span></code></a></p></td>
<td><p>The current epoch in the Trainer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.example_input_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">example_input_array</span></code></a></p></td>
<td><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.global_rank" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.global_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_rank</span></code></a></p></td>
<td><p>The index of the current process across all nodes and devices.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.global_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.global_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code></a></p></td>
<td><p>Total training batches seen across all epochs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams_initial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams_initial</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.local_rank" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.local_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_rank</span></code></a></p></td>
<td><p>The index of the current process within a single node.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.logger" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logger</span></code></a></p></td>
<td><p>Reference to the logger object in the Trainer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.model_size" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.model_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">model_size</span></code></a></p></td>
<td><p>Returns the model size in MegaBytes (MB)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_gpu" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_gpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_gpu</span></code></a></p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.truncated_bptt_steps" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.truncated_bptt_steps"><code class="xref py py-obj docutils literal notranslate"><span class="pre">truncated_bptt_steps</span></code></a></p></td>
<td><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 76%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>device</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>dtype</strong></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><strong>epochs_trained</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>loaded_optimizer_states_dict</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_module</span></code></a>(name, module)</p></td>
<td><p>Adds a child module to the current module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_to_queue" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_to_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_to_queue</span></code></a>(queue)</p></td>
<td><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.all_gather" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.all_gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all_gather</span></code></a>(data[, group, sync_grads])</p></td>
<td><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation accelerator agnostic.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.apply" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(fn)</p></td>
<td><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.backward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>(loss, optimizer, optimizer_idx, ...)</p></td>
<td><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.bfloat16" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bfloat16</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.buffers" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">buffers</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module buffers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.children" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.clip_gradients" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.clip_gradients"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip_gradients</span></code></a>(optimizer[, ...])</p></td>
<td><p>Handles gradient clipping internally.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_callbacks" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_callbacks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_callbacks</span></code></a>()</p></td>
<td><p>Configure model-specific callbacks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_gradient_clipping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_gradient_clipping</span></code></a>(optimizer, ...)</p></td>
<td><p>Perform gradient clipping for the optimizer parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizers</span></code></a>()</p></td>
<td><p>configures optimizers and learning rate schedulers for for model optimization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_sharded_model" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_sharded_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_sharded_model</span></code></a>()</p></td>
<td><p>Hook to create modules in a distributed aware context.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cpu" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code></a>()</p></td>
<td><p>Moves all model parameters and buffers to the CPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuda</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.double" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">double</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.eval" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval</span></code></a>()</p></td>
<td><p>Sets the module in evaluation mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.extra_repr" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.extra_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_repr</span></code></a>()</p></td>
<td><p>Set the extra representation of the module</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(*args, **kwargs)</p></td>
<td><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.freeze" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.freeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze</span></code></a>()</p></td>
<td><p>Freeze all params for inference.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_buffer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_buffer</span></code></a>(target)</p></td>
<td><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_extra_state</span></code></a>()</p></td>
<td><p>Returns any extra state to include in the module's state_dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_from_queue" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_from_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_from_queue</span></code></a>(queue)</p></td>
<td><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_parameter" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_parameter</span></code></a>(target)</p></td>
<td><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_progress_bar_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_progress_bar_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_progress_bar_dict</span></code></a>()</p></td>
<td><p><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5.</span></p>
</div>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_submodule" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_submodule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_submodule</span></code></a>(target)</p></td>
<td><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.half" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">half</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_from_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_from_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code></a>(checkpoint_path[, ...])</p></td>
<td><p>Primary way of loading a model from a checkpoint.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_state_dict</span></code></a>(state_dict[, strict])</p></td>
<td><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>(name, value[, prog_bar, logger, ...])</p></td>
<td><p>Log a key, value pair.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_dict</span></code></a>(dictionary[, prog_bar, logger, ...])</p></td>
<td><p>Log a dictionary of values at once.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log_grad_norm" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log_grad_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_grad_norm</span></code></a>(grad_norm_dict)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.lr_schedulers" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.lr_schedulers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lr_schedulers</span></code></a>()</p></td>
<td><p>Returns the learning rate scheduler(s) that are being used during training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.manual_backward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.manual_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_backward</span></code></a>(loss, *args, **kwargs)</p></td>
<td><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.modules" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules</span></code></a>()</p></td>
<td><p>Returns an iterator over all modules in the network.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_buffers" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_buffers</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_children" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_modules" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_modules</span></code></a>([memo, prefix, remove_duplicate])</p></td>
<td><p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_parameters" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_parameters</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_backward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_backward</span></code></a>()</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_backward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_backward</span></code></a>(loss)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code></a>(optimizer, ...)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_zero_grad</span></code></a>(optimizer)</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch ends.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_start</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch begins.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_fit_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_fit_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_end</span></code></a>()</p></td>
<td><p>Called at the very end of fit.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_fit_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_fit_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_start</span></code></a>()</p></td>
<td><p>Called at the very beginning of fit.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_hpc_load" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_hpc_load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_load</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager loads the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_hpc_save" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_hpc_save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_save</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager saves the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_load_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_load_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_load_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning to restore your model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_post_move_to_device" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_post_move_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_post_move_to_device</span></code></a>()</p></td>
<td><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after <code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the predict loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the predict loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the predict dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_end</span></code></a>()</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_end</span></code></a>(results)</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the predict loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_pretrain_routine_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_pretrain_routine_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_end</span></code></a>()</p></td>
<td><p>Called at the end of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_pretrain_routine_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_pretrain_routine_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_start</span></code></a>()</p></td>
<td><p>Called at the beginning of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_save_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_save_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_end</span></code></a>(outputs, batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop after the batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the test dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_end</span></code></a>()</p></td>
<td><p>Called at the end of testing.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_end</span></code></a>()</p></td>
<td><p>Called in the test loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_start</span></code></a>()</p></td>
<td><p>Called in the test loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the test loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_model_train" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the test loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_start</span></code></a>()</p></td>
<td><p>Called at the beginning of testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_end</span></code></a>(outputs, batch, batch_idx)</p></td>
<td><p>Called in the training loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_start</span></code></a>(batch, batch_idx[, unused])</p></td>
<td><p>Called in the training loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the train dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_end</span></code></a>()</p></td>
<td><p>Called at the end of training before logger experiment is closed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_end</span></code></a>()</p></td>
<td><p>Called in the training loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_start</span></code></a>()</p></td>
<td><p>Called in the training loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_start</span></code></a>()</p></td>
<td><p>Called at the beginning of training after sanity check.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_val_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the val dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the validation loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the validation loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_end</span></code></a>()</p></td>
<td><p>Called at the end of validation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_end</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_start</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the val loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_model_train" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the val loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_start" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_start</span></code></a>()</p></td>
<td><p>Called at the beginning of validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_step</span></code></a>(epoch, batch_idx, optimizer)</p></td>
<td><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_zero_grad</span></code></a>(epoch, batch_idx, ...)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizers" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizers</span></code></a>([use_pl_optimizer])</p></td>
<td><p>Returns the optimizer(s) that are being used during training.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.parameters" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parameters</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for prediction.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_step</span></code></a>(batch, batch_idx[, dataloader_idx])</p></td>
<td><p>performs the prediction step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data</span></code></a>()</p></td>
<td><p>Use this to download and prepare data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.print" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.print"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print</span></code></a>(*args, **kwargs)</p></td>
<td><p>Prints only from process 0.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_buffer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_buffer</span></code></a>(name, tensor[, persistent])</p></td>
<td><p>Adds a buffer to the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_forward_hook" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_forward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_forward_pre_hook" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_forward_pre_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward pre-hook on the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_full_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_full_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_full_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_module" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_module</span></code></a>(name, module)</p></td>
<td><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_parameter" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_parameter</span></code></a>(name, param)</p></td>
<td><p>Adds a parameter to the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.requires_grad_" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad_</span></code></a>([requires_grad])</p></td>
<td><p>Change if autograd should record operations on parameters in this module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_hyperparameters</span></code></a>(*args[, ignore, frame, ...])</p></td>
<td><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_extra_state</span></code></a>(state)</p></td>
<td><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state found within the <cite>state_dict</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_predict_parameters" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_predict_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_predict_parameters</span></code></a>(n, num_samples, ...)</p></td>
<td><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setup</span></code></a>([stage])</p></td>
<td><p>Called at the beginning of fit (train + validate), validate, test, and predict.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.share_memory" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.share_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">share_memory</span></code></a>()</p></td>
<td><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_dict</span></code></a>([destination, prefix, keep_vars])</p></td>
<td><p>Returns a dictionary containing a whole state of the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.summarize" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.summarize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">summarize</span></code></a>([mode, max_depth])</p></td>
<td><p>Summarize this LightningModule.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.tbptt_split_batch" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.tbptt_split_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tbptt_split_batch</span></code></a>(batch, split_size)</p></td>
<td><p>When using truncated backpropagation through time, each batch must be split along the time dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.teardown" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.teardown"><code class="xref py py-obj docutils literal notranslate"><span class="pre">teardown</span></code></a>([stage])</p></td>
<td><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of a test epoch with the output of all test steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(*args, **kwargs)</p></td>
<td><p>Operates on a single batch of data from the test set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code></a>(*args, **kwargs)</p></td>
<td><p>Moves and/or casts the parameters and buffers.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_empty" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_empty</span></code></a>(*, device)</p></td>
<td><p>Moves the parameters and buffers to the specified device without copying storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_onnx" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_onnx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_onnx</span></code></a>(file_path[, input_sample])</p></td>
<td><p>Saves the model in ONNX format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_torchscript" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_torchscript"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_torchscript</span></code></a>([file_path, method, ...])</p></td>
<td><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.toggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">toggle_optimizer</span></code></a>(optimizer, optimizer_idx)</p></td>
<td><p>Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>([mode])</p></td>
<td><p>Sets the module in training mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_dataloader</span></code></a>()</p></td>
<td><p>Implement one or more PyTorch DataLoaders for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the training epoch with the outputs of all training steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step</span></code></a>(train_batch, batch_idx)</p></td>
<td><p>performs the training step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.transfer_batch_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transfer_batch_to_device</span></code></a>(batch, device, ...)</p></td>
<td><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom data structure.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.type" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a>(dst_type)</p></td>
<td><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.unfreeze" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.unfreeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unfreeze</span></code></a>()</p></td>
<td><p>Unfreeze all parameters for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.untoggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">untoggle_optimizer</span></code></a>(optimizer_idx)</p></td>
<td><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">val_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the validation epoch with the outputs of all validation steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step</span></code></a>(val_batch, batch_idx)</p></td>
<td><p>performs the validation step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.xpu" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xpu</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the XPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_grad</span></code></a>([set_to_none])</p></td>
<td><p>Sets gradients of all model parameters to zero.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 55%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>__call__</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_KEY</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hyper_parameters'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_NAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_name'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_TYPE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_type'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.T_destination">
<span class="sig-name descname"><span class="pre">T_destination</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.T_destination" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of TypeVar(‘T_destination’, bound=<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>])</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<em>Module</em>) – child module to be added to the module.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_to_queue">
<span class="sig-name descname"><span class="pre">add_to_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_to_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue. To avoid issues with memory
sharing, we cast the data to numpy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue to append the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.add_to_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation
accelerator agnostic. <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> is a function provided by accelerators to gather a tensor from several
distributed processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>]) – int, float, tensor of shape (batch, …), or a (possibly nested) collection thereof.</p></li>
<li><p><strong>group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><strong>sync_grads</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, …), or if the input was a collection
the output will also be a collection with tensors of this shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.automatic_optimization">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">automatic_optimization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.automatic_optimization" title="Permalink to this definition">¶</a></dt>
<dd><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. Override this hook with your
own implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The loss tensor returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Index of the current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>torch.Tensor</em> – module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a child module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.clip_gradients">
<span class="sig-name descname"><span class="pre">clip_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.clip_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles gradient clipping internally.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Do not override this method. If you want to customize gradient clipping, consider
using <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_gradient_clipping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_gradient_clipping()</span></code></a> method.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_callbacks">
<span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_callbacks" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code>
gets called, the list returned here will be merged with the list of callbacks passed to the Trainer’s
<code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks already
present in the Trainer’s callbacks list, it will take priority and replace them. In addition, Lightning
will make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain callback methods like <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_init_start()</span></code>
will never be invoked on the new callbacks returned here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_gradient_clipping">
<span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients. By default value passed in Trainer
will be available here.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. By default value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Lightning will handle the gradient clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
            <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>configures optimizers and learning rate schedulers for for model optimization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_sharded_model">
<span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_sharded_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,
where we’d like to shard the model instantly, which is useful for extremely large models which can save
memory and initialization time.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is idempotent.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
different objects. So it should be called before constructing optimizer if the module will live on GPU
while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.current_epoch">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">current_epoch</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.current_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The current epoch in the Trainer.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.dtype">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.dtype</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.dtype" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">dtype</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.dump_patches">
<span class="sig-name descname"><span class="pre">dump_patches</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.epochs_trained">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">epochs_trained</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.epochs_trained" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.example_input_array">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">example_input_array</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.example_input_array" title="Permalink to this definition">¶</a></dt>
<dd><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.
The return type is interpreted as follows:</p>
<ul class="simple">
<li><p>Single tensor: It is assumed the model takes a single argument, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(model.example_input_array)</span></code></p></li>
<li><p>Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(*model.example_input_array)</span></code></p></li>
<li><p>Dict: The input array represents named keyword arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(**model.example_input_array)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Your model’s output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the buffer
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not a
buffer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns any extra state to include in the module’s state_dict.
Implement this and a corresponding <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
module’s <cite>state_dict()</cite>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Any extra state to store in the module’s state_dict</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_from_queue">
<span class="sig-name descname"><span class="pre">get_from_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue. To preserve consistency,
we cast back the data to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue from where to get the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.get_from_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the Parameter
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Parameter</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_progress_bar_dict">
<span class="sig-name descname"><span class="pre">get_progress_bar_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_progress_bar_dict" title="Permalink to this definition">¶</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of
<cite>pytorch_lightning.callbacks.progress.base.get_metrics</cite> and will be removed in v1.7.</p>
</div>
<p>Implement this to override the default items displayed in the progress bar.
By default it includes the average loss value, split index of BPTT (if used)
and the version of the experiment when using a logger.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Epoch 1:   4%|▎         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]
</pre></div>
</div>
<p>Here is an example how to override the defaults:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># don&#39;t show the version number</span>
    <span class="n">items</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_progress_bar_dict</span><span class="p">()</span>
    <span class="n">items</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;v_num&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">items</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary with the items to be displayed in the progress bar.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_submodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.global_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.global_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process across all nodes and devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.global_step">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_step</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.global_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Total training batches seen across all epochs.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">argparse.Namespace</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. It is mutable by the user.
For the frozen set of initial hyperparameters, use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams_initial"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams_initial</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>mutable hyperparameters dicionary</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Union[AttributeDict, dict, Namespace]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams_initial">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams_initial</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams_initial" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. These contents are read-only.
Manual updates to the saved hyperparameters can instead be performed through <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.hparams"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>immutable initial hyperparameters</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>AttributeDict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_from_checkpoint">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_from_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <cite>__init__</cite>  in the checkpoint under <cite>hyper_parameters</cite></p>
<p>Any arguments specified through *args and **kwargs will override args stored in <cite>hyper_parameters</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code>]) – Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><strong>map_location</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p></li>
<li><p><strong>hparams_file</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Optional path to a .yaml file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely won’t need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights don’t have the hyperparameters saved,
use this method to pass in a .yaml file with the hparams you’d like to use.
These will be converted into a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your model’s <cite>hparams</cite> argument is <code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code>
and .yaml file has hierarchical structure, you need to refactor your model to treat
<cite>hparams</cite> as <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
</p></li>
<li><p><strong>strict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this module’s state dict. Default: <cite>True</cite>.</p></li>
<li><p><strong>kwargs</strong> – Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.loaded_optimizer_states_dict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">loaded_optimizer_states_dict</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.loaded_optimizer_states_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.local_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">local_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.local_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process within a single node.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is as follows:</p>
<table class="colwidths-given table" id="id61">
<caption><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">*</span></code> also applies to the test loop</span><a class="headerlink" href="#id61" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>LightningModule Hook</p></th>
<th class="head"><p>on_step</p></th>
<th class="head"><p>on_epoch</p></th>
<th class="head"><p>prog_bar</p></th>
<th class="head"><p>logger</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>training_step</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>training_step_end</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>training_epoch_end</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_step*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>validation_step_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_epoch_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – key to log</p></li>
<li><p><strong>value</strong> – value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> – if True logs to the progress bar</p></li>
<li><p><strong>logger</strong> – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> – if True logs at this step. None auto-logs at the training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> – the ddp group to sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>metric_attribute</strong> – To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><strong>rank_zero_only</strong> – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log_dict">
<span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dictionary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]]]]) – key value pairs.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the progress base</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs at this step. None auto-logs for training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the ddp group sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>rank_zero_only</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log_grad_norm">
<span class="sig-name descname"><span class="pre">log_grad_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_norm_dict</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.log_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grad_norm_dict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]) – Dictionary containing current grad norm metrics</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.logger">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logger</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Reference to the logger object in the Trainer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.lr_schedulers">
<span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.lr_schedulers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.configure_optimizers"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.manual_backward">
<span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.manual_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <span class="xref std std-ref">manual optimization</span> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><strong>*args</strong> – Additional positional arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.model_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float"><span class="pre">float</span></a></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.model_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model size in MegaBytes (MB)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This property will not return correct value for Deepspeed (stage 3) and fully-sharded training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memo</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Set</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]) – a memo to store the set of modules already added to the result</p></li>
<li><p><strong>prefix</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – a prefix that will be added to the name of the module</p></li>
<li><p><strong>remove_duplicate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – whether to remove the duplicated module instances in the result
or not</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Module)</em> – Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all parameter names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_batch_transfer">
<span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_backward">
<span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_batch_transfer">
<span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_after_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_optimizer_step">
<span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>The hook is only called if gradients do not need to be accumulated.
See: <a href="#id1"><span class="problematic" id="id2">:paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`</span></a>.</p>
<p>If using native AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_zero_grad">
<span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_before_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The optimizer for which grads should be zeroed.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch ends.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_epoch_start">
<span class="sig-name descname"><span class="pre">on_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch begins.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_fit_end">
<span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_fit_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_fit_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_gpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">on_gpu</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p>
<p>Useful to set flags around the LightningModule for different CPU vs GPU behavior.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_hpc_load">
<span class="sig-name descname"><span class="pre">on_hpc_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_hpc_load" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager loads the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with variables from the checkpoint.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_hpc_save">
<span class="sig-name descname"><span class="pre">on_hpc_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_hpc_save" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager saves the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning to restore your model.
If you saved something with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_post_move_to_device">
<span class="sig-name descname"><span class="pre">on_post_move_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_post_move_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called. This is a good place to tie weights between
modules after moving them to a device. Can be used when training models with weight sharing properties on
TPU.</p>
<p>Addresses the handling of shared weights on TPU:
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks">https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The outputs of predict_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_batch_start">
<span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_dataloader">
<span class="sig-name descname"><span class="pre">on_predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the predict dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_predict_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_end">
<span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_epoch_end">
<span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_epoch_start">
<span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_model_eval">
<span class="sig-name descname"><span class="pre">on_predict_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the predict loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_start">
<span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_predict_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_pretrain_routine_end">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_pretrain_routine_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_pretrain_routine_start">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_pretrain_routine_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything
else you might want to save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of test_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_batch_start">
<span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_dataloader">
<span class="sig-name descname"><span class="pre">on_test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the test dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_end">
<span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_epoch_start">
<span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_model_eval">
<span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_model_train">
<span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_start">
<span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_test_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – The outputs of training_step_end(training_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_batch_start">
<span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_dataloader">
<span class="sig-name descname"><span class="pre">on_train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the train dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, either:</p>
<ol class="arabic simple">
<li><p>Implement <cite>training_epoch_end</cite> in the LightningModule OR</p></li>
<li><p>Cache data across steps on the attribute(s) of the <cite>LightningModule</cite> and access them in this hook</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_start">
<span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_train_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_val_dataloader">
<span class="sig-name descname"><span class="pre">on_val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the val dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_val_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of validation_step_end(validation_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_batch_start">
<span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_model_eval">
<span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_model_train">
<span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.on_validation_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_step">
<span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_tpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_native_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_lbfgs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to adjust the default way the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.
By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example
once per optimizer. This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) won’t be called during the
accumulation phase when <code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers, this indexes into that list.</p></li>
<li><p><strong>optimizer_closure</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Closure for all optimizers. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
<li><p><strong>on_tpu</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if TPU backward is required</p></li>
<li><p><strong>using_native_amp</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if using native amp</p></li>
<li><p><strong>using_lbfgs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True if the matching optimizer is <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

<span class="c1"># Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># update generator opt every step</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># update discriminator opt every 2 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
            <span class="n">optimizer_closure</span><span class="p">()</span>

    <span class="c1"># ...</span>
    <span class="c1"># add as many optimizers as you want</span>
</pre></div>
</div>
<p>Here’s another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">,</span>
    <span class="n">optimizer_closure</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># warm up lr</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_zero_grad">
<span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizer_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers this indexes into that list.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance.</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for the explanation of the above example.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizers">
<span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_pl_optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision and
profiling.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Parameter</em> – module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple prediction dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the prediction step</p>
<dl class="simple">
<dt>batch</dt><dd><p>output of Darts’ <code class="xref py py-class docutils literal notranslate"><span class="pre">InferenceDataset</span></code> - tuple of <code class="docutils literal notranslate"><span class="pre">(past_target,</span> <span class="pre">past_covariates,</span>
<span class="pre">historic_future_covariates,</span> <span class="pre">future_covariates,</span> <span class="pre">future_past_covariates,</span> <span class="pre">input_timeseries)</span></code></p>
</dd>
<dt>batch_idx</dt><dd><p>the batch index of the current batch</p>
</dd>
<dt>dataloader_idx</dt><dd><p>the dataloader index</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<a class="reference internal" href="darts.timeseries.html#darts.timeseries.TimeSeries" title="darts.timeseries.TimeSeries"><code class="xref py py-class docutils literal notranslate"><span class="pre">TimeSeries</span></code></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this to download and prepare data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <cite>setup</cite> instead)
since this is NOT called on every GPU in DDP/TPU</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> with the trainer flag is deprecated and will be removed in v1.7.0.
Please set <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> in LightningDataModule or LightningModule directly instead.</p>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.print">
<span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.print" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – The thing to print. The same as for Python’s built-in print function.</p></li>
<li><p><strong>**kwargs</strong> – The same as for Python’s built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of <code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code> and
the behavior of this function will change in future versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the module’s state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em><em> or </em><em>None</em>) – buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations
that run on buffers, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
<li><p><strong>persistent</strong> (<em>bool</em>) – whether the buffer is part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module’s forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em><em> or </em><em>None</em>) – parameter to be added to the module. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<em>bool</em>) – whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters">
<span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.save_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><strong>ignore</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><strong>frame</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">frame</span></code>]) – a frame object. Default is None</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>dict</em>) – Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_predict_parameters">
<span class="sig-name descname"><span class="pre">set_predict_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roll_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.set_predict_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.share_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>~T</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.summarize">
<span class="sig-name descname"><span class="pre">summarize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.summarize" title="Permalink to this definition">¶</a></dt>
<dd><p>Summarize this LightningModule.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>pytorch_lightning.utilities.model_summary.summarize</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Can be either <code class="docutils literal notranslate"><span class="pre">'top'</span></code> (summarize only direct submodules) or <code class="docutils literal notranslate"><span class="pre">'full'</span></code> (summarize all layers).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.4: </span>This parameter was deprecated in v1.4 in favor of <cite>max_depth</cite> and will be removed in v1.6.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The maximum depth of layer nesting that the summary will include. A value of 0 turns the
layer summary off. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSummary</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The model summary object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.tbptt_split_batch">
<span class="sig-name descname"><span class="pre">tbptt_split_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.tbptt_split_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Current batch</p></li>
<li><p><strong>split_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The size of the split</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of batch splits. Each split will be passed to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> to enable truncated
back propagation through time. The default implementation splits root level Tensors and
Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">split_size</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                  <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>
        <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Called in the training loop after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_batch_start()</span></code>
if <a href="#id3"><span class="problematic" id="id4">:paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps`</span></a> &gt; 0.
Each returned batch split is passed separately to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.teardown">
<span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.teardown" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id5"><span class="problematic" id="id6">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_epoch_end">
<span class="sig-name descname"><span class="pre">test_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># do something with the outputs of all test batches</span>
    <span class="n">all_test_preds</span> <span class="o">=</span> <span class="n">test_step_outputs</span><span class="o">.</span><span class="n">predictions</span>

    <span class="n">some_result</span> <span class="o">=</span> <span class="n">calc_all_results</span><span class="p">(</span><span class="n">all_test_preds</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">some_result</span><span class="p">)</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">final_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">test_step_out</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="c1"># do something</span>
            <span class="n">final_value</span> <span class="o">+=</span> <span class="n">test_step_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step_end">
<span class="sig-name descname"><span class="pre">test_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">test_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT test_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with test_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_results</span><span class="p">):</span>
    <span class="c1"># this out is now the full size of the batch</span>
    <span class="n">all_test_step_outs</span> <span class="o">=</span> <span class="n">output_results</span><span class="o">.</span><span class="n">out</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">all_test_step_outs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> s. In addition, this method will
only cast the floating point parameters and buffers to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.device" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ExampleModule</span><span class="p">(</span><span class="n">DeviceDtypeModuleMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">ExampleModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float16</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – The desired device of the parameters
and buffers in this module.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>]) – The path of the file the onnx model should be saved to.</p></li>
<li><p><strong>input_sample</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><strong>**kwargs</strong> – Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_torchscript">
<span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.to_torchscript" title="Permalink to this definition">¶</a></dt>
<dd><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a> set. If you would like to customize the modules that
are scripted you should override this method. In case you want to return multiple modules, we recommend
using a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><strong>method</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use TorchScript’s script or trace method. Default: ‘script’</p></li>
<li><p><strong>example_inputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input to be used to do tracing when method is set to ‘trace’.
Default: None (uses <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a>)</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments that will be passed to the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code>
documentation for supported features.</p></li>
</ul>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(),</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> 
<span class="gp">... </span>                                    <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">)</span>  
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.toggle_optimizer">
<span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.toggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes sure only the gradients of the current optimizer’s parameters are calculated in the training step
to prevent dangling gradients in multiple-optimizer setup.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.
It works with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – The optimizer to toggle.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to toggle.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>bool</em>) – whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">page</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id7"><span class="problematic" id="id8">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_epoch_end">
<span class="sig-name descname"><span class="pre">training_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the training epoch with the outputs of all training steps. Use this in case you
need to do something with all the outputs returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">train_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">train_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">train_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.
If there are multiple optimizers, it is a list containing a list of outputs for each optimizer.
If using <code class="docutils literal notranslate"><span class="pre">truncated_bptt_steps</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, each element is a list of outputs corresponding to the outputs
of each processed split batch.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this method is not overridden, this won’t be called.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="c1"># do something with all training_step outputs</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the training step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step_end">
<span class="sig-name descname"><span class="pre">training_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">training_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <cite>training_step</cite> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Anything</p>
</dd>
</dl>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># softmax uses only a portion of the batch in the denominator</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">out</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="n">gpu_0_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_1_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_n_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>

    <span class="c1"># this softmax now uses the full batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">([</span><span class="n">gpu_0_pred</span><span class="p">,</span> <span class="n">gpu_1_pred</span><span class="p">,</span> <span class="n">gpu_n_pred</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.transfer_batch_to_device">
<span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.transfer_batch_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom
data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or anything that implements <cite>.to(…)</cite></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torchtext.data.batch.Batch</span></code></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, …).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be transferred to a new device.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>) – The target device as defined in PyTorch.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.truncated_bptt_steps">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">truncated_bptt_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.truncated_bptt_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p>
<p>It represents
the number of times <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> gets called before backpropagation. If this is &gt; 0, the
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> receives an additional argument <code class="docutils literal notranslate"><span class="pre">hiddens</span></code> and is expected to return a hidden state.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<em>type</em><em> or </em><em>string</em>) – the desired type</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.untoggle_optimizer">
<span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.untoggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to untoggle.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id9"><span class="problematic" id="id10">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_epoch_end">
<span class="sig-name descname"><span class="pre">validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dataloader_output_result</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">dataloader_outs</span> <span class="o">=</span> <span class="n">dataloader_output_result</span><span class="o">.</span><span class="n">dataloader_i_outputs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the validation step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step_end">
<span class="sig-name descname"><span class="pre">validation_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">validation_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT validation_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with validation_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.xpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLDualCovariatesModule.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero. See similar function
under <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> for more context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for details.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">darts.models.forecasting.pl_forecasting_module.</span></span><span class="sig-name descname"><span class="pre">PLForecastingModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn=MSELoss()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_cls=&lt;class</span> <span class="pre">'torch.optim.adam.Adam'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_cls=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_kwargs=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">pytorch_lightning.core.lightning.LightningModule</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>PyTorch Lightning-based Forecasting Module.</p>
<p>This class is meant to be inherited to create a new PyTorch Lightning-based forecasting module.
When subclassing this class, please make sure to add the following methods with the given signatures:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.__init__()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.forward()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._produce_train_output()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._get_batch_prediction()</span></code></p></li>
</ul>
</div></blockquote>
<p>In subclass <cite>MyModel</cite>’s <code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code> function call <code class="docutils literal notranslate"><span class="pre">super(MyModel,</span> <span class="pre">self).__init__(**kwargs)</span></code> where
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> are the parameters of <code class="xref py py-class docutils literal notranslate"><span class="pre">PLTorchForecastingModel</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of input past time steps per chunk.</p></li>
<li><p><strong>output_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of output time steps per chunk.</p></li>
<li><p><strong>loss_fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">_Loss</span></code>) – PyTorch loss function used for training.
This parameter will be ignored for probabilistic models if the <code class="docutils literal notranslate"><span class="pre">likelihood</span></code> parameter is specified.
Default: <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss()</span></code>.</p></li>
<li><p><strong>likelihood</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-class docutils literal notranslate"><span class="pre">Likelihood</span></code></a>]) – One of Darts’ <a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Likelihood</span></code></a> models to be used for
probabilistic forecasts. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>optimizer_cls</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The PyTorch optimizer class to be used. Default: <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam</span></code>.</p></li>
<li><p><strong>optimizer_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch optimizer (e.g., <code class="docutils literal notranslate"><span class="pre">{'lr':</span> <span class="pre">1e-3}</span></code>
for specifying a learning rate). Otherwise the default values of the selected <code class="docutils literal notranslate"><span class="pre">optimizer_cls</span></code>
will be used. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_cls</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code>]) – Optionally, the PyTorch learning rate scheduler class to be used. Specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> corresponds
to using a constant learning rate. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.automatic_optimization" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.automatic_optimization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">automatic_optimization</span></code></a></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.current_epoch" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.current_epoch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_epoch</span></code></a></p></td>
<td><p>The current epoch in the Trainer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.example_input_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">example_input_array</span></code></a></p></td>
<td><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.global_rank" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.global_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_rank</span></code></a></p></td>
<td><p>The index of the current process across all nodes and devices.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.global_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.global_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code></a></p></td>
<td><p>Total training batches seen across all epochs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams_initial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams_initial</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.local_rank" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.local_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_rank</span></code></a></p></td>
<td><p>The index of the current process within a single node.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.logger" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logger</span></code></a></p></td>
<td><p>Reference to the logger object in the Trainer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.model_size" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.model_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">model_size</span></code></a></p></td>
<td><p>Returns the model size in MegaBytes (MB)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_gpu" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_gpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_gpu</span></code></a></p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.truncated_bptt_steps" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.truncated_bptt_steps"><code class="xref py py-obj docutils literal notranslate"><span class="pre">truncated_bptt_steps</span></code></a></p></td>
<td><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 76%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>device</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>dtype</strong></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><strong>epochs_trained</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>loaded_optimizer_states_dict</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_module</span></code></a>(name, module)</p></td>
<td><p>Adds a child module to the current module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_to_queue" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_to_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_to_queue</span></code></a>(queue)</p></td>
<td><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.all_gather" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.all_gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all_gather</span></code></a>(data[, group, sync_grads])</p></td>
<td><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation accelerator agnostic.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.apply" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(fn)</p></td>
<td><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.backward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>(loss, optimizer, optimizer_idx, ...)</p></td>
<td><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.bfloat16" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bfloat16</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.buffers" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">buffers</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module buffers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.children" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.clip_gradients" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.clip_gradients"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip_gradients</span></code></a>(optimizer[, ...])</p></td>
<td><p>Handles gradient clipping internally.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_callbacks" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_callbacks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_callbacks</span></code></a>()</p></td>
<td><p>Configure model-specific callbacks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_gradient_clipping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_gradient_clipping</span></code></a>(optimizer, ...)</p></td>
<td><p>Perform gradient clipping for the optimizer parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizers</span></code></a>()</p></td>
<td><p>configures optimizers and learning rate schedulers for for model optimization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_sharded_model" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_sharded_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_sharded_model</span></code></a>()</p></td>
<td><p>Hook to create modules in a distributed aware context.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cpu" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code></a>()</p></td>
<td><p>Moves all model parameters and buffers to the CPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuda</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.double" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">double</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.eval" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval</span></code></a>()</p></td>
<td><p>Sets the module in evaluation mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.extra_repr" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.extra_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_repr</span></code></a>()</p></td>
<td><p>Set the extra representation of the module</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(*args, **kwargs)</p></td>
<td><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.freeze" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.freeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze</span></code></a>()</p></td>
<td><p>Freeze all params for inference.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_buffer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_buffer</span></code></a>(target)</p></td>
<td><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_extra_state</span></code></a>()</p></td>
<td><p>Returns any extra state to include in the module's state_dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_from_queue" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_from_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_from_queue</span></code></a>(queue)</p></td>
<td><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_parameter" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_parameter</span></code></a>(target)</p></td>
<td><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_progress_bar_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_progress_bar_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_progress_bar_dict</span></code></a>()</p></td>
<td><p><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5.</span></p>
</div>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_submodule" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_submodule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_submodule</span></code></a>(target)</p></td>
<td><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.half" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">half</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_from_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_from_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code></a>(checkpoint_path[, ...])</p></td>
<td><p>Primary way of loading a model from a checkpoint.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_state_dict</span></code></a>(state_dict[, strict])</p></td>
<td><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>(name, value[, prog_bar, logger, ...])</p></td>
<td><p>Log a key, value pair.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_dict</span></code></a>(dictionary[, prog_bar, logger, ...])</p></td>
<td><p>Log a dictionary of values at once.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log_grad_norm" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log_grad_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_grad_norm</span></code></a>(grad_norm_dict)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.lr_schedulers" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.lr_schedulers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lr_schedulers</span></code></a>()</p></td>
<td><p>Returns the learning rate scheduler(s) that are being used during training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.manual_backward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.manual_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_backward</span></code></a>(loss, *args, **kwargs)</p></td>
<td><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.modules" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules</span></code></a>()</p></td>
<td><p>Returns an iterator over all modules in the network.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_buffers" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_buffers</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_children" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_modules" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_modules</span></code></a>([memo, prefix, remove_duplicate])</p></td>
<td><p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_parameters" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_parameters</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_backward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_backward</span></code></a>()</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_backward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_backward</span></code></a>(loss)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code></a>(optimizer, ...)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_zero_grad</span></code></a>(optimizer)</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch ends.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_start</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch begins.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_fit_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_fit_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_end</span></code></a>()</p></td>
<td><p>Called at the very end of fit.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_fit_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_fit_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_start</span></code></a>()</p></td>
<td><p>Called at the very beginning of fit.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_hpc_load" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_hpc_load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_load</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager loads the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_hpc_save" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_hpc_save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_save</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager saves the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_load_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_load_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_load_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning to restore your model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_post_move_to_device" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_post_move_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_post_move_to_device</span></code></a>()</p></td>
<td><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after <code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the predict loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the predict loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the predict dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_end</span></code></a>()</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_end</span></code></a>(results)</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the predict loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_pretrain_routine_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_pretrain_routine_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_end</span></code></a>()</p></td>
<td><p>Called at the end of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_pretrain_routine_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_pretrain_routine_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_start</span></code></a>()</p></td>
<td><p>Called at the beginning of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_save_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_save_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_end</span></code></a>(outputs, batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop after the batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the test dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_end</span></code></a>()</p></td>
<td><p>Called at the end of testing.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_end</span></code></a>()</p></td>
<td><p>Called in the test loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_start</span></code></a>()</p></td>
<td><p>Called in the test loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the test loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_model_train" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the test loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_start</span></code></a>()</p></td>
<td><p>Called at the beginning of testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_end</span></code></a>(outputs, batch, batch_idx)</p></td>
<td><p>Called in the training loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_start</span></code></a>(batch, batch_idx[, unused])</p></td>
<td><p>Called in the training loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the train dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_end</span></code></a>()</p></td>
<td><p>Called at the end of training before logger experiment is closed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_end</span></code></a>()</p></td>
<td><p>Called in the training loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_start</span></code></a>()</p></td>
<td><p>Called in the training loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_start</span></code></a>()</p></td>
<td><p>Called at the beginning of training after sanity check.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_val_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the val dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the validation loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the validation loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_end</span></code></a>()</p></td>
<td><p>Called at the end of validation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_end</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_start</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the val loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_model_train" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the val loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_start" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_start</span></code></a>()</p></td>
<td><p>Called at the beginning of validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_step</span></code></a>(epoch, batch_idx, optimizer)</p></td>
<td><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_zero_grad</span></code></a>(epoch, batch_idx, ...)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizers" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizers</span></code></a>([use_pl_optimizer])</p></td>
<td><p>Returns the optimizer(s) that are being used during training.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.parameters" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parameters</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for prediction.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_step</span></code></a>(batch, batch_idx[, dataloader_idx])</p></td>
<td><p>performs the prediction step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data</span></code></a>()</p></td>
<td><p>Use this to download and prepare data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.print" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.print"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print</span></code></a>(*args, **kwargs)</p></td>
<td><p>Prints only from process 0.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_buffer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_buffer</span></code></a>(name, tensor[, persistent])</p></td>
<td><p>Adds a buffer to the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_forward_hook" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_forward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_forward_pre_hook" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_forward_pre_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward pre-hook on the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_full_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_full_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_full_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_module" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_module</span></code></a>(name, module)</p></td>
<td><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_parameter" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_parameter</span></code></a>(name, param)</p></td>
<td><p>Adds a parameter to the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.requires_grad_" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad_</span></code></a>([requires_grad])</p></td>
<td><p>Change if autograd should record operations on parameters in this module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_hyperparameters</span></code></a>(*args[, ignore, frame, ...])</p></td>
<td><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_extra_state</span></code></a>(state)</p></td>
<td><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state found within the <cite>state_dict</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_predict_parameters" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_predict_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_predict_parameters</span></code></a>(n, num_samples, ...)</p></td>
<td><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setup</span></code></a>([stage])</p></td>
<td><p>Called at the beginning of fit (train + validate), validate, test, and predict.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.share_memory" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.share_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">share_memory</span></code></a>()</p></td>
<td><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_dict</span></code></a>([destination, prefix, keep_vars])</p></td>
<td><p>Returns a dictionary containing a whole state of the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.summarize" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.summarize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">summarize</span></code></a>([mode, max_depth])</p></td>
<td><p>Summarize this LightningModule.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.tbptt_split_batch" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.tbptt_split_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tbptt_split_batch</span></code></a>(batch, split_size)</p></td>
<td><p>When using truncated backpropagation through time, each batch must be split along the time dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.teardown" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.teardown"><code class="xref py py-obj docutils literal notranslate"><span class="pre">teardown</span></code></a>([stage])</p></td>
<td><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of a test epoch with the output of all test steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(*args, **kwargs)</p></td>
<td><p>Operates on a single batch of data from the test set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code></a>(*args, **kwargs)</p></td>
<td><p>Moves and/or casts the parameters and buffers.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_empty" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_empty</span></code></a>(*, device)</p></td>
<td><p>Moves the parameters and buffers to the specified device without copying storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_onnx" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_onnx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_onnx</span></code></a>(file_path[, input_sample])</p></td>
<td><p>Saves the model in ONNX format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_torchscript" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_torchscript"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_torchscript</span></code></a>([file_path, method, ...])</p></td>
<td><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.toggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">toggle_optimizer</span></code></a>(optimizer, optimizer_idx)</p></td>
<td><p>Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>([mode])</p></td>
<td><p>Sets the module in training mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_dataloader</span></code></a>()</p></td>
<td><p>Implement one or more PyTorch DataLoaders for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the training epoch with the outputs of all training steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step</span></code></a>(train_batch, batch_idx)</p></td>
<td><p>performs the training step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.transfer_batch_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transfer_batch_to_device</span></code></a>(batch, device, ...)</p></td>
<td><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom data structure.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.type" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a>(dst_type)</p></td>
<td><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.unfreeze" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.unfreeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unfreeze</span></code></a>()</p></td>
<td><p>Unfreeze all parameters for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.untoggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">untoggle_optimizer</span></code></a>(optimizer_idx)</p></td>
<td><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">val_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the validation epoch with the outputs of all validation steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step</span></code></a>(val_batch, batch_idx)</p></td>
<td><p>performs the validation step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.xpu" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xpu</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the XPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_grad</span></code></a>([set_to_none])</p></td>
<td><p>Sets gradients of all model parameters to zero.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 55%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>__call__</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.CHECKPOINT_HYPER_PARAMS_KEY">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_KEY</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hyper_parameters'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.CHECKPOINT_HYPER_PARAMS_KEY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.CHECKPOINT_HYPER_PARAMS_NAME">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_NAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_name'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.CHECKPOINT_HYPER_PARAMS_NAME" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.CHECKPOINT_HYPER_PARAMS_TYPE">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_TYPE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_type'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.CHECKPOINT_HYPER_PARAMS_TYPE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.T_destination">
<span class="sig-name descname"><span class="pre">T_destination</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.T_destination" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of TypeVar(‘T_destination’, bound=<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>])</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<em>Module</em>) – child module to be added to the module.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_to_queue">
<span class="sig-name descname"><span class="pre">add_to_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_to_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue. To avoid issues with memory
sharing, we cast the data to numpy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue to append the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.add_to_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation
accelerator agnostic. <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> is a function provided by accelerators to gather a tensor from several
distributed processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>]) – int, float, tensor of shape (batch, …), or a (possibly nested) collection thereof.</p></li>
<li><p><strong>group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><strong>sync_grads</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, …), or if the input was a collection
the output will also be a collection with tensors of this shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.automatic_optimization">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">automatic_optimization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.automatic_optimization" title="Permalink to this definition">¶</a></dt>
<dd><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. Override this hook with your
own implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The loss tensor returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Index of the current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>torch.Tensor</em> – module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a child module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.clip_gradients">
<span class="sig-name descname"><span class="pre">clip_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.clip_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles gradient clipping internally.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Do not override this method. If you want to customize gradient clipping, consider
using <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_gradient_clipping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_gradient_clipping()</span></code></a> method.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_callbacks">
<span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_callbacks" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code>
gets called, the list returned here will be merged with the list of callbacks passed to the Trainer’s
<code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks already
present in the Trainer’s callbacks list, it will take priority and replace them. In addition, Lightning
will make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain callback methods like <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_init_start()</span></code>
will never be invoked on the new callbacks returned here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_gradient_clipping">
<span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients. By default value passed in Trainer
will be available here.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. By default value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Lightning will handle the gradient clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
            <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>configures optimizers and learning rate schedulers for for model optimization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_sharded_model">
<span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_sharded_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,
where we’d like to shard the model instantly, which is useful for extremely large models which can save
memory and initialization time.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is idempotent.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
different objects. So it should be called before constructing optimizer if the module will live on GPU
while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.current_epoch">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">current_epoch</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.current_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The current epoch in the Trainer.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.dtype">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.dtype</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.dtype" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">dtype</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.dump_patches">
<span class="sig-name descname"><span class="pre">dump_patches</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.epochs_trained">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">epochs_trained</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.epochs_trained" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.example_input_array">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">example_input_array</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.example_input_array" title="Permalink to this definition">¶</a></dt>
<dd><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.
The return type is interpreted as follows:</p>
<ul class="simple">
<li><p>Single tensor: It is assumed the model takes a single argument, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(model.example_input_array)</span></code></p></li>
<li><p>Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(*model.example_input_array)</span></code></p></li>
<li><p>Dict: The input array represents named keyword arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(**model.example_input_array)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Your model’s output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the buffer
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not a
buffer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns any extra state to include in the module’s state_dict.
Implement this and a corresponding <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
module’s <cite>state_dict()</cite>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Any extra state to store in the module’s state_dict</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_from_queue">
<span class="sig-name descname"><span class="pre">get_from_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue. To preserve consistency,
we cast back the data to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue from where to get the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.get_from_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the Parameter
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Parameter</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_progress_bar_dict">
<span class="sig-name descname"><span class="pre">get_progress_bar_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_progress_bar_dict" title="Permalink to this definition">¶</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of
<cite>pytorch_lightning.callbacks.progress.base.get_metrics</cite> and will be removed in v1.7.</p>
</div>
<p>Implement this to override the default items displayed in the progress bar.
By default it includes the average loss value, split index of BPTT (if used)
and the version of the experiment when using a logger.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Epoch 1:   4%|▎         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]
</pre></div>
</div>
<p>Here is an example how to override the defaults:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># don&#39;t show the version number</span>
    <span class="n">items</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_progress_bar_dict</span><span class="p">()</span>
    <span class="n">items</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;v_num&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">items</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary with the items to be displayed in the progress bar.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_submodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.global_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.global_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process across all nodes and devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.global_step">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_step</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.global_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Total training batches seen across all epochs.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">argparse.Namespace</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. It is mutable by the user.
For the frozen set of initial hyperparameters, use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams_initial"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams_initial</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>mutable hyperparameters dicionary</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Union[AttributeDict, dict, Namespace]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams_initial">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams_initial</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams_initial" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. These contents are read-only.
Manual updates to the saved hyperparameters can instead be performed through <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.hparams"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>immutable initial hyperparameters</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>AttributeDict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_from_checkpoint">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_from_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <cite>__init__</cite>  in the checkpoint under <cite>hyper_parameters</cite></p>
<p>Any arguments specified through *args and **kwargs will override args stored in <cite>hyper_parameters</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code>]) – Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><strong>map_location</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p></li>
<li><p><strong>hparams_file</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Optional path to a .yaml file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely won’t need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights don’t have the hyperparameters saved,
use this method to pass in a .yaml file with the hparams you’d like to use.
These will be converted into a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your model’s <cite>hparams</cite> argument is <code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code>
and .yaml file has hierarchical structure, you need to refactor your model to treat
<cite>hparams</cite> as <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
</p></li>
<li><p><strong>strict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this module’s state dict. Default: <cite>True</cite>.</p></li>
<li><p><strong>kwargs</strong> – Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.loaded_optimizer_states_dict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">loaded_optimizer_states_dict</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.loaded_optimizer_states_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.local_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">local_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.local_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process within a single node.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is as follows:</p>
<table class="colwidths-given table" id="id62">
<caption><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">*</span></code> also applies to the test loop</span><a class="headerlink" href="#id62" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>LightningModule Hook</p></th>
<th class="head"><p>on_step</p></th>
<th class="head"><p>on_epoch</p></th>
<th class="head"><p>prog_bar</p></th>
<th class="head"><p>logger</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>training_step</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>training_step_end</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>training_epoch_end</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_step*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>validation_step_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_epoch_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – key to log</p></li>
<li><p><strong>value</strong> – value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> – if True logs to the progress bar</p></li>
<li><p><strong>logger</strong> – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> – if True logs at this step. None auto-logs at the training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> – the ddp group to sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>metric_attribute</strong> – To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><strong>rank_zero_only</strong> – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log_dict">
<span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dictionary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]]]]) – key value pairs.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the progress base</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs at this step. None auto-logs for training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the ddp group sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>rank_zero_only</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log_grad_norm">
<span class="sig-name descname"><span class="pre">log_grad_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_norm_dict</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.log_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grad_norm_dict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]) – Dictionary containing current grad norm metrics</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.logger">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logger</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Reference to the logger object in the Trainer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.lr_schedulers">
<span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.lr_schedulers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.configure_optimizers"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.manual_backward">
<span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.manual_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <span class="xref std std-ref">manual optimization</span> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><strong>*args</strong> – Additional positional arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.model_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float"><span class="pre">float</span></a></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.model_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model size in MegaBytes (MB)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This property will not return correct value for Deepspeed (stage 3) and fully-sharded training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memo</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Set</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]) – a memo to store the set of modules already added to the result</p></li>
<li><p><strong>prefix</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – a prefix that will be added to the name of the module</p></li>
<li><p><strong>remove_duplicate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – whether to remove the duplicated module instances in the result
or not</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Module)</em> – Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all parameter names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_batch_transfer">
<span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_backward">
<span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_batch_transfer">
<span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_after_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_optimizer_step">
<span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>The hook is only called if gradients do not need to be accumulated.
See: <a href="#id11"><span class="problematic" id="id12">:paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`</span></a>.</p>
<p>If using native AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_zero_grad">
<span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_before_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The optimizer for which grads should be zeroed.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch ends.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_epoch_start">
<span class="sig-name descname"><span class="pre">on_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch begins.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_fit_end">
<span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_fit_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_fit_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_gpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">on_gpu</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p>
<p>Useful to set flags around the LightningModule for different CPU vs GPU behavior.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_hpc_load">
<span class="sig-name descname"><span class="pre">on_hpc_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_hpc_load" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager loads the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with variables from the checkpoint.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_hpc_save">
<span class="sig-name descname"><span class="pre">on_hpc_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_hpc_save" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager saves the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule.on_load_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning to restore your model.
If you saved something with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_post_move_to_device">
<span class="sig-name descname"><span class="pre">on_post_move_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_post_move_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called. This is a good place to tie weights between
modules after moving them to a device. Can be used when training models with weight sharing properties on
TPU.</p>
<p>Addresses the handling of shared weights on TPU:
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks">https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The outputs of predict_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_batch_start">
<span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_dataloader">
<span class="sig-name descname"><span class="pre">on_predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the predict dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_predict_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_end">
<span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_epoch_end">
<span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_epoch_start">
<span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_model_eval">
<span class="sig-name descname"><span class="pre">on_predict_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the predict loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_start">
<span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_predict_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_pretrain_routine_end">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_pretrain_routine_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_pretrain_routine_start">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_pretrain_routine_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule.on_save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything
else you might want to save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of test_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_batch_start">
<span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_dataloader">
<span class="sig-name descname"><span class="pre">on_test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the test dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_end">
<span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_epoch_start">
<span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_model_eval">
<span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_model_train">
<span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_start">
<span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_test_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – The outputs of training_step_end(training_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_batch_start">
<span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_dataloader">
<span class="sig-name descname"><span class="pre">on_train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the train dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, either:</p>
<ol class="arabic simple">
<li><p>Implement <cite>training_epoch_end</cite> in the LightningModule OR</p></li>
<li><p>Cache data across steps on the attribute(s) of the <cite>LightningModule</cite> and access them in this hook</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_start">
<span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_train_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_val_dataloader">
<span class="sig-name descname"><span class="pre">on_val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the val dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_val_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of validation_step_end(validation_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_batch_start">
<span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_model_eval">
<span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_model_train">
<span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.on_validation_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_step">
<span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_tpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_native_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_lbfgs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to adjust the default way the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.
By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example
once per optimizer. This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) won’t be called during the
accumulation phase when <code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers, this indexes into that list.</p></li>
<li><p><strong>optimizer_closure</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Closure for all optimizers. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
<li><p><strong>on_tpu</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if TPU backward is required</p></li>
<li><p><strong>using_native_amp</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if using native amp</p></li>
<li><p><strong>using_lbfgs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True if the matching optimizer is <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

<span class="c1"># Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># update generator opt every step</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># update discriminator opt every 2 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
            <span class="n">optimizer_closure</span><span class="p">()</span>

    <span class="c1"># ...</span>
    <span class="c1"># add as many optimizers as you want</span>
</pre></div>
</div>
<p>Here’s another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">,</span>
    <span class="n">optimizer_closure</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># warm up lr</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_zero_grad">
<span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizer_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers this indexes into that list.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance.</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for the explanation of the above example.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizers">
<span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_pl_optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision and
profiling.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Parameter</em> – module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple prediction dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule.predict_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the prediction step</p>
<dl class="simple">
<dt>batch</dt><dd><p>output of Darts’ <code class="xref py py-class docutils literal notranslate"><span class="pre">InferenceDataset</span></code> - tuple of <code class="docutils literal notranslate"><span class="pre">(past_target,</span> <span class="pre">past_covariates,</span>
<span class="pre">historic_future_covariates,</span> <span class="pre">future_covariates,</span> <span class="pre">future_past_covariates,</span> <span class="pre">input_timeseries)</span></code></p>
</dd>
<dt>batch_idx</dt><dd><p>the batch index of the current batch</p>
</dd>
<dt>dataloader_idx</dt><dd><p>the dataloader index</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<a class="reference internal" href="darts.timeseries.html#darts.timeseries.TimeSeries" title="darts.timeseries.TimeSeries"><code class="xref py py-class docutils literal notranslate"><span class="pre">TimeSeries</span></code></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this to download and prepare data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <cite>setup</cite> instead)
since this is NOT called on every GPU in DDP/TPU</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> with the trainer flag is deprecated and will be removed in v1.7.0.
Please set <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> in LightningDataModule or LightningModule directly instead.</p>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.print">
<span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.print" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – The thing to print. The same as for Python’s built-in print function.</p></li>
<li><p><strong>**kwargs</strong> – The same as for Python’s built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of <code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code> and
the behavior of this function will change in future versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the module’s state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em><em> or </em><em>None</em>) – buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations
that run on buffers, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
<li><p><strong>persistent</strong> (<em>bool</em>) – whether the buffer is part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module’s forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em><em> or </em><em>None</em>) – parameter to be added to the module. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<em>bool</em>) – whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters">
<span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.save_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><strong>ignore</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><strong>frame</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">frame</span></code>]) – a frame object. Default is None</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>dict</em>) – Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_predict_parameters">
<span class="sig-name descname"><span class="pre">set_predict_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roll_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule.set_predict_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.set_predict_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.share_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>~T</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.summarize">
<span class="sig-name descname"><span class="pre">summarize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.summarize" title="Permalink to this definition">¶</a></dt>
<dd><p>Summarize this LightningModule.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>pytorch_lightning.utilities.model_summary.summarize</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Can be either <code class="docutils literal notranslate"><span class="pre">'top'</span></code> (summarize only direct submodules) or <code class="docutils literal notranslate"><span class="pre">'full'</span></code> (summarize all layers).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.4: </span>This parameter was deprecated in v1.4 in favor of <cite>max_depth</cite> and will be removed in v1.6.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The maximum depth of layer nesting that the summary will include. A value of 0 turns the
layer summary off. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSummary</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The model summary object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.tbptt_split_batch">
<span class="sig-name descname"><span class="pre">tbptt_split_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.tbptt_split_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Current batch</p></li>
<li><p><strong>split_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The size of the split</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of batch splits. Each split will be passed to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> to enable truncated
back propagation through time. The default implementation splits root level Tensors and
Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">split_size</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                  <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>
        <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Called in the training loop after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_batch_start()</span></code>
if <a href="#id13"><span class="problematic" id="id14">:paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps`</span></a> &gt; 0.
Each returned batch split is passed separately to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.teardown">
<span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.teardown" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id15"><span class="problematic" id="id16">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_epoch_end">
<span class="sig-name descname"><span class="pre">test_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># do something with the outputs of all test batches</span>
    <span class="n">all_test_preds</span> <span class="o">=</span> <span class="n">test_step_outputs</span><span class="o">.</span><span class="n">predictions</span>

    <span class="n">some_result</span> <span class="o">=</span> <span class="n">calc_all_results</span><span class="p">(</span><span class="n">all_test_preds</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">some_result</span><span class="p">)</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">final_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">test_step_out</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="c1"># do something</span>
            <span class="n">final_value</span> <span class="o">+=</span> <span class="n">test_step_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step_end">
<span class="sig-name descname"><span class="pre">test_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">test_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT test_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with test_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_results</span><span class="p">):</span>
    <span class="c1"># this out is now the full size of the batch</span>
    <span class="n">all_test_step_outs</span> <span class="o">=</span> <span class="n">output_results</span><span class="o">.</span><span class="n">out</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">all_test_step_outs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> s. In addition, this method will
only cast the floating point parameters and buffers to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.device" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ExampleModule</span><span class="p">(</span><span class="n">DeviceDtypeModuleMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">ExampleModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float16</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – The desired device of the parameters
and buffers in this module.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>]) – The path of the file the onnx model should be saved to.</p></li>
<li><p><strong>input_sample</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><strong>**kwargs</strong> – Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_torchscript">
<span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.to_torchscript" title="Permalink to this definition">¶</a></dt>
<dd><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a> set. If you would like to customize the modules that
are scripted you should override this method. In case you want to return multiple modules, we recommend
using a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><strong>method</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use TorchScript’s script or trace method. Default: ‘script’</p></li>
<li><p><strong>example_inputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input to be used to do tracing when method is set to ‘trace’.
Default: None (uses <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a>)</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments that will be passed to the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code>
documentation for supported features.</p></li>
</ul>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(),</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> 
<span class="gp">... </span>                                    <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">)</span>  
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.toggle_optimizer">
<span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.toggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes sure only the gradients of the current optimizer’s parameters are calculated in the training step
to prevent dangling gradients in multiple-optimizer setup.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.
It works with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – The optimizer to toggle.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to toggle.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>bool</em>) – whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">page</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id17"><span class="problematic" id="id18">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_epoch_end">
<span class="sig-name descname"><span class="pre">training_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the training epoch with the outputs of all training steps. Use this in case you
need to do something with all the outputs returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">train_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">train_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">train_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.
If there are multiple optimizers, it is a list containing a list of outputs for each optimizer.
If using <code class="docutils literal notranslate"><span class="pre">truncated_bptt_steps</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, each element is a list of outputs corresponding to the outputs
of each processed split batch.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this method is not overridden, this won’t be called.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="c1"># do something with all training_step outputs</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the training step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step_end">
<span class="sig-name descname"><span class="pre">training_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">training_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <cite>training_step</cite> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Anything</p>
</dd>
</dl>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># softmax uses only a portion of the batch in the denominator</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">out</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="n">gpu_0_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_1_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_n_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>

    <span class="c1"># this softmax now uses the full batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">([</span><span class="n">gpu_0_pred</span><span class="p">,</span> <span class="n">gpu_1_pred</span><span class="p">,</span> <span class="n">gpu_n_pred</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.transfer_batch_to_device">
<span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.transfer_batch_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom
data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or anything that implements <cite>.to(…)</cite></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torchtext.data.batch.Batch</span></code></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, …).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be transferred to a new device.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>) – The target device as defined in PyTorch.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.truncated_bptt_steps">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">truncated_bptt_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.truncated_bptt_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p>
<p>It represents
the number of times <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> gets called before backpropagation. If this is &gt; 0, the
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> receives an additional argument <code class="docutils literal notranslate"><span class="pre">hiddens</span></code> and is expected to return a hidden state.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<em>type</em><em> or </em><em>string</em>) – the desired type</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.untoggle_optimizer">
<span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.untoggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to untoggle.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id19"><span class="problematic" id="id20">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_epoch_end">
<span class="sig-name descname"><span class="pre">validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dataloader_output_result</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">dataloader_outs</span> <span class="o">=</span> <span class="n">dataloader_output_result</span><span class="o">.</span><span class="n">dataloader_i_outputs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLForecastingModule.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the validation step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step_end">
<span class="sig-name descname"><span class="pre">validation_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">validation_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT validation_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with validation_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.xpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLForecastingModule.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero. See similar function
under <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> for more context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for details.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">darts.models.forecasting.pl_forecasting_module.</span></span><span class="sig-name descname"><span class="pre">PLFutureCovariatesModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn=MSELoss()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_cls=&lt;class</span> <span class="pre">'torch.optim.adam.Adam'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_cls=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_kwargs=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLFutureCovariatesModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">darts.models.forecasting.pl_forecasting_module.PLForecastingModule</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>PyTorch Lightning-based Forecasting Module.</p>
<p>This class is meant to be inherited to create a new PyTorch Lightning-based forecasting module.
When subclassing this class, please make sure to add the following methods with the given signatures:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.__init__()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.forward()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._produce_train_output()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._get_batch_prediction()</span></code></p></li>
</ul>
</div></blockquote>
<p>In subclass <cite>MyModel</cite>’s <code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code> function call <code class="docutils literal notranslate"><span class="pre">super(MyModel,</span> <span class="pre">self).__init__(**kwargs)</span></code> where
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> are the parameters of <code class="xref py py-class docutils literal notranslate"><span class="pre">PLTorchForecastingModel</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of input past time steps per chunk.</p></li>
<li><p><strong>output_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of output time steps per chunk.</p></li>
<li><p><strong>loss_fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">_Loss</span></code>) – PyTorch loss function used for training.
This parameter will be ignored for probabilistic models if the <code class="docutils literal notranslate"><span class="pre">likelihood</span></code> parameter is specified.
Default: <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss()</span></code>.</p></li>
<li><p><strong>likelihood</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-class docutils literal notranslate"><span class="pre">Likelihood</span></code></a>]) – One of Darts’ <a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Likelihood</span></code></a> models to be used for
probabilistic forecasts. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>optimizer_cls</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The PyTorch optimizer class to be used. Default: <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam</span></code>.</p></li>
<li><p><strong>optimizer_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch optimizer (e.g., <code class="docutils literal notranslate"><span class="pre">{'lr':</span> <span class="pre">1e-3}</span></code>
for specifying a learning rate). Otherwise the default values of the selected <code class="docutils literal notranslate"><span class="pre">optimizer_cls</span></code>
will be used. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_cls</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code>]) – Optionally, the PyTorch learning rate scheduler class to be used. Specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> corresponds
to using a constant learning rate. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.automatic_optimization" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.automatic_optimization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">automatic_optimization</span></code></a></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.current_epoch" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.current_epoch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_epoch</span></code></a></p></td>
<td><p>The current epoch in the Trainer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.example_input_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">example_input_array</span></code></a></p></td>
<td><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.global_rank" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.global_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_rank</span></code></a></p></td>
<td><p>The index of the current process across all nodes and devices.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.global_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.global_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code></a></p></td>
<td><p>Total training batches seen across all epochs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams_initial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams_initial</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.local_rank" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.local_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_rank</span></code></a></p></td>
<td><p>The index of the current process within a single node.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.logger" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logger</span></code></a></p></td>
<td><p>Reference to the logger object in the Trainer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.model_size" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.model_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">model_size</span></code></a></p></td>
<td><p>Returns the model size in MegaBytes (MB)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_gpu" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_gpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_gpu</span></code></a></p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.truncated_bptt_steps" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.truncated_bptt_steps"><code class="xref py py-obj docutils literal notranslate"><span class="pre">truncated_bptt_steps</span></code></a></p></td>
<td><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 76%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>device</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>dtype</strong></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><strong>epochs_trained</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>loaded_optimizer_states_dict</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_module</span></code></a>(name, module)</p></td>
<td><p>Adds a child module to the current module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_to_queue" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_to_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_to_queue</span></code></a>(queue)</p></td>
<td><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.all_gather" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.all_gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all_gather</span></code></a>(data[, group, sync_grads])</p></td>
<td><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation accelerator agnostic.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.apply" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(fn)</p></td>
<td><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.backward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>(loss, optimizer, optimizer_idx, ...)</p></td>
<td><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.bfloat16" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bfloat16</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.buffers" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">buffers</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module buffers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.children" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.clip_gradients" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.clip_gradients"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip_gradients</span></code></a>(optimizer[, ...])</p></td>
<td><p>Handles gradient clipping internally.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_callbacks" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_callbacks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_callbacks</span></code></a>()</p></td>
<td><p>Configure model-specific callbacks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_gradient_clipping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_gradient_clipping</span></code></a>(optimizer, ...)</p></td>
<td><p>Perform gradient clipping for the optimizer parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizers</span></code></a>()</p></td>
<td><p>configures optimizers and learning rate schedulers for for model optimization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_sharded_model" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_sharded_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_sharded_model</span></code></a>()</p></td>
<td><p>Hook to create modules in a distributed aware context.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cpu" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code></a>()</p></td>
<td><p>Moves all model parameters and buffers to the CPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuda</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.double" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">double</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.eval" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval</span></code></a>()</p></td>
<td><p>Sets the module in evaluation mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.extra_repr" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.extra_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_repr</span></code></a>()</p></td>
<td><p>Set the extra representation of the module</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(*args, **kwargs)</p></td>
<td><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.freeze" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.freeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze</span></code></a>()</p></td>
<td><p>Freeze all params for inference.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_buffer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_buffer</span></code></a>(target)</p></td>
<td><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_extra_state</span></code></a>()</p></td>
<td><p>Returns any extra state to include in the module's state_dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_from_queue" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_from_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_from_queue</span></code></a>(queue)</p></td>
<td><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_parameter" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_parameter</span></code></a>(target)</p></td>
<td><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_progress_bar_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_progress_bar_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_progress_bar_dict</span></code></a>()</p></td>
<td><p><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5.</span></p>
</div>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_submodule" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_submodule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_submodule</span></code></a>(target)</p></td>
<td><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.half" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">half</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_from_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_from_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code></a>(checkpoint_path[, ...])</p></td>
<td><p>Primary way of loading a model from a checkpoint.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_state_dict</span></code></a>(state_dict[, strict])</p></td>
<td><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>(name, value[, prog_bar, logger, ...])</p></td>
<td><p>Log a key, value pair.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_dict</span></code></a>(dictionary[, prog_bar, logger, ...])</p></td>
<td><p>Log a dictionary of values at once.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log_grad_norm" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log_grad_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_grad_norm</span></code></a>(grad_norm_dict)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.lr_schedulers" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.lr_schedulers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lr_schedulers</span></code></a>()</p></td>
<td><p>Returns the learning rate scheduler(s) that are being used during training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.manual_backward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.manual_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_backward</span></code></a>(loss, *args, **kwargs)</p></td>
<td><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.modules" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules</span></code></a>()</p></td>
<td><p>Returns an iterator over all modules in the network.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_buffers" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_buffers</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_children" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_modules" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_modules</span></code></a>([memo, prefix, remove_duplicate])</p></td>
<td><p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_parameters" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_parameters</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_backward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_backward</span></code></a>()</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_backward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_backward</span></code></a>(loss)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code></a>(optimizer, ...)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_zero_grad</span></code></a>(optimizer)</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch ends.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_start</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch begins.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_fit_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_fit_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_end</span></code></a>()</p></td>
<td><p>Called at the very end of fit.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_fit_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_fit_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_start</span></code></a>()</p></td>
<td><p>Called at the very beginning of fit.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_hpc_load" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_hpc_load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_load</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager loads the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_hpc_save" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_hpc_save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_save</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager saves the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_load_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_load_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_load_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning to restore your model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_post_move_to_device" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_post_move_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_post_move_to_device</span></code></a>()</p></td>
<td><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after <code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the predict loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the predict loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the predict dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_end</span></code></a>()</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_end</span></code></a>(results)</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the predict loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_pretrain_routine_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_pretrain_routine_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_end</span></code></a>()</p></td>
<td><p>Called at the end of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_pretrain_routine_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_pretrain_routine_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_start</span></code></a>()</p></td>
<td><p>Called at the beginning of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_save_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_save_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_end</span></code></a>(outputs, batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop after the batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the test dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_end</span></code></a>()</p></td>
<td><p>Called at the end of testing.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_end</span></code></a>()</p></td>
<td><p>Called in the test loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_start</span></code></a>()</p></td>
<td><p>Called in the test loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the test loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_model_train" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the test loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_start</span></code></a>()</p></td>
<td><p>Called at the beginning of testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_end</span></code></a>(outputs, batch, batch_idx)</p></td>
<td><p>Called in the training loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_start</span></code></a>(batch, batch_idx[, unused])</p></td>
<td><p>Called in the training loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the train dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_end</span></code></a>()</p></td>
<td><p>Called at the end of training before logger experiment is closed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_end</span></code></a>()</p></td>
<td><p>Called in the training loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_start</span></code></a>()</p></td>
<td><p>Called in the training loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_start</span></code></a>()</p></td>
<td><p>Called at the beginning of training after sanity check.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_val_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the val dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the validation loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the validation loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_end</span></code></a>()</p></td>
<td><p>Called at the end of validation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_end</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_start</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the val loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_model_train" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the val loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_start" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_start</span></code></a>()</p></td>
<td><p>Called at the beginning of validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_step</span></code></a>(epoch, batch_idx, optimizer)</p></td>
<td><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_zero_grad</span></code></a>(epoch, batch_idx, ...)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizers" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizers</span></code></a>([use_pl_optimizer])</p></td>
<td><p>Returns the optimizer(s) that are being used during training.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.parameters" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parameters</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for prediction.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_step</span></code></a>(batch, batch_idx[, dataloader_idx])</p></td>
<td><p>performs the prediction step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data</span></code></a>()</p></td>
<td><p>Use this to download and prepare data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.print" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.print"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print</span></code></a>(*args, **kwargs)</p></td>
<td><p>Prints only from process 0.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_buffer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_buffer</span></code></a>(name, tensor[, persistent])</p></td>
<td><p>Adds a buffer to the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_forward_hook" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_forward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_forward_pre_hook" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_forward_pre_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward pre-hook on the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_full_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_full_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_full_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_module" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_module</span></code></a>(name, module)</p></td>
<td><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_parameter" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_parameter</span></code></a>(name, param)</p></td>
<td><p>Adds a parameter to the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.requires_grad_" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad_</span></code></a>([requires_grad])</p></td>
<td><p>Change if autograd should record operations on parameters in this module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_hyperparameters</span></code></a>(*args[, ignore, frame, ...])</p></td>
<td><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_extra_state</span></code></a>(state)</p></td>
<td><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state found within the <cite>state_dict</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_predict_parameters" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_predict_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_predict_parameters</span></code></a>(n, num_samples, ...)</p></td>
<td><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setup</span></code></a>([stage])</p></td>
<td><p>Called at the beginning of fit (train + validate), validate, test, and predict.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.share_memory" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.share_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">share_memory</span></code></a>()</p></td>
<td><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_dict</span></code></a>([destination, prefix, keep_vars])</p></td>
<td><p>Returns a dictionary containing a whole state of the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.summarize" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.summarize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">summarize</span></code></a>([mode, max_depth])</p></td>
<td><p>Summarize this LightningModule.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.tbptt_split_batch" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.tbptt_split_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tbptt_split_batch</span></code></a>(batch, split_size)</p></td>
<td><p>When using truncated backpropagation through time, each batch must be split along the time dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.teardown" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.teardown"><code class="xref py py-obj docutils literal notranslate"><span class="pre">teardown</span></code></a>([stage])</p></td>
<td><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of a test epoch with the output of all test steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(*args, **kwargs)</p></td>
<td><p>Operates on a single batch of data from the test set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code></a>(*args, **kwargs)</p></td>
<td><p>Moves and/or casts the parameters and buffers.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_empty" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_empty</span></code></a>(*, device)</p></td>
<td><p>Moves the parameters and buffers to the specified device without copying storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_onnx" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_onnx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_onnx</span></code></a>(file_path[, input_sample])</p></td>
<td><p>Saves the model in ONNX format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_torchscript" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_torchscript"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_torchscript</span></code></a>([file_path, method, ...])</p></td>
<td><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.toggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">toggle_optimizer</span></code></a>(optimizer, optimizer_idx)</p></td>
<td><p>Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>([mode])</p></td>
<td><p>Sets the module in training mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_dataloader</span></code></a>()</p></td>
<td><p>Implement one or more PyTorch DataLoaders for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the training epoch with the outputs of all training steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step</span></code></a>(train_batch, batch_idx)</p></td>
<td><p>performs the training step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.transfer_batch_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transfer_batch_to_device</span></code></a>(batch, device, ...)</p></td>
<td><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom data structure.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.type" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a>(dst_type)</p></td>
<td><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.unfreeze" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.unfreeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unfreeze</span></code></a>()</p></td>
<td><p>Unfreeze all parameters for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.untoggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">untoggle_optimizer</span></code></a>(optimizer_idx)</p></td>
<td><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">val_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the validation epoch with the outputs of all validation steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step</span></code></a>(val_batch, batch_idx)</p></td>
<td><p>performs the validation step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.xpu" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xpu</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the XPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_grad</span></code></a>([set_to_none])</p></td>
<td><p>Sets gradients of all model parameters to zero.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 55%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>__call__</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_KEY</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hyper_parameters'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_NAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_name'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_TYPE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_type'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.T_destination">
<span class="sig-name descname"><span class="pre">T_destination</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.T_destination" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of TypeVar(‘T_destination’, bound=<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>])</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<em>Module</em>) – child module to be added to the module.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_to_queue">
<span class="sig-name descname"><span class="pre">add_to_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_to_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue. To avoid issues with memory
sharing, we cast the data to numpy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue to append the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.add_to_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation
accelerator agnostic. <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> is a function provided by accelerators to gather a tensor from several
distributed processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>]) – int, float, tensor of shape (batch, …), or a (possibly nested) collection thereof.</p></li>
<li><p><strong>group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><strong>sync_grads</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, …), or if the input was a collection
the output will also be a collection with tensors of this shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.allow_zero_length_dataloader_with_multiple_devices">
<span class="sig-name descname"><span class="pre">allow_zero_length_dataloader_with_multiple_devices</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.allow_zero_length_dataloader_with_multiple_devices" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.automatic_optimization">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">automatic_optimization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.automatic_optimization" title="Permalink to this definition">¶</a></dt>
<dd><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. Override this hook with your
own implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The loss tensor returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Index of the current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>torch.Tensor</em> – module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a child module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.clip_gradients">
<span class="sig-name descname"><span class="pre">clip_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.clip_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles gradient clipping internally.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Do not override this method. If you want to customize gradient clipping, consider
using <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_gradient_clipping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_gradient_clipping()</span></code></a> method.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_callbacks">
<span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_callbacks" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code>
gets called, the list returned here will be merged with the list of callbacks passed to the Trainer’s
<code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks already
present in the Trainer’s callbacks list, it will take priority and replace them. In addition, Lightning
will make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain callback methods like <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_init_start()</span></code>
will never be invoked on the new callbacks returned here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_gradient_clipping">
<span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients. By default value passed in Trainer
will be available here.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. By default value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Lightning will handle the gradient clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
            <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>configures optimizers and learning rate schedulers for for model optimization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_sharded_model">
<span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_sharded_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,
where we’d like to shard the model instantly, which is useful for extremely large models which can save
memory and initialization time.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is idempotent.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
different objects. So it should be called before constructing optimizer if the module will live on GPU
while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.current_epoch">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">current_epoch</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.current_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The current epoch in the Trainer.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.dtype">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.dtype</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.dtype" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">dtype</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.dump_patches">
<span class="sig-name descname"><span class="pre">dump_patches</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.epochs_trained">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">epochs_trained</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.epochs_trained" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.example_input_array">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">example_input_array</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.example_input_array" title="Permalink to this definition">¶</a></dt>
<dd><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.
The return type is interpreted as follows:</p>
<ul class="simple">
<li><p>Single tensor: It is assumed the model takes a single argument, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(model.example_input_array)</span></code></p></li>
<li><p>Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(*model.example_input_array)</span></code></p></li>
<li><p>Dict: The input array represents named keyword arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(**model.example_input_array)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Your model’s output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the buffer
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not a
buffer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns any extra state to include in the module’s state_dict.
Implement this and a corresponding <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
module’s <cite>state_dict()</cite>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Any extra state to store in the module’s state_dict</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_from_queue">
<span class="sig-name descname"><span class="pre">get_from_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue. To preserve consistency,
we cast back the data to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue from where to get the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.get_from_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the Parameter
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Parameter</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_progress_bar_dict">
<span class="sig-name descname"><span class="pre">get_progress_bar_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_progress_bar_dict" title="Permalink to this definition">¶</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of
<cite>pytorch_lightning.callbacks.progress.base.get_metrics</cite> and will be removed in v1.7.</p>
</div>
<p>Implement this to override the default items displayed in the progress bar.
By default it includes the average loss value, split index of BPTT (if used)
and the version of the experiment when using a logger.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Epoch 1:   4%|▎         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]
</pre></div>
</div>
<p>Here is an example how to override the defaults:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># don&#39;t show the version number</span>
    <span class="n">items</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_progress_bar_dict</span><span class="p">()</span>
    <span class="n">items</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;v_num&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">items</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary with the items to be displayed in the progress bar.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_submodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.global_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.global_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process across all nodes and devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.global_step">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_step</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.global_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Total training batches seen across all epochs.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">argparse.Namespace</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. It is mutable by the user.
For the frozen set of initial hyperparameters, use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams_initial"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams_initial</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>mutable hyperparameters dicionary</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Union[AttributeDict, dict, Namespace]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams_initial">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams_initial</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams_initial" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. These contents are read-only.
Manual updates to the saved hyperparameters can instead be performed through <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.hparams"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>immutable initial hyperparameters</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>AttributeDict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_from_checkpoint">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_from_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <cite>__init__</cite>  in the checkpoint under <cite>hyper_parameters</cite></p>
<p>Any arguments specified through *args and **kwargs will override args stored in <cite>hyper_parameters</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code>]) – Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><strong>map_location</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p></li>
<li><p><strong>hparams_file</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Optional path to a .yaml file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely won’t need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights don’t have the hyperparameters saved,
use this method to pass in a .yaml file with the hparams you’d like to use.
These will be converted into a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your model’s <cite>hparams</cite> argument is <code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code>
and .yaml file has hierarchical structure, you need to refactor your model to treat
<cite>hparams</cite> as <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
</p></li>
<li><p><strong>strict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this module’s state dict. Default: <cite>True</cite>.</p></li>
<li><p><strong>kwargs</strong> – Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.loaded_optimizer_states_dict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">loaded_optimizer_states_dict</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.loaded_optimizer_states_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.local_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">local_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.local_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process within a single node.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is as follows:</p>
<table class="colwidths-given table" id="id63">
<caption><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">*</span></code> also applies to the test loop</span><a class="headerlink" href="#id63" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>LightningModule Hook</p></th>
<th class="head"><p>on_step</p></th>
<th class="head"><p>on_epoch</p></th>
<th class="head"><p>prog_bar</p></th>
<th class="head"><p>logger</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>training_step</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>training_step_end</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>training_epoch_end</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_step*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>validation_step_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_epoch_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – key to log</p></li>
<li><p><strong>value</strong> – value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> – if True logs to the progress bar</p></li>
<li><p><strong>logger</strong> – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> – if True logs at this step. None auto-logs at the training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> – the ddp group to sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>metric_attribute</strong> – To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><strong>rank_zero_only</strong> – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log_dict">
<span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dictionary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]]]]) – key value pairs.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the progress base</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs at this step. None auto-logs for training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the ddp group sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>rank_zero_only</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log_grad_norm">
<span class="sig-name descname"><span class="pre">log_grad_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_norm_dict</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.log_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grad_norm_dict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]) – Dictionary containing current grad norm metrics</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.logger">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logger</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Reference to the logger object in the Trainer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.lr_schedulers">
<span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.lr_schedulers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.configure_optimizers"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.manual_backward">
<span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.manual_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <span class="xref std std-ref">manual optimization</span> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><strong>*args</strong> – Additional positional arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.model_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float"><span class="pre">float</span></a></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.model_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model size in MegaBytes (MB)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This property will not return correct value for Deepspeed (stage 3) and fully-sharded training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memo</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Set</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]) – a memo to store the set of modules already added to the result</p></li>
<li><p><strong>prefix</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – a prefix that will be added to the name of the module</p></li>
<li><p><strong>remove_duplicate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – whether to remove the duplicated module instances in the result
or not</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Module)</em> – Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all parameter names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_batch_transfer">
<span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_backward">
<span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_batch_transfer">
<span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_after_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_optimizer_step">
<span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>The hook is only called if gradients do not need to be accumulated.
See: <a href="#id21"><span class="problematic" id="id22">:paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`</span></a>.</p>
<p>If using native AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_zero_grad">
<span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_before_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The optimizer for which grads should be zeroed.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch ends.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_epoch_start">
<span class="sig-name descname"><span class="pre">on_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch begins.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_fit_end">
<span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_fit_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_fit_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_gpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">on_gpu</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p>
<p>Useful to set flags around the LightningModule for different CPU vs GPU behavior.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_hpc_load">
<span class="sig-name descname"><span class="pre">on_hpc_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_hpc_load" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager loads the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with variables from the checkpoint.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_hpc_save">
<span class="sig-name descname"><span class="pre">on_hpc_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_hpc_save" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager saves the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning to restore your model.
If you saved something with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_post_move_to_device">
<span class="sig-name descname"><span class="pre">on_post_move_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_post_move_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called. This is a good place to tie weights between
modules after moving them to a device. Can be used when training models with weight sharing properties on
TPU.</p>
<p>Addresses the handling of shared weights on TPU:
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks">https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The outputs of predict_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_batch_start">
<span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_dataloader">
<span class="sig-name descname"><span class="pre">on_predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the predict dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_predict_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_end">
<span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_epoch_end">
<span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_epoch_start">
<span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_model_eval">
<span class="sig-name descname"><span class="pre">on_predict_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the predict loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_start">
<span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_predict_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_pretrain_routine_end">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_pretrain_routine_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_pretrain_routine_start">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_pretrain_routine_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything
else you might want to save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of test_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_batch_start">
<span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_dataloader">
<span class="sig-name descname"><span class="pre">on_test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the test dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_end">
<span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_epoch_start">
<span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_model_eval">
<span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_model_train">
<span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_start">
<span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_test_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – The outputs of training_step_end(training_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_batch_start">
<span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_dataloader">
<span class="sig-name descname"><span class="pre">on_train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the train dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, either:</p>
<ol class="arabic simple">
<li><p>Implement <cite>training_epoch_end</cite> in the LightningModule OR</p></li>
<li><p>Cache data across steps on the attribute(s) of the <cite>LightningModule</cite> and access them in this hook</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_start">
<span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_train_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_val_dataloader">
<span class="sig-name descname"><span class="pre">on_val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the val dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_val_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of validation_step_end(validation_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_batch_start">
<span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_model_eval">
<span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_model_train">
<span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.on_validation_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_step">
<span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_tpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_native_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_lbfgs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to adjust the default way the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.
By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example
once per optimizer. This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) won’t be called during the
accumulation phase when <code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers, this indexes into that list.</p></li>
<li><p><strong>optimizer_closure</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Closure for all optimizers. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
<li><p><strong>on_tpu</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if TPU backward is required</p></li>
<li><p><strong>using_native_amp</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if using native amp</p></li>
<li><p><strong>using_lbfgs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True if the matching optimizer is <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

<span class="c1"># Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># update generator opt every step</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># update discriminator opt every 2 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
            <span class="n">optimizer_closure</span><span class="p">()</span>

    <span class="c1"># ...</span>
    <span class="c1"># add as many optimizers as you want</span>
</pre></div>
</div>
<p>Here’s another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">,</span>
    <span class="n">optimizer_closure</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># warm up lr</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_zero_grad">
<span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizer_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers this indexes into that list.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance.</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for the explanation of the above example.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizers">
<span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_pl_optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision and
profiling.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Parameter</em> – module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.precision" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_batch_size">
<span class="sig-name descname"><span class="pre">pred_batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_n">
<span class="sig-name descname"><span class="pre">pred_n</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_n" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_n_jobs">
<span class="sig-name descname"><span class="pre">pred_n_jobs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_n_jobs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_num_samples">
<span class="sig-name descname"><span class="pre">pred_num_samples</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_num_samples" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_roll_size">
<span class="sig-name descname"><span class="pre">pred_roll_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.pred_roll_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple prediction dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the prediction step</p>
<dl class="simple">
<dt>batch</dt><dd><p>output of Darts’ <code class="xref py py-class docutils literal notranslate"><span class="pre">InferenceDataset</span></code> - tuple of <code class="docutils literal notranslate"><span class="pre">(past_target,</span> <span class="pre">past_covariates,</span>
<span class="pre">historic_future_covariates,</span> <span class="pre">future_covariates,</span> <span class="pre">future_past_covariates,</span> <span class="pre">input_timeseries)</span></code></p>
</dd>
<dt>batch_idx</dt><dd><p>the batch index of the current batch</p>
</dd>
<dt>dataloader_idx</dt><dd><p>the dataloader index</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<a class="reference internal" href="darts.timeseries.html#darts.timeseries.TimeSeries" title="darts.timeseries.TimeSeries"><code class="xref py py-class docutils literal notranslate"><span class="pre">TimeSeries</span></code></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this to download and prepare data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <cite>setup</cite> instead)
since this is NOT called on every GPU in DDP/TPU</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> with the trainer flag is deprecated and will be removed in v1.7.0.
Please set <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> in LightningDataModule or LightningModule directly instead.</p>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data_per_node">
<span class="sig-name descname"><span class="pre">prepare_data_per_node</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data_per_node" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.print">
<span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.print" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – The thing to print. The same as for Python’s built-in print function.</p></li>
<li><p><strong>**kwargs</strong> – The same as for Python’s built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of <code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code> and
the behavior of this function will change in future versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the module’s state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em><em> or </em><em>None</em>) – buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations
that run on buffers, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
<li><p><strong>persistent</strong> (<em>bool</em>) – whether the buffer is part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module’s forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em><em> or </em><em>None</em>) – parameter to be added to the module. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<em>bool</em>) – whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters">
<span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.save_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><strong>ignore</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><strong>frame</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">frame</span></code>]) – a frame object. Default is None</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>dict</em>) – Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_predict_parameters">
<span class="sig-name descname"><span class="pre">set_predict_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roll_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.set_predict_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.share_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>~T</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.summarize">
<span class="sig-name descname"><span class="pre">summarize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.summarize" title="Permalink to this definition">¶</a></dt>
<dd><p>Summarize this LightningModule.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>pytorch_lightning.utilities.model_summary.summarize</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Can be either <code class="docutils literal notranslate"><span class="pre">'top'</span></code> (summarize only direct submodules) or <code class="docutils literal notranslate"><span class="pre">'full'</span></code> (summarize all layers).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.4: </span>This parameter was deprecated in v1.4 in favor of <cite>max_depth</cite> and will be removed in v1.6.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The maximum depth of layer nesting that the summary will include. A value of 0 turns the
layer summary off. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSummary</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The model summary object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.tbptt_split_batch">
<span class="sig-name descname"><span class="pre">tbptt_split_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.tbptt_split_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Current batch</p></li>
<li><p><strong>split_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The size of the split</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of batch splits. Each split will be passed to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> to enable truncated
back propagation through time. The default implementation splits root level Tensors and
Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">split_size</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                  <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>
        <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Called in the training loop after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_batch_start()</span></code>
if <a href="#id23"><span class="problematic" id="id24">:paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps`</span></a> &gt; 0.
Each returned batch split is passed separately to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.teardown">
<span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.teardown" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id25"><span class="problematic" id="id26">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_epoch_end">
<span class="sig-name descname"><span class="pre">test_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># do something with the outputs of all test batches</span>
    <span class="n">all_test_preds</span> <span class="o">=</span> <span class="n">test_step_outputs</span><span class="o">.</span><span class="n">predictions</span>

    <span class="n">some_result</span> <span class="o">=</span> <span class="n">calc_all_results</span><span class="p">(</span><span class="n">all_test_preds</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">some_result</span><span class="p">)</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">final_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">test_step_out</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="c1"># do something</span>
            <span class="n">final_value</span> <span class="o">+=</span> <span class="n">test_step_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step_end">
<span class="sig-name descname"><span class="pre">test_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">test_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT test_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with test_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_results</span><span class="p">):</span>
    <span class="c1"># this out is now the full size of the batch</span>
    <span class="n">all_test_step_outs</span> <span class="o">=</span> <span class="n">output_results</span><span class="o">.</span><span class="n">out</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">all_test_step_outs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> s. In addition, this method will
only cast the floating point parameters and buffers to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.device" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ExampleModule</span><span class="p">(</span><span class="n">DeviceDtypeModuleMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">ExampleModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float16</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – The desired device of the parameters
and buffers in this module.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>]) – The path of the file the onnx model should be saved to.</p></li>
<li><p><strong>input_sample</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><strong>**kwargs</strong> – Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_torchscript">
<span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.to_torchscript" title="Permalink to this definition">¶</a></dt>
<dd><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a> set. If you would like to customize the modules that
are scripted you should override this method. In case you want to return multiple modules, we recommend
using a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><strong>method</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use TorchScript’s script or trace method. Default: ‘script’</p></li>
<li><p><strong>example_inputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input to be used to do tracing when method is set to ‘trace’.
Default: None (uses <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a>)</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments that will be passed to the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code>
documentation for supported features.</p></li>
</ul>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(),</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> 
<span class="gp">... </span>                                    <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">)</span>  
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.toggle_optimizer">
<span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.toggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes sure only the gradients of the current optimizer’s parameters are calculated in the training step
to prevent dangling gradients in multiple-optimizer setup.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.
It works with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – The optimizer to toggle.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to toggle.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>bool</em>) – whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">page</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id27"><span class="problematic" id="id28">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_epoch_end">
<span class="sig-name descname"><span class="pre">training_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the training epoch with the outputs of all training steps. Use this in case you
need to do something with all the outputs returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">train_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">train_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">train_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.
If there are multiple optimizers, it is a list containing a list of outputs for each optimizer.
If using <code class="docutils literal notranslate"><span class="pre">truncated_bptt_steps</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, each element is a list of outputs corresponding to the outputs
of each processed split batch.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this method is not overridden, this won’t be called.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="c1"># do something with all training_step outputs</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the training step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step_end">
<span class="sig-name descname"><span class="pre">training_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">training_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <cite>training_step</cite> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Anything</p>
</dd>
</dl>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># softmax uses only a portion of the batch in the denominator</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">out</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="n">gpu_0_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_1_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_n_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>

    <span class="c1"># this softmax now uses the full batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">([</span><span class="n">gpu_0_pred</span><span class="p">,</span> <span class="n">gpu_1_pred</span><span class="p">,</span> <span class="n">gpu_n_pred</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.transfer_batch_to_device">
<span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.transfer_batch_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom
data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or anything that implements <cite>.to(…)</cite></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torchtext.data.batch.Batch</span></code></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, …).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be transferred to a new device.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>) – The target device as defined in PyTorch.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.truncated_bptt_steps">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">truncated_bptt_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.truncated_bptt_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p>
<p>It represents
the number of times <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> gets called before backpropagation. If this is &gt; 0, the
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> receives an additional argument <code class="docutils literal notranslate"><span class="pre">hiddens</span></code> and is expected to return a hidden state.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<em>type</em><em> or </em><em>string</em>) – the desired type</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.untoggle_optimizer">
<span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.untoggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to untoggle.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.use_amp">
<span class="sig-name descname"><span class="pre">use_amp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.use_amp" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id29"><span class="problematic" id="id30">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_epoch_end">
<span class="sig-name descname"><span class="pre">validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dataloader_output_result</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">dataloader_outs</span> <span class="o">=</span> <span class="n">dataloader_output_result</span><span class="o">.</span><span class="n">dataloader_i_outputs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the validation step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step_end">
<span class="sig-name descname"><span class="pre">validation_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">validation_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT validation_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with validation_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.xpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLFutureCovariatesModule.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero. See similar function
under <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> for more context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for details.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">darts.models.forecasting.pl_forecasting_module.</span></span><span class="sig-name descname"><span class="pre">PLMixedCovariatesModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn=MSELoss()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_cls=&lt;class</span> <span class="pre">'torch.optim.adam.Adam'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_cls=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_kwargs=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLMixedCovariatesModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">darts.models.forecasting.pl_forecasting_module.PLForecastingModule</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>PyTorch Lightning-based Forecasting Module.</p>
<p>This class is meant to be inherited to create a new PyTorch Lightning-based forecasting module.
When subclassing this class, please make sure to add the following methods with the given signatures:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.__init__()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.forward()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._produce_train_output()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._get_batch_prediction()</span></code></p></li>
</ul>
</div></blockquote>
<p>In subclass <cite>MyModel</cite>’s <code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code> function call <code class="docutils literal notranslate"><span class="pre">super(MyModel,</span> <span class="pre">self).__init__(**kwargs)</span></code> where
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> are the parameters of <code class="xref py py-class docutils literal notranslate"><span class="pre">PLTorchForecastingModel</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of input past time steps per chunk.</p></li>
<li><p><strong>output_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of output time steps per chunk.</p></li>
<li><p><strong>loss_fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">_Loss</span></code>) – PyTorch loss function used for training.
This parameter will be ignored for probabilistic models if the <code class="docutils literal notranslate"><span class="pre">likelihood</span></code> parameter is specified.
Default: <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss()</span></code>.</p></li>
<li><p><strong>likelihood</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-class docutils literal notranslate"><span class="pre">Likelihood</span></code></a>]) – One of Darts’ <a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Likelihood</span></code></a> models to be used for
probabilistic forecasts. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>optimizer_cls</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The PyTorch optimizer class to be used. Default: <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam</span></code>.</p></li>
<li><p><strong>optimizer_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch optimizer (e.g., <code class="docutils literal notranslate"><span class="pre">{'lr':</span> <span class="pre">1e-3}</span></code>
for specifying a learning rate). Otherwise the default values of the selected <code class="docutils literal notranslate"><span class="pre">optimizer_cls</span></code>
will be used. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_cls</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code>]) – Optionally, the PyTorch learning rate scheduler class to be used. Specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> corresponds
to using a constant learning rate. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.automatic_optimization" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.automatic_optimization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">automatic_optimization</span></code></a></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.current_epoch" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.current_epoch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_epoch</span></code></a></p></td>
<td><p>The current epoch in the Trainer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.example_input_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">example_input_array</span></code></a></p></td>
<td><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.global_rank" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.global_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_rank</span></code></a></p></td>
<td><p>The index of the current process across all nodes and devices.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.global_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.global_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code></a></p></td>
<td><p>Total training batches seen across all epochs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams_initial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams_initial</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.local_rank" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.local_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_rank</span></code></a></p></td>
<td><p>The index of the current process within a single node.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.logger" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logger</span></code></a></p></td>
<td><p>Reference to the logger object in the Trainer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.model_size" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.model_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">model_size</span></code></a></p></td>
<td><p>Returns the model size in MegaBytes (MB)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_gpu" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_gpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_gpu</span></code></a></p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.truncated_bptt_steps" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.truncated_bptt_steps"><code class="xref py py-obj docutils literal notranslate"><span class="pre">truncated_bptt_steps</span></code></a></p></td>
<td><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 76%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>device</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>dtype</strong></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><strong>epochs_trained</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>loaded_optimizer_states_dict</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_module</span></code></a>(name, module)</p></td>
<td><p>Adds a child module to the current module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_to_queue" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_to_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_to_queue</span></code></a>(queue)</p></td>
<td><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.all_gather" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.all_gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all_gather</span></code></a>(data[, group, sync_grads])</p></td>
<td><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation accelerator agnostic.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.apply" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(fn)</p></td>
<td><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.backward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>(loss, optimizer, optimizer_idx, ...)</p></td>
<td><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.bfloat16" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bfloat16</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.buffers" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">buffers</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module buffers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.children" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.clip_gradients" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.clip_gradients"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip_gradients</span></code></a>(optimizer[, ...])</p></td>
<td><p>Handles gradient clipping internally.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_callbacks" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_callbacks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_callbacks</span></code></a>()</p></td>
<td><p>Configure model-specific callbacks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_gradient_clipping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_gradient_clipping</span></code></a>(optimizer, ...)</p></td>
<td><p>Perform gradient clipping for the optimizer parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizers</span></code></a>()</p></td>
<td><p>configures optimizers and learning rate schedulers for for model optimization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_sharded_model" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_sharded_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_sharded_model</span></code></a>()</p></td>
<td><p>Hook to create modules in a distributed aware context.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cpu" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code></a>()</p></td>
<td><p>Moves all model parameters and buffers to the CPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuda</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.double" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">double</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.eval" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval</span></code></a>()</p></td>
<td><p>Sets the module in evaluation mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.extra_repr" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.extra_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_repr</span></code></a>()</p></td>
<td><p>Set the extra representation of the module</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(*args, **kwargs)</p></td>
<td><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.freeze" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.freeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze</span></code></a>()</p></td>
<td><p>Freeze all params for inference.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_buffer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_buffer</span></code></a>(target)</p></td>
<td><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_extra_state</span></code></a>()</p></td>
<td><p>Returns any extra state to include in the module's state_dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_from_queue" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_from_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_from_queue</span></code></a>(queue)</p></td>
<td><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_parameter" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_parameter</span></code></a>(target)</p></td>
<td><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_progress_bar_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_progress_bar_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_progress_bar_dict</span></code></a>()</p></td>
<td><p><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5.</span></p>
</div>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_submodule" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_submodule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_submodule</span></code></a>(target)</p></td>
<td><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.half" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">half</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_from_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_from_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code></a>(checkpoint_path[, ...])</p></td>
<td><p>Primary way of loading a model from a checkpoint.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_state_dict</span></code></a>(state_dict[, strict])</p></td>
<td><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>(name, value[, prog_bar, logger, ...])</p></td>
<td><p>Log a key, value pair.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_dict</span></code></a>(dictionary[, prog_bar, logger, ...])</p></td>
<td><p>Log a dictionary of values at once.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log_grad_norm" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log_grad_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_grad_norm</span></code></a>(grad_norm_dict)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.lr_schedulers" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.lr_schedulers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lr_schedulers</span></code></a>()</p></td>
<td><p>Returns the learning rate scheduler(s) that are being used during training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.manual_backward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.manual_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_backward</span></code></a>(loss, *args, **kwargs)</p></td>
<td><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.modules" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules</span></code></a>()</p></td>
<td><p>Returns an iterator over all modules in the network.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_buffers" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_buffers</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_children" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_modules" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_modules</span></code></a>([memo, prefix, remove_duplicate])</p></td>
<td><p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_parameters" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_parameters</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_backward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_backward</span></code></a>()</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_backward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_backward</span></code></a>(loss)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code></a>(optimizer, ...)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_zero_grad</span></code></a>(optimizer)</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch ends.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_start</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch begins.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_fit_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_fit_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_end</span></code></a>()</p></td>
<td><p>Called at the very end of fit.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_fit_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_fit_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_start</span></code></a>()</p></td>
<td><p>Called at the very beginning of fit.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_hpc_load" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_hpc_load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_load</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager loads the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_hpc_save" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_hpc_save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_save</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager saves the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_load_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_load_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_load_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning to restore your model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_post_move_to_device" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_post_move_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_post_move_to_device</span></code></a>()</p></td>
<td><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after <code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the predict loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the predict loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the predict dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_end</span></code></a>()</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_end</span></code></a>(results)</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the predict loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_pretrain_routine_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_pretrain_routine_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_end</span></code></a>()</p></td>
<td><p>Called at the end of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_pretrain_routine_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_pretrain_routine_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_start</span></code></a>()</p></td>
<td><p>Called at the beginning of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_save_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_save_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_end</span></code></a>(outputs, batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop after the batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the test dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_end</span></code></a>()</p></td>
<td><p>Called at the end of testing.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_end</span></code></a>()</p></td>
<td><p>Called in the test loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_start</span></code></a>()</p></td>
<td><p>Called in the test loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the test loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_model_train" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the test loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_start</span></code></a>()</p></td>
<td><p>Called at the beginning of testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_end</span></code></a>(outputs, batch, batch_idx)</p></td>
<td><p>Called in the training loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_start</span></code></a>(batch, batch_idx[, unused])</p></td>
<td><p>Called in the training loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the train dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_end</span></code></a>()</p></td>
<td><p>Called at the end of training before logger experiment is closed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_end</span></code></a>()</p></td>
<td><p>Called in the training loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_start</span></code></a>()</p></td>
<td><p>Called in the training loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_start</span></code></a>()</p></td>
<td><p>Called at the beginning of training after sanity check.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_val_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the val dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the validation loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the validation loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_end</span></code></a>()</p></td>
<td><p>Called at the end of validation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_end</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_start</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the val loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_model_train" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the val loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_start" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_start</span></code></a>()</p></td>
<td><p>Called at the beginning of validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_step</span></code></a>(epoch, batch_idx, optimizer)</p></td>
<td><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_zero_grad</span></code></a>(epoch, batch_idx, ...)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizers" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizers</span></code></a>([use_pl_optimizer])</p></td>
<td><p>Returns the optimizer(s) that are being used during training.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.parameters" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parameters</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for prediction.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_step</span></code></a>(batch, batch_idx[, dataloader_idx])</p></td>
<td><p>performs the prediction step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data</span></code></a>()</p></td>
<td><p>Use this to download and prepare data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.print" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.print"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print</span></code></a>(*args, **kwargs)</p></td>
<td><p>Prints only from process 0.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_buffer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_buffer</span></code></a>(name, tensor[, persistent])</p></td>
<td><p>Adds a buffer to the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_forward_hook" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_forward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_forward_pre_hook" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_forward_pre_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward pre-hook on the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_full_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_full_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_full_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_module" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_module</span></code></a>(name, module)</p></td>
<td><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_parameter" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_parameter</span></code></a>(name, param)</p></td>
<td><p>Adds a parameter to the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.requires_grad_" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad_</span></code></a>([requires_grad])</p></td>
<td><p>Change if autograd should record operations on parameters in this module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_hyperparameters</span></code></a>(*args[, ignore, frame, ...])</p></td>
<td><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_extra_state</span></code></a>(state)</p></td>
<td><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state found within the <cite>state_dict</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_predict_parameters" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_predict_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_predict_parameters</span></code></a>(n, num_samples, ...)</p></td>
<td><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setup</span></code></a>([stage])</p></td>
<td><p>Called at the beginning of fit (train + validate), validate, test, and predict.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.share_memory" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.share_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">share_memory</span></code></a>()</p></td>
<td><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_dict</span></code></a>([destination, prefix, keep_vars])</p></td>
<td><p>Returns a dictionary containing a whole state of the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.summarize" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.summarize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">summarize</span></code></a>([mode, max_depth])</p></td>
<td><p>Summarize this LightningModule.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.tbptt_split_batch" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.tbptt_split_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tbptt_split_batch</span></code></a>(batch, split_size)</p></td>
<td><p>When using truncated backpropagation through time, each batch must be split along the time dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.teardown" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.teardown"><code class="xref py py-obj docutils literal notranslate"><span class="pre">teardown</span></code></a>([stage])</p></td>
<td><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of a test epoch with the output of all test steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(*args, **kwargs)</p></td>
<td><p>Operates on a single batch of data from the test set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code></a>(*args, **kwargs)</p></td>
<td><p>Moves and/or casts the parameters and buffers.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_empty" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_empty</span></code></a>(*, device)</p></td>
<td><p>Moves the parameters and buffers to the specified device without copying storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_onnx" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_onnx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_onnx</span></code></a>(file_path[, input_sample])</p></td>
<td><p>Saves the model in ONNX format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_torchscript" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_torchscript"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_torchscript</span></code></a>([file_path, method, ...])</p></td>
<td><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.toggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">toggle_optimizer</span></code></a>(optimizer, optimizer_idx)</p></td>
<td><p>Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>([mode])</p></td>
<td><p>Sets the module in training mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_dataloader</span></code></a>()</p></td>
<td><p>Implement one or more PyTorch DataLoaders for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the training epoch with the outputs of all training steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step</span></code></a>(train_batch, batch_idx)</p></td>
<td><p>performs the training step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.transfer_batch_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transfer_batch_to_device</span></code></a>(batch, device, ...)</p></td>
<td><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom data structure.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.type" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a>(dst_type)</p></td>
<td><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.unfreeze" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.unfreeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unfreeze</span></code></a>()</p></td>
<td><p>Unfreeze all parameters for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.untoggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">untoggle_optimizer</span></code></a>(optimizer_idx)</p></td>
<td><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">val_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the validation epoch with the outputs of all validation steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step</span></code></a>(val_batch, batch_idx)</p></td>
<td><p>performs the validation step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.xpu" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xpu</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the XPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_grad</span></code></a>([set_to_none])</p></td>
<td><p>Sets gradients of all model parameters to zero.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 55%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>__call__</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_KEY</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hyper_parameters'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_NAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_name'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_TYPE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_type'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.T_destination">
<span class="sig-name descname"><span class="pre">T_destination</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.T_destination" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of TypeVar(‘T_destination’, bound=<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>])</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<em>Module</em>) – child module to be added to the module.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_to_queue">
<span class="sig-name descname"><span class="pre">add_to_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_to_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue. To avoid issues with memory
sharing, we cast the data to numpy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue to append the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.add_to_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation
accelerator agnostic. <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> is a function provided by accelerators to gather a tensor from several
distributed processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>]) – int, float, tensor of shape (batch, …), or a (possibly nested) collection thereof.</p></li>
<li><p><strong>group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><strong>sync_grads</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, …), or if the input was a collection
the output will also be a collection with tensors of this shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.allow_zero_length_dataloader_with_multiple_devices">
<span class="sig-name descname"><span class="pre">allow_zero_length_dataloader_with_multiple_devices</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.allow_zero_length_dataloader_with_multiple_devices" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.automatic_optimization">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">automatic_optimization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.automatic_optimization" title="Permalink to this definition">¶</a></dt>
<dd><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. Override this hook with your
own implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The loss tensor returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Index of the current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>torch.Tensor</em> – module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a child module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.clip_gradients">
<span class="sig-name descname"><span class="pre">clip_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.clip_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles gradient clipping internally.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Do not override this method. If you want to customize gradient clipping, consider
using <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_gradient_clipping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_gradient_clipping()</span></code></a> method.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_callbacks">
<span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_callbacks" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code>
gets called, the list returned here will be merged with the list of callbacks passed to the Trainer’s
<code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks already
present in the Trainer’s callbacks list, it will take priority and replace them. In addition, Lightning
will make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain callback methods like <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_init_start()</span></code>
will never be invoked on the new callbacks returned here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_gradient_clipping">
<span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients. By default value passed in Trainer
will be available here.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. By default value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Lightning will handle the gradient clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
            <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>configures optimizers and learning rate schedulers for for model optimization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_sharded_model">
<span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_sharded_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,
where we’d like to shard the model instantly, which is useful for extremely large models which can save
memory and initialization time.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is idempotent.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
different objects. So it should be called before constructing optimizer if the module will live on GPU
while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.current_epoch">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">current_epoch</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.current_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The current epoch in the Trainer.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.dtype">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.dtype</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.dtype" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">dtype</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.dump_patches">
<span class="sig-name descname"><span class="pre">dump_patches</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.epochs_trained">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">epochs_trained</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.epochs_trained" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.example_input_array">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">example_input_array</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.example_input_array" title="Permalink to this definition">¶</a></dt>
<dd><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.
The return type is interpreted as follows:</p>
<ul class="simple">
<li><p>Single tensor: It is assumed the model takes a single argument, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(model.example_input_array)</span></code></p></li>
<li><p>Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(*model.example_input_array)</span></code></p></li>
<li><p>Dict: The input array represents named keyword arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(**model.example_input_array)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Your model’s output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the buffer
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not a
buffer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns any extra state to include in the module’s state_dict.
Implement this and a corresponding <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
module’s <cite>state_dict()</cite>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Any extra state to store in the module’s state_dict</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_from_queue">
<span class="sig-name descname"><span class="pre">get_from_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue. To preserve consistency,
we cast back the data to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue from where to get the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.get_from_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the Parameter
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Parameter</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_progress_bar_dict">
<span class="sig-name descname"><span class="pre">get_progress_bar_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_progress_bar_dict" title="Permalink to this definition">¶</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of
<cite>pytorch_lightning.callbacks.progress.base.get_metrics</cite> and will be removed in v1.7.</p>
</div>
<p>Implement this to override the default items displayed in the progress bar.
By default it includes the average loss value, split index of BPTT (if used)
and the version of the experiment when using a logger.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Epoch 1:   4%|▎         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]
</pre></div>
</div>
<p>Here is an example how to override the defaults:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># don&#39;t show the version number</span>
    <span class="n">items</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_progress_bar_dict</span><span class="p">()</span>
    <span class="n">items</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;v_num&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">items</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary with the items to be displayed in the progress bar.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_submodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.global_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.global_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process across all nodes and devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.global_step">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_step</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.global_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Total training batches seen across all epochs.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">argparse.Namespace</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. It is mutable by the user.
For the frozen set of initial hyperparameters, use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams_initial"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams_initial</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>mutable hyperparameters dicionary</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Union[AttributeDict, dict, Namespace]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams_initial">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams_initial</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams_initial" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. These contents are read-only.
Manual updates to the saved hyperparameters can instead be performed through <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.hparams"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>immutable initial hyperparameters</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>AttributeDict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_from_checkpoint">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_from_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <cite>__init__</cite>  in the checkpoint under <cite>hyper_parameters</cite></p>
<p>Any arguments specified through *args and **kwargs will override args stored in <cite>hyper_parameters</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code>]) – Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><strong>map_location</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p></li>
<li><p><strong>hparams_file</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Optional path to a .yaml file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely won’t need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights don’t have the hyperparameters saved,
use this method to pass in a .yaml file with the hparams you’d like to use.
These will be converted into a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your model’s <cite>hparams</cite> argument is <code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code>
and .yaml file has hierarchical structure, you need to refactor your model to treat
<cite>hparams</cite> as <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
</p></li>
<li><p><strong>strict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this module’s state dict. Default: <cite>True</cite>.</p></li>
<li><p><strong>kwargs</strong> – Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.loaded_optimizer_states_dict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">loaded_optimizer_states_dict</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.loaded_optimizer_states_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.local_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">local_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.local_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process within a single node.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is as follows:</p>
<table class="colwidths-given table" id="id64">
<caption><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">*</span></code> also applies to the test loop</span><a class="headerlink" href="#id64" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>LightningModule Hook</p></th>
<th class="head"><p>on_step</p></th>
<th class="head"><p>on_epoch</p></th>
<th class="head"><p>prog_bar</p></th>
<th class="head"><p>logger</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>training_step</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>training_step_end</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>training_epoch_end</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_step*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>validation_step_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_epoch_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – key to log</p></li>
<li><p><strong>value</strong> – value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> – if True logs to the progress bar</p></li>
<li><p><strong>logger</strong> – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> – if True logs at this step. None auto-logs at the training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> – the ddp group to sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>metric_attribute</strong> – To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><strong>rank_zero_only</strong> – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log_dict">
<span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dictionary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]]]]) – key value pairs.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the progress base</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs at this step. None auto-logs for training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the ddp group sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>rank_zero_only</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log_grad_norm">
<span class="sig-name descname"><span class="pre">log_grad_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_norm_dict</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.log_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grad_norm_dict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]) – Dictionary containing current grad norm metrics</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.logger">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logger</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Reference to the logger object in the Trainer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.lr_schedulers">
<span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.lr_schedulers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.configure_optimizers"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.manual_backward">
<span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.manual_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <span class="xref std std-ref">manual optimization</span> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><strong>*args</strong> – Additional positional arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.model_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float"><span class="pre">float</span></a></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.model_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model size in MegaBytes (MB)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This property will not return correct value for Deepspeed (stage 3) and fully-sharded training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memo</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Set</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]) – a memo to store the set of modules already added to the result</p></li>
<li><p><strong>prefix</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – a prefix that will be added to the name of the module</p></li>
<li><p><strong>remove_duplicate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – whether to remove the duplicated module instances in the result
or not</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Module)</em> – Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all parameter names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_batch_transfer">
<span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_backward">
<span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_batch_transfer">
<span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_after_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_optimizer_step">
<span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>The hook is only called if gradients do not need to be accumulated.
See: <a href="#id31"><span class="problematic" id="id32">:paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`</span></a>.</p>
<p>If using native AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_zero_grad">
<span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_before_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The optimizer for which grads should be zeroed.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch ends.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_epoch_start">
<span class="sig-name descname"><span class="pre">on_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch begins.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_fit_end">
<span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_fit_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_fit_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_gpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">on_gpu</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p>
<p>Useful to set flags around the LightningModule for different CPU vs GPU behavior.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_hpc_load">
<span class="sig-name descname"><span class="pre">on_hpc_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_hpc_load" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager loads the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with variables from the checkpoint.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_hpc_save">
<span class="sig-name descname"><span class="pre">on_hpc_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_hpc_save" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager saves the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning to restore your model.
If you saved something with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_post_move_to_device">
<span class="sig-name descname"><span class="pre">on_post_move_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_post_move_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called. This is a good place to tie weights between
modules after moving them to a device. Can be used when training models with weight sharing properties on
TPU.</p>
<p>Addresses the handling of shared weights on TPU:
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks">https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The outputs of predict_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_batch_start">
<span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_dataloader">
<span class="sig-name descname"><span class="pre">on_predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the predict dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_predict_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_end">
<span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_epoch_end">
<span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_epoch_start">
<span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_model_eval">
<span class="sig-name descname"><span class="pre">on_predict_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the predict loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_start">
<span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_predict_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_pretrain_routine_end">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_pretrain_routine_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_pretrain_routine_start">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_pretrain_routine_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything
else you might want to save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of test_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_batch_start">
<span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_dataloader">
<span class="sig-name descname"><span class="pre">on_test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the test dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_end">
<span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_epoch_start">
<span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_model_eval">
<span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_model_train">
<span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_start">
<span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_test_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – The outputs of training_step_end(training_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_batch_start">
<span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_dataloader">
<span class="sig-name descname"><span class="pre">on_train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the train dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, either:</p>
<ol class="arabic simple">
<li><p>Implement <cite>training_epoch_end</cite> in the LightningModule OR</p></li>
<li><p>Cache data across steps on the attribute(s) of the <cite>LightningModule</cite> and access them in this hook</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_start">
<span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_train_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_val_dataloader">
<span class="sig-name descname"><span class="pre">on_val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the val dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_val_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of validation_step_end(validation_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_batch_start">
<span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_model_eval">
<span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_model_train">
<span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.on_validation_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_step">
<span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_tpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_native_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_lbfgs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to adjust the default way the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.
By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example
once per optimizer. This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) won’t be called during the
accumulation phase when <code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers, this indexes into that list.</p></li>
<li><p><strong>optimizer_closure</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Closure for all optimizers. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
<li><p><strong>on_tpu</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if TPU backward is required</p></li>
<li><p><strong>using_native_amp</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if using native amp</p></li>
<li><p><strong>using_lbfgs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True if the matching optimizer is <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

<span class="c1"># Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># update generator opt every step</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># update discriminator opt every 2 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
            <span class="n">optimizer_closure</span><span class="p">()</span>

    <span class="c1"># ...</span>
    <span class="c1"># add as many optimizers as you want</span>
</pre></div>
</div>
<p>Here’s another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">,</span>
    <span class="n">optimizer_closure</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># warm up lr</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_zero_grad">
<span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizer_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers this indexes into that list.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance.</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for the explanation of the above example.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizers">
<span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_pl_optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision and
profiling.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Parameter</em> – module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.precision" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_batch_size">
<span class="sig-name descname"><span class="pre">pred_batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_n">
<span class="sig-name descname"><span class="pre">pred_n</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_n" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_n_jobs">
<span class="sig-name descname"><span class="pre">pred_n_jobs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_n_jobs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_num_samples">
<span class="sig-name descname"><span class="pre">pred_num_samples</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_num_samples" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_roll_size">
<span class="sig-name descname"><span class="pre">pred_roll_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.pred_roll_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple prediction dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the prediction step</p>
<dl class="simple">
<dt>batch</dt><dd><p>output of Darts’ <code class="xref py py-class docutils literal notranslate"><span class="pre">InferenceDataset</span></code> - tuple of <code class="docutils literal notranslate"><span class="pre">(past_target,</span> <span class="pre">past_covariates,</span>
<span class="pre">historic_future_covariates,</span> <span class="pre">future_covariates,</span> <span class="pre">future_past_covariates,</span> <span class="pre">input_timeseries)</span></code></p>
</dd>
<dt>batch_idx</dt><dd><p>the batch index of the current batch</p>
</dd>
<dt>dataloader_idx</dt><dd><p>the dataloader index</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<a class="reference internal" href="darts.timeseries.html#darts.timeseries.TimeSeries" title="darts.timeseries.TimeSeries"><code class="xref py py-class docutils literal notranslate"><span class="pre">TimeSeries</span></code></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this to download and prepare data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <cite>setup</cite> instead)
since this is NOT called on every GPU in DDP/TPU</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> with the trainer flag is deprecated and will be removed in v1.7.0.
Please set <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> in LightningDataModule or LightningModule directly instead.</p>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data_per_node">
<span class="sig-name descname"><span class="pre">prepare_data_per_node</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data_per_node" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.print">
<span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.print" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – The thing to print. The same as for Python’s built-in print function.</p></li>
<li><p><strong>**kwargs</strong> – The same as for Python’s built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of <code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code> and
the behavior of this function will change in future versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the module’s state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em><em> or </em><em>None</em>) – buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations
that run on buffers, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
<li><p><strong>persistent</strong> (<em>bool</em>) – whether the buffer is part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module’s forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em><em> or </em><em>None</em>) – parameter to be added to the module. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<em>bool</em>) – whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters">
<span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.save_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><strong>ignore</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><strong>frame</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">frame</span></code>]) – a frame object. Default is None</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>dict</em>) – Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_predict_parameters">
<span class="sig-name descname"><span class="pre">set_predict_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roll_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.set_predict_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.share_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>~T</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.summarize">
<span class="sig-name descname"><span class="pre">summarize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.summarize" title="Permalink to this definition">¶</a></dt>
<dd><p>Summarize this LightningModule.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>pytorch_lightning.utilities.model_summary.summarize</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Can be either <code class="docutils literal notranslate"><span class="pre">'top'</span></code> (summarize only direct submodules) or <code class="docutils literal notranslate"><span class="pre">'full'</span></code> (summarize all layers).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.4: </span>This parameter was deprecated in v1.4 in favor of <cite>max_depth</cite> and will be removed in v1.6.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The maximum depth of layer nesting that the summary will include. A value of 0 turns the
layer summary off. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSummary</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The model summary object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.tbptt_split_batch">
<span class="sig-name descname"><span class="pre">tbptt_split_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.tbptt_split_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Current batch</p></li>
<li><p><strong>split_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The size of the split</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of batch splits. Each split will be passed to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> to enable truncated
back propagation through time. The default implementation splits root level Tensors and
Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">split_size</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                  <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>
        <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Called in the training loop after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_batch_start()</span></code>
if <a href="#id33"><span class="problematic" id="id34">:paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps`</span></a> &gt; 0.
Each returned batch split is passed separately to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.teardown">
<span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.teardown" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id35"><span class="problematic" id="id36">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_epoch_end">
<span class="sig-name descname"><span class="pre">test_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># do something with the outputs of all test batches</span>
    <span class="n">all_test_preds</span> <span class="o">=</span> <span class="n">test_step_outputs</span><span class="o">.</span><span class="n">predictions</span>

    <span class="n">some_result</span> <span class="o">=</span> <span class="n">calc_all_results</span><span class="p">(</span><span class="n">all_test_preds</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">some_result</span><span class="p">)</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">final_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">test_step_out</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="c1"># do something</span>
            <span class="n">final_value</span> <span class="o">+=</span> <span class="n">test_step_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step_end">
<span class="sig-name descname"><span class="pre">test_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">test_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT test_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with test_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_results</span><span class="p">):</span>
    <span class="c1"># this out is now the full size of the batch</span>
    <span class="n">all_test_step_outs</span> <span class="o">=</span> <span class="n">output_results</span><span class="o">.</span><span class="n">out</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">all_test_step_outs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> s. In addition, this method will
only cast the floating point parameters and buffers to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.device" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ExampleModule</span><span class="p">(</span><span class="n">DeviceDtypeModuleMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">ExampleModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float16</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – The desired device of the parameters
and buffers in this module.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>]) – The path of the file the onnx model should be saved to.</p></li>
<li><p><strong>input_sample</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><strong>**kwargs</strong> – Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_torchscript">
<span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.to_torchscript" title="Permalink to this definition">¶</a></dt>
<dd><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a> set. If you would like to customize the modules that
are scripted you should override this method. In case you want to return multiple modules, we recommend
using a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><strong>method</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use TorchScript’s script or trace method. Default: ‘script’</p></li>
<li><p><strong>example_inputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input to be used to do tracing when method is set to ‘trace’.
Default: None (uses <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a>)</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments that will be passed to the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code>
documentation for supported features.</p></li>
</ul>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(),</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> 
<span class="gp">... </span>                                    <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">)</span>  
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.toggle_optimizer">
<span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.toggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes sure only the gradients of the current optimizer’s parameters are calculated in the training step
to prevent dangling gradients in multiple-optimizer setup.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.
It works with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – The optimizer to toggle.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to toggle.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>bool</em>) – whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">page</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id37"><span class="problematic" id="id38">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_epoch_end">
<span class="sig-name descname"><span class="pre">training_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the training epoch with the outputs of all training steps. Use this in case you
need to do something with all the outputs returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">train_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">train_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">train_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.
If there are multiple optimizers, it is a list containing a list of outputs for each optimizer.
If using <code class="docutils literal notranslate"><span class="pre">truncated_bptt_steps</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, each element is a list of outputs corresponding to the outputs
of each processed split batch.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this method is not overridden, this won’t be called.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="c1"># do something with all training_step outputs</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the training step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step_end">
<span class="sig-name descname"><span class="pre">training_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">training_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <cite>training_step</cite> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Anything</p>
</dd>
</dl>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># softmax uses only a portion of the batch in the denominator</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">out</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="n">gpu_0_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_1_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_n_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>

    <span class="c1"># this softmax now uses the full batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">([</span><span class="n">gpu_0_pred</span><span class="p">,</span> <span class="n">gpu_1_pred</span><span class="p">,</span> <span class="n">gpu_n_pred</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.transfer_batch_to_device">
<span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.transfer_batch_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom
data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or anything that implements <cite>.to(…)</cite></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torchtext.data.batch.Batch</span></code></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, …).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be transferred to a new device.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>) – The target device as defined in PyTorch.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.truncated_bptt_steps">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">truncated_bptt_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.truncated_bptt_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p>
<p>It represents
the number of times <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> gets called before backpropagation. If this is &gt; 0, the
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> receives an additional argument <code class="docutils literal notranslate"><span class="pre">hiddens</span></code> and is expected to return a hidden state.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<em>type</em><em> or </em><em>string</em>) – the desired type</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.untoggle_optimizer">
<span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.untoggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to untoggle.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.use_amp">
<span class="sig-name descname"><span class="pre">use_amp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.use_amp" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id39"><span class="problematic" id="id40">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_epoch_end">
<span class="sig-name descname"><span class="pre">validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dataloader_output_result</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">dataloader_outs</span> <span class="o">=</span> <span class="n">dataloader_output_result</span><span class="o">.</span><span class="n">dataloader_i_outputs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the validation step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step_end">
<span class="sig-name descname"><span class="pre">validation_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">validation_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT validation_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with validation_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.xpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLMixedCovariatesModule.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero. See similar function
under <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> for more context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for details.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">darts.models.forecasting.pl_forecasting_module.</span></span><span class="sig-name descname"><span class="pre">PLPastCovariatesModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn=MSELoss()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_cls=&lt;class</span> <span class="pre">'torch.optim.adam.Adam'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_cls=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_kwargs=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLPastCovariatesModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">darts.models.forecasting.pl_forecasting_module.PLForecastingModule</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>PyTorch Lightning-based Forecasting Module.</p>
<p>This class is meant to be inherited to create a new PyTorch Lightning-based forecasting module.
When subclassing this class, please make sure to add the following methods with the given signatures:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.__init__()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.forward()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._produce_train_output()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._get_batch_prediction()</span></code></p></li>
</ul>
</div></blockquote>
<p>In subclass <cite>MyModel</cite>’s <code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code> function call <code class="docutils literal notranslate"><span class="pre">super(MyModel,</span> <span class="pre">self).__init__(**kwargs)</span></code> where
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> are the parameters of <code class="xref py py-class docutils literal notranslate"><span class="pre">PLTorchForecastingModel</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of input past time steps per chunk.</p></li>
<li><p><strong>output_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of output time steps per chunk.</p></li>
<li><p><strong>loss_fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">_Loss</span></code>) – PyTorch loss function used for training.
This parameter will be ignored for probabilistic models if the <code class="docutils literal notranslate"><span class="pre">likelihood</span></code> parameter is specified.
Default: <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss()</span></code>.</p></li>
<li><p><strong>likelihood</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-class docutils literal notranslate"><span class="pre">Likelihood</span></code></a>]) – One of Darts’ <a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Likelihood</span></code></a> models to be used for
probabilistic forecasts. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>optimizer_cls</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The PyTorch optimizer class to be used. Default: <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam</span></code>.</p></li>
<li><p><strong>optimizer_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch optimizer (e.g., <code class="docutils literal notranslate"><span class="pre">{'lr':</span> <span class="pre">1e-3}</span></code>
for specifying a learning rate). Otherwise the default values of the selected <code class="docutils literal notranslate"><span class="pre">optimizer_cls</span></code>
will be used. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_cls</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code>]) – Optionally, the PyTorch learning rate scheduler class to be used. Specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> corresponds
to using a constant learning rate. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.automatic_optimization" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.automatic_optimization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">automatic_optimization</span></code></a></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.current_epoch" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.current_epoch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_epoch</span></code></a></p></td>
<td><p>The current epoch in the Trainer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.example_input_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">example_input_array</span></code></a></p></td>
<td><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.global_rank" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.global_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_rank</span></code></a></p></td>
<td><p>The index of the current process across all nodes and devices.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.global_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.global_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code></a></p></td>
<td><p>Total training batches seen across all epochs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams_initial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams_initial</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.local_rank" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.local_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_rank</span></code></a></p></td>
<td><p>The index of the current process within a single node.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.logger" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logger</span></code></a></p></td>
<td><p>Reference to the logger object in the Trainer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.model_size" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.model_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">model_size</span></code></a></p></td>
<td><p>Returns the model size in MegaBytes (MB)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_gpu" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_gpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_gpu</span></code></a></p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.truncated_bptt_steps" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.truncated_bptt_steps"><code class="xref py py-obj docutils literal notranslate"><span class="pre">truncated_bptt_steps</span></code></a></p></td>
<td><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 76%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>device</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>dtype</strong></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><strong>epochs_trained</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>loaded_optimizer_states_dict</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_module</span></code></a>(name, module)</p></td>
<td><p>Adds a child module to the current module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_to_queue" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_to_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_to_queue</span></code></a>(queue)</p></td>
<td><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.all_gather" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.all_gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all_gather</span></code></a>(data[, group, sync_grads])</p></td>
<td><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation accelerator agnostic.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.apply" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(fn)</p></td>
<td><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.backward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>(loss, optimizer, optimizer_idx, ...)</p></td>
<td><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.bfloat16" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bfloat16</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.buffers" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">buffers</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module buffers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.children" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.clip_gradients" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.clip_gradients"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip_gradients</span></code></a>(optimizer[, ...])</p></td>
<td><p>Handles gradient clipping internally.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_callbacks" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_callbacks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_callbacks</span></code></a>()</p></td>
<td><p>Configure model-specific callbacks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_gradient_clipping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_gradient_clipping</span></code></a>(optimizer, ...)</p></td>
<td><p>Perform gradient clipping for the optimizer parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizers</span></code></a>()</p></td>
<td><p>configures optimizers and learning rate schedulers for for model optimization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_sharded_model" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_sharded_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_sharded_model</span></code></a>()</p></td>
<td><p>Hook to create modules in a distributed aware context.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cpu" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code></a>()</p></td>
<td><p>Moves all model parameters and buffers to the CPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuda</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.double" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">double</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.eval" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval</span></code></a>()</p></td>
<td><p>Sets the module in evaluation mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.extra_repr" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.extra_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_repr</span></code></a>()</p></td>
<td><p>Set the extra representation of the module</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(*args, **kwargs)</p></td>
<td><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.freeze" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.freeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze</span></code></a>()</p></td>
<td><p>Freeze all params for inference.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_buffer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_buffer</span></code></a>(target)</p></td>
<td><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_extra_state</span></code></a>()</p></td>
<td><p>Returns any extra state to include in the module's state_dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_from_queue" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_from_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_from_queue</span></code></a>(queue)</p></td>
<td><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_parameter" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_parameter</span></code></a>(target)</p></td>
<td><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_progress_bar_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_progress_bar_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_progress_bar_dict</span></code></a>()</p></td>
<td><p><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5.</span></p>
</div>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_submodule" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_submodule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_submodule</span></code></a>(target)</p></td>
<td><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.half" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">half</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_from_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_from_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code></a>(checkpoint_path[, ...])</p></td>
<td><p>Primary way of loading a model from a checkpoint.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_state_dict</span></code></a>(state_dict[, strict])</p></td>
<td><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>(name, value[, prog_bar, logger, ...])</p></td>
<td><p>Log a key, value pair.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_dict</span></code></a>(dictionary[, prog_bar, logger, ...])</p></td>
<td><p>Log a dictionary of values at once.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log_grad_norm" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log_grad_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_grad_norm</span></code></a>(grad_norm_dict)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.lr_schedulers" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.lr_schedulers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lr_schedulers</span></code></a>()</p></td>
<td><p>Returns the learning rate scheduler(s) that are being used during training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.manual_backward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.manual_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_backward</span></code></a>(loss, *args, **kwargs)</p></td>
<td><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.modules" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules</span></code></a>()</p></td>
<td><p>Returns an iterator over all modules in the network.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_buffers" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_buffers</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_children" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_modules" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_modules</span></code></a>([memo, prefix, remove_duplicate])</p></td>
<td><p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_parameters" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_parameters</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_backward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_backward</span></code></a>()</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_backward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_backward</span></code></a>(loss)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code></a>(optimizer, ...)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_zero_grad</span></code></a>(optimizer)</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch ends.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_start</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch begins.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_fit_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_fit_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_end</span></code></a>()</p></td>
<td><p>Called at the very end of fit.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_fit_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_fit_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_start</span></code></a>()</p></td>
<td><p>Called at the very beginning of fit.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_hpc_load" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_hpc_load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_load</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager loads the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_hpc_save" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_hpc_save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_save</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager saves the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_load_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_load_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_load_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning to restore your model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_post_move_to_device" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_post_move_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_post_move_to_device</span></code></a>()</p></td>
<td><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after <code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the predict loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the predict loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the predict dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_end</span></code></a>()</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_end</span></code></a>(results)</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the predict loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_pretrain_routine_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_pretrain_routine_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_end</span></code></a>()</p></td>
<td><p>Called at the end of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_pretrain_routine_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_pretrain_routine_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_start</span></code></a>()</p></td>
<td><p>Called at the beginning of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_save_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_save_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_end</span></code></a>(outputs, batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop after the batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the test dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_end</span></code></a>()</p></td>
<td><p>Called at the end of testing.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_end</span></code></a>()</p></td>
<td><p>Called in the test loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_start</span></code></a>()</p></td>
<td><p>Called in the test loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the test loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_model_train" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the test loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_start</span></code></a>()</p></td>
<td><p>Called at the beginning of testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_end</span></code></a>(outputs, batch, batch_idx)</p></td>
<td><p>Called in the training loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_start</span></code></a>(batch, batch_idx[, unused])</p></td>
<td><p>Called in the training loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the train dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_end</span></code></a>()</p></td>
<td><p>Called at the end of training before logger experiment is closed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_end</span></code></a>()</p></td>
<td><p>Called in the training loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_start</span></code></a>()</p></td>
<td><p>Called in the training loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_start</span></code></a>()</p></td>
<td><p>Called at the beginning of training after sanity check.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_val_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the val dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the validation loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the validation loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_end</span></code></a>()</p></td>
<td><p>Called at the end of validation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_end</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_start</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the val loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_model_train" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the val loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_start" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_start</span></code></a>()</p></td>
<td><p>Called at the beginning of validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_step</span></code></a>(epoch, batch_idx, optimizer)</p></td>
<td><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_zero_grad</span></code></a>(epoch, batch_idx, ...)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizers" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizers</span></code></a>([use_pl_optimizer])</p></td>
<td><p>Returns the optimizer(s) that are being used during training.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.parameters" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parameters</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for prediction.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_step</span></code></a>(batch, batch_idx[, dataloader_idx])</p></td>
<td><p>performs the prediction step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data</span></code></a>()</p></td>
<td><p>Use this to download and prepare data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.print" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.print"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print</span></code></a>(*args, **kwargs)</p></td>
<td><p>Prints only from process 0.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_buffer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_buffer</span></code></a>(name, tensor[, persistent])</p></td>
<td><p>Adds a buffer to the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_forward_hook" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_forward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_forward_pre_hook" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_forward_pre_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward pre-hook on the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_full_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_full_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_full_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_module" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_module</span></code></a>(name, module)</p></td>
<td><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_parameter" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_parameter</span></code></a>(name, param)</p></td>
<td><p>Adds a parameter to the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.requires_grad_" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad_</span></code></a>([requires_grad])</p></td>
<td><p>Change if autograd should record operations on parameters in this module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_hyperparameters</span></code></a>(*args[, ignore, frame, ...])</p></td>
<td><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_extra_state</span></code></a>(state)</p></td>
<td><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state found within the <cite>state_dict</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_predict_parameters" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_predict_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_predict_parameters</span></code></a>(n, num_samples, ...)</p></td>
<td><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setup</span></code></a>([stage])</p></td>
<td><p>Called at the beginning of fit (train + validate), validate, test, and predict.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.share_memory" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.share_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">share_memory</span></code></a>()</p></td>
<td><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_dict</span></code></a>([destination, prefix, keep_vars])</p></td>
<td><p>Returns a dictionary containing a whole state of the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.summarize" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.summarize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">summarize</span></code></a>([mode, max_depth])</p></td>
<td><p>Summarize this LightningModule.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.tbptt_split_batch" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.tbptt_split_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tbptt_split_batch</span></code></a>(batch, split_size)</p></td>
<td><p>When using truncated backpropagation through time, each batch must be split along the time dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.teardown" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.teardown"><code class="xref py py-obj docutils literal notranslate"><span class="pre">teardown</span></code></a>([stage])</p></td>
<td><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of a test epoch with the output of all test steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(*args, **kwargs)</p></td>
<td><p>Operates on a single batch of data from the test set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code></a>(*args, **kwargs)</p></td>
<td><p>Moves and/or casts the parameters and buffers.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_empty" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_empty</span></code></a>(*, device)</p></td>
<td><p>Moves the parameters and buffers to the specified device without copying storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_onnx" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_onnx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_onnx</span></code></a>(file_path[, input_sample])</p></td>
<td><p>Saves the model in ONNX format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_torchscript" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_torchscript"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_torchscript</span></code></a>([file_path, method, ...])</p></td>
<td><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.toggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">toggle_optimizer</span></code></a>(optimizer, optimizer_idx)</p></td>
<td><p>Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>([mode])</p></td>
<td><p>Sets the module in training mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_dataloader</span></code></a>()</p></td>
<td><p>Implement one or more PyTorch DataLoaders for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the training epoch with the outputs of all training steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step</span></code></a>(train_batch, batch_idx)</p></td>
<td><p>performs the training step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.transfer_batch_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transfer_batch_to_device</span></code></a>(batch, device, ...)</p></td>
<td><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom data structure.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.type" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a>(dst_type)</p></td>
<td><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.unfreeze" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.unfreeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unfreeze</span></code></a>()</p></td>
<td><p>Unfreeze all parameters for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.untoggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">untoggle_optimizer</span></code></a>(optimizer_idx)</p></td>
<td><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">val_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the validation epoch with the outputs of all validation steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step</span></code></a>(val_batch, batch_idx)</p></td>
<td><p>performs the validation step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.xpu" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xpu</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the XPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_grad</span></code></a>([set_to_none])</p></td>
<td><p>Sets gradients of all model parameters to zero.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 55%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>__call__</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_KEY</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hyper_parameters'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_NAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_name'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_TYPE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_type'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.T_destination">
<span class="sig-name descname"><span class="pre">T_destination</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.T_destination" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of TypeVar(‘T_destination’, bound=<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>])</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<em>Module</em>) – child module to be added to the module.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_to_queue">
<span class="sig-name descname"><span class="pre">add_to_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_to_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue. To avoid issues with memory
sharing, we cast the data to numpy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue to append the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.add_to_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation
accelerator agnostic. <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> is a function provided by accelerators to gather a tensor from several
distributed processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>]) – int, float, tensor of shape (batch, …), or a (possibly nested) collection thereof.</p></li>
<li><p><strong>group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><strong>sync_grads</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, …), or if the input was a collection
the output will also be a collection with tensors of this shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.allow_zero_length_dataloader_with_multiple_devices">
<span class="sig-name descname"><span class="pre">allow_zero_length_dataloader_with_multiple_devices</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.allow_zero_length_dataloader_with_multiple_devices" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.automatic_optimization">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">automatic_optimization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.automatic_optimization" title="Permalink to this definition">¶</a></dt>
<dd><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. Override this hook with your
own implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The loss tensor returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Index of the current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>torch.Tensor</em> – module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a child module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.clip_gradients">
<span class="sig-name descname"><span class="pre">clip_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.clip_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles gradient clipping internally.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Do not override this method. If you want to customize gradient clipping, consider
using <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_gradient_clipping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_gradient_clipping()</span></code></a> method.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_callbacks">
<span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_callbacks" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code>
gets called, the list returned here will be merged with the list of callbacks passed to the Trainer’s
<code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks already
present in the Trainer’s callbacks list, it will take priority and replace them. In addition, Lightning
will make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain callback methods like <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_init_start()</span></code>
will never be invoked on the new callbacks returned here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_gradient_clipping">
<span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients. By default value passed in Trainer
will be available here.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. By default value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Lightning will handle the gradient clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
            <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>configures optimizers and learning rate schedulers for for model optimization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_sharded_model">
<span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_sharded_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,
where we’d like to shard the model instantly, which is useful for extremely large models which can save
memory and initialization time.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is idempotent.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
different objects. So it should be called before constructing optimizer if the module will live on GPU
while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.current_epoch">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">current_epoch</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.current_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The current epoch in the Trainer.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.dtype">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.dtype</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.dtype" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">dtype</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.dump_patches">
<span class="sig-name descname"><span class="pre">dump_patches</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.epochs_trained">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">epochs_trained</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.epochs_trained" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.example_input_array">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">example_input_array</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.example_input_array" title="Permalink to this definition">¶</a></dt>
<dd><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.
The return type is interpreted as follows:</p>
<ul class="simple">
<li><p>Single tensor: It is assumed the model takes a single argument, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(model.example_input_array)</span></code></p></li>
<li><p>Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(*model.example_input_array)</span></code></p></li>
<li><p>Dict: The input array represents named keyword arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(**model.example_input_array)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Your model’s output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the buffer
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not a
buffer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns any extra state to include in the module’s state_dict.
Implement this and a corresponding <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
module’s <cite>state_dict()</cite>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Any extra state to store in the module’s state_dict</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_from_queue">
<span class="sig-name descname"><span class="pre">get_from_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue. To preserve consistency,
we cast back the data to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue from where to get the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.get_from_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the Parameter
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Parameter</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_progress_bar_dict">
<span class="sig-name descname"><span class="pre">get_progress_bar_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_progress_bar_dict" title="Permalink to this definition">¶</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of
<cite>pytorch_lightning.callbacks.progress.base.get_metrics</cite> and will be removed in v1.7.</p>
</div>
<p>Implement this to override the default items displayed in the progress bar.
By default it includes the average loss value, split index of BPTT (if used)
and the version of the experiment when using a logger.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Epoch 1:   4%|▎         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]
</pre></div>
</div>
<p>Here is an example how to override the defaults:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># don&#39;t show the version number</span>
    <span class="n">items</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_progress_bar_dict</span><span class="p">()</span>
    <span class="n">items</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;v_num&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">items</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary with the items to be displayed in the progress bar.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_submodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.global_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.global_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process across all nodes and devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.global_step">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_step</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.global_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Total training batches seen across all epochs.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">argparse.Namespace</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. It is mutable by the user.
For the frozen set of initial hyperparameters, use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams_initial"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams_initial</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>mutable hyperparameters dicionary</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Union[AttributeDict, dict, Namespace]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams_initial">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams_initial</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams_initial" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. These contents are read-only.
Manual updates to the saved hyperparameters can instead be performed through <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.hparams"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>immutable initial hyperparameters</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>AttributeDict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_from_checkpoint">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_from_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <cite>__init__</cite>  in the checkpoint under <cite>hyper_parameters</cite></p>
<p>Any arguments specified through *args and **kwargs will override args stored in <cite>hyper_parameters</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code>]) – Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><strong>map_location</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p></li>
<li><p><strong>hparams_file</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Optional path to a .yaml file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely won’t need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights don’t have the hyperparameters saved,
use this method to pass in a .yaml file with the hparams you’d like to use.
These will be converted into a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your model’s <cite>hparams</cite> argument is <code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code>
and .yaml file has hierarchical structure, you need to refactor your model to treat
<cite>hparams</cite> as <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
</p></li>
<li><p><strong>strict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this module’s state dict. Default: <cite>True</cite>.</p></li>
<li><p><strong>kwargs</strong> – Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.loaded_optimizer_states_dict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">loaded_optimizer_states_dict</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.loaded_optimizer_states_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.local_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">local_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.local_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process within a single node.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is as follows:</p>
<table class="colwidths-given table" id="id65">
<caption><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">*</span></code> also applies to the test loop</span><a class="headerlink" href="#id65" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>LightningModule Hook</p></th>
<th class="head"><p>on_step</p></th>
<th class="head"><p>on_epoch</p></th>
<th class="head"><p>prog_bar</p></th>
<th class="head"><p>logger</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>training_step</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>training_step_end</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>training_epoch_end</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_step*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>validation_step_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_epoch_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – key to log</p></li>
<li><p><strong>value</strong> – value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> – if True logs to the progress bar</p></li>
<li><p><strong>logger</strong> – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> – if True logs at this step. None auto-logs at the training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> – the ddp group to sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>metric_attribute</strong> – To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><strong>rank_zero_only</strong> – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log_dict">
<span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dictionary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]]]]) – key value pairs.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the progress base</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs at this step. None auto-logs for training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the ddp group sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>rank_zero_only</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log_grad_norm">
<span class="sig-name descname"><span class="pre">log_grad_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_norm_dict</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.log_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grad_norm_dict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]) – Dictionary containing current grad norm metrics</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.logger">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logger</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Reference to the logger object in the Trainer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.lr_schedulers">
<span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.lr_schedulers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.configure_optimizers"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.manual_backward">
<span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.manual_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <span class="xref std std-ref">manual optimization</span> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><strong>*args</strong> – Additional positional arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.model_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float"><span class="pre">float</span></a></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.model_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model size in MegaBytes (MB)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This property will not return correct value for Deepspeed (stage 3) and fully-sharded training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memo</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Set</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]) – a memo to store the set of modules already added to the result</p></li>
<li><p><strong>prefix</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – a prefix that will be added to the name of the module</p></li>
<li><p><strong>remove_duplicate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – whether to remove the duplicated module instances in the result
or not</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Module)</em> – Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all parameter names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_batch_transfer">
<span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_backward">
<span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_batch_transfer">
<span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_after_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_optimizer_step">
<span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>The hook is only called if gradients do not need to be accumulated.
See: <a href="#id41"><span class="problematic" id="id42">:paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`</span></a>.</p>
<p>If using native AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_zero_grad">
<span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_before_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The optimizer for which grads should be zeroed.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch ends.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_epoch_start">
<span class="sig-name descname"><span class="pre">on_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch begins.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_fit_end">
<span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_fit_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_fit_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_gpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">on_gpu</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p>
<p>Useful to set flags around the LightningModule for different CPU vs GPU behavior.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_hpc_load">
<span class="sig-name descname"><span class="pre">on_hpc_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_hpc_load" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager loads the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with variables from the checkpoint.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_hpc_save">
<span class="sig-name descname"><span class="pre">on_hpc_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_hpc_save" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager saves the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning to restore your model.
If you saved something with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_post_move_to_device">
<span class="sig-name descname"><span class="pre">on_post_move_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_post_move_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called. This is a good place to tie weights between
modules after moving them to a device. Can be used when training models with weight sharing properties on
TPU.</p>
<p>Addresses the handling of shared weights on TPU:
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks">https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The outputs of predict_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_batch_start">
<span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_dataloader">
<span class="sig-name descname"><span class="pre">on_predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the predict dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_predict_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_end">
<span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_epoch_end">
<span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_epoch_start">
<span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_model_eval">
<span class="sig-name descname"><span class="pre">on_predict_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the predict loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_start">
<span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_predict_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_pretrain_routine_end">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_pretrain_routine_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_pretrain_routine_start">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_pretrain_routine_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything
else you might want to save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of test_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_batch_start">
<span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_dataloader">
<span class="sig-name descname"><span class="pre">on_test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the test dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_end">
<span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_epoch_start">
<span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_model_eval">
<span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_model_train">
<span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_start">
<span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_test_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – The outputs of training_step_end(training_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_batch_start">
<span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_dataloader">
<span class="sig-name descname"><span class="pre">on_train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the train dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, either:</p>
<ol class="arabic simple">
<li><p>Implement <cite>training_epoch_end</cite> in the LightningModule OR</p></li>
<li><p>Cache data across steps on the attribute(s) of the <cite>LightningModule</cite> and access them in this hook</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_start">
<span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_train_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_val_dataloader">
<span class="sig-name descname"><span class="pre">on_val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the val dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_val_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of validation_step_end(validation_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_batch_start">
<span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_model_eval">
<span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_model_train">
<span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.on_validation_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_step">
<span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_tpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_native_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_lbfgs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to adjust the default way the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.
By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example
once per optimizer. This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) won’t be called during the
accumulation phase when <code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers, this indexes into that list.</p></li>
<li><p><strong>optimizer_closure</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Closure for all optimizers. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
<li><p><strong>on_tpu</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if TPU backward is required</p></li>
<li><p><strong>using_native_amp</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if using native amp</p></li>
<li><p><strong>using_lbfgs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True if the matching optimizer is <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

<span class="c1"># Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># update generator opt every step</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># update discriminator opt every 2 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
            <span class="n">optimizer_closure</span><span class="p">()</span>

    <span class="c1"># ...</span>
    <span class="c1"># add as many optimizers as you want</span>
</pre></div>
</div>
<p>Here’s another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">,</span>
    <span class="n">optimizer_closure</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># warm up lr</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_zero_grad">
<span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizer_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers this indexes into that list.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance.</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for the explanation of the above example.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizers">
<span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_pl_optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision and
profiling.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Parameter</em> – module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.precision" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_batch_size">
<span class="sig-name descname"><span class="pre">pred_batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_n">
<span class="sig-name descname"><span class="pre">pred_n</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_n" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_n_jobs">
<span class="sig-name descname"><span class="pre">pred_n_jobs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_n_jobs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_num_samples">
<span class="sig-name descname"><span class="pre">pred_num_samples</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_num_samples" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_roll_size">
<span class="sig-name descname"><span class="pre">pred_roll_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.pred_roll_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple prediction dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the prediction step</p>
<dl class="simple">
<dt>batch</dt><dd><p>output of Darts’ <code class="xref py py-class docutils literal notranslate"><span class="pre">InferenceDataset</span></code> - tuple of <code class="docutils literal notranslate"><span class="pre">(past_target,</span> <span class="pre">past_covariates,</span>
<span class="pre">historic_future_covariates,</span> <span class="pre">future_covariates,</span> <span class="pre">future_past_covariates,</span> <span class="pre">input_timeseries)</span></code></p>
</dd>
<dt>batch_idx</dt><dd><p>the batch index of the current batch</p>
</dd>
<dt>dataloader_idx</dt><dd><p>the dataloader index</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<a class="reference internal" href="darts.timeseries.html#darts.timeseries.TimeSeries" title="darts.timeseries.TimeSeries"><code class="xref py py-class docutils literal notranslate"><span class="pre">TimeSeries</span></code></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this to download and prepare data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <cite>setup</cite> instead)
since this is NOT called on every GPU in DDP/TPU</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> with the trainer flag is deprecated and will be removed in v1.7.0.
Please set <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> in LightningDataModule or LightningModule directly instead.</p>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data_per_node">
<span class="sig-name descname"><span class="pre">prepare_data_per_node</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data_per_node" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.print">
<span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.print" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – The thing to print. The same as for Python’s built-in print function.</p></li>
<li><p><strong>**kwargs</strong> – The same as for Python’s built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of <code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code> and
the behavior of this function will change in future versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the module’s state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em><em> or </em><em>None</em>) – buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations
that run on buffers, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
<li><p><strong>persistent</strong> (<em>bool</em>) – whether the buffer is part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module’s forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em><em> or </em><em>None</em>) – parameter to be added to the module. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<em>bool</em>) – whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters">
<span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.save_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><strong>ignore</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><strong>frame</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">frame</span></code>]) – a frame object. Default is None</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>dict</em>) – Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_predict_parameters">
<span class="sig-name descname"><span class="pre">set_predict_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roll_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.set_predict_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.share_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>~T</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.summarize">
<span class="sig-name descname"><span class="pre">summarize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.summarize" title="Permalink to this definition">¶</a></dt>
<dd><p>Summarize this LightningModule.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>pytorch_lightning.utilities.model_summary.summarize</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Can be either <code class="docutils literal notranslate"><span class="pre">'top'</span></code> (summarize only direct submodules) or <code class="docutils literal notranslate"><span class="pre">'full'</span></code> (summarize all layers).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.4: </span>This parameter was deprecated in v1.4 in favor of <cite>max_depth</cite> and will be removed in v1.6.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The maximum depth of layer nesting that the summary will include. A value of 0 turns the
layer summary off. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSummary</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The model summary object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.tbptt_split_batch">
<span class="sig-name descname"><span class="pre">tbptt_split_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.tbptt_split_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Current batch</p></li>
<li><p><strong>split_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The size of the split</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of batch splits. Each split will be passed to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> to enable truncated
back propagation through time. The default implementation splits root level Tensors and
Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">split_size</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                  <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>
        <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Called in the training loop after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_batch_start()</span></code>
if <a href="#id43"><span class="problematic" id="id44">:paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps`</span></a> &gt; 0.
Each returned batch split is passed separately to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.teardown">
<span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.teardown" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id45"><span class="problematic" id="id46">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_epoch_end">
<span class="sig-name descname"><span class="pre">test_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># do something with the outputs of all test batches</span>
    <span class="n">all_test_preds</span> <span class="o">=</span> <span class="n">test_step_outputs</span><span class="o">.</span><span class="n">predictions</span>

    <span class="n">some_result</span> <span class="o">=</span> <span class="n">calc_all_results</span><span class="p">(</span><span class="n">all_test_preds</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">some_result</span><span class="p">)</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">final_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">test_step_out</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="c1"># do something</span>
            <span class="n">final_value</span> <span class="o">+=</span> <span class="n">test_step_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step_end">
<span class="sig-name descname"><span class="pre">test_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">test_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT test_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with test_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_results</span><span class="p">):</span>
    <span class="c1"># this out is now the full size of the batch</span>
    <span class="n">all_test_step_outs</span> <span class="o">=</span> <span class="n">output_results</span><span class="o">.</span><span class="n">out</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">all_test_step_outs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> s. In addition, this method will
only cast the floating point parameters and buffers to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.device" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ExampleModule</span><span class="p">(</span><span class="n">DeviceDtypeModuleMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">ExampleModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float16</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – The desired device of the parameters
and buffers in this module.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>]) – The path of the file the onnx model should be saved to.</p></li>
<li><p><strong>input_sample</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><strong>**kwargs</strong> – Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_torchscript">
<span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.to_torchscript" title="Permalink to this definition">¶</a></dt>
<dd><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a> set. If you would like to customize the modules that
are scripted you should override this method. In case you want to return multiple modules, we recommend
using a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><strong>method</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use TorchScript’s script or trace method. Default: ‘script’</p></li>
<li><p><strong>example_inputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input to be used to do tracing when method is set to ‘trace’.
Default: None (uses <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a>)</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments that will be passed to the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code>
documentation for supported features.</p></li>
</ul>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(),</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> 
<span class="gp">... </span>                                    <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">)</span>  
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.toggle_optimizer">
<span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.toggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes sure only the gradients of the current optimizer’s parameters are calculated in the training step
to prevent dangling gradients in multiple-optimizer setup.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.
It works with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – The optimizer to toggle.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to toggle.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>bool</em>) – whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">page</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id47"><span class="problematic" id="id48">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_epoch_end">
<span class="sig-name descname"><span class="pre">training_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the training epoch with the outputs of all training steps. Use this in case you
need to do something with all the outputs returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">train_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">train_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">train_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.
If there are multiple optimizers, it is a list containing a list of outputs for each optimizer.
If using <code class="docutils literal notranslate"><span class="pre">truncated_bptt_steps</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, each element is a list of outputs corresponding to the outputs
of each processed split batch.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this method is not overridden, this won’t be called.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="c1"># do something with all training_step outputs</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the training step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step_end">
<span class="sig-name descname"><span class="pre">training_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">training_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <cite>training_step</cite> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Anything</p>
</dd>
</dl>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># softmax uses only a portion of the batch in the denominator</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">out</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="n">gpu_0_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_1_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_n_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>

    <span class="c1"># this softmax now uses the full batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">([</span><span class="n">gpu_0_pred</span><span class="p">,</span> <span class="n">gpu_1_pred</span><span class="p">,</span> <span class="n">gpu_n_pred</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.transfer_batch_to_device">
<span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.transfer_batch_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom
data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or anything that implements <cite>.to(…)</cite></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torchtext.data.batch.Batch</span></code></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, …).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be transferred to a new device.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>) – The target device as defined in PyTorch.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.truncated_bptt_steps">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">truncated_bptt_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.truncated_bptt_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p>
<p>It represents
the number of times <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> gets called before backpropagation. If this is &gt; 0, the
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> receives an additional argument <code class="docutils literal notranslate"><span class="pre">hiddens</span></code> and is expected to return a hidden state.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<em>type</em><em> or </em><em>string</em>) – the desired type</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.untoggle_optimizer">
<span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.untoggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to untoggle.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.use_amp">
<span class="sig-name descname"><span class="pre">use_amp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.use_amp" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id49"><span class="problematic" id="id50">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_epoch_end">
<span class="sig-name descname"><span class="pre">validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dataloader_output_result</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">dataloader_outs</span> <span class="o">=</span> <span class="n">dataloader_output_result</span><span class="o">.</span><span class="n">dataloader_i_outputs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the validation step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step_end">
<span class="sig-name descname"><span class="pre">validation_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">validation_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT validation_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with validation_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.xpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLPastCovariatesModule.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero. See similar function
under <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> for more context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for details.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">darts.models.forecasting.pl_forecasting_module.</span></span><span class="sig-name descname"><span class="pre">PLSplitCovariatesModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_chunk_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn=MSELoss()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">likelihood=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_cls=&lt;class</span> <span class="pre">'torch.optim.adam.Adam'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_kwargs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_cls=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_kwargs=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/darts/models/forecasting/pl_forecasting_module.html#PLSplitCovariatesModule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLForecastingModule" title="darts.models.forecasting.pl_forecasting_module.PLForecastingModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">darts.models.forecasting.pl_forecasting_module.PLForecastingModule</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>PyTorch Lightning-based Forecasting Module.</p>
<p>This class is meant to be inherited to create a new PyTorch Lightning-based forecasting module.
When subclassing this class, please make sure to add the following methods with the given signatures:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.__init__()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel.forward()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._produce_train_output()</span></code></p></li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">PLTorchForecastingModel._get_batch_prediction()</span></code></p></li>
</ul>
</div></blockquote>
<p>In subclass <cite>MyModel</cite>’s <code class="xref py py-func docutils literal notranslate"><span class="pre">__init__()</span></code> function call <code class="docutils literal notranslate"><span class="pre">super(MyModel,</span> <span class="pre">self).__init__(**kwargs)</span></code> where
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> are the parameters of <code class="xref py py-class docutils literal notranslate"><span class="pre">PLTorchForecastingModel</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of input past time steps per chunk.</p></li>
<li><p><strong>output_chunk_length</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Number of output time steps per chunk.</p></li>
<li><p><strong>loss_fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">_Loss</span></code>) – PyTorch loss function used for training.
This parameter will be ignored for probabilistic models if the <code class="docutils literal notranslate"><span class="pre">likelihood</span></code> parameter is specified.
Default: <code class="docutils literal notranslate"><span class="pre">torch.nn.MSELoss()</span></code>.</p></li>
<li><p><strong>likelihood</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-class docutils literal notranslate"><span class="pre">Likelihood</span></code></a>]) – One of Darts’ <a class="reference internal" href="darts.utils.likelihood_models.html#darts.utils.likelihood_models.Likelihood" title="darts.utils.likelihood_models.Likelihood"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Likelihood</span></code></a> models to be used for
probabilistic forecasts. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>optimizer_cls</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The PyTorch optimizer class to be used. Default: <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam</span></code>.</p></li>
<li><p><strong>optimizer_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch optimizer (e.g., <code class="docutils literal notranslate"><span class="pre">{'lr':</span> <span class="pre">1e-3}</span></code>
for specifying a learning rate). Otherwise the default values of the selected <code class="docutils literal notranslate"><span class="pre">optimizer_cls</span></code>
will be used. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_cls</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">_LRScheduler</span></code>]) – Optionally, the PyTorch learning rate scheduler class to be used. Specifying <code class="docutils literal notranslate"><span class="pre">None</span></code> corresponds
to using a constant learning rate. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>lr_scheduler_kwargs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>]) – Optionally, some keyword arguments for the PyTorch learning rate scheduler. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Attributes</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.automatic_optimization" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.automatic_optimization"><code class="xref py py-obj docutils literal notranslate"><span class="pre">automatic_optimization</span></code></a></p></td>
<td><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.current_epoch" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.current_epoch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">current_epoch</span></code></a></p></td>
<td><p>The current epoch in the Trainer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.example_input_array"><code class="xref py py-obj docutils literal notranslate"><span class="pre">example_input_array</span></code></a></p></td>
<td><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.global_rank" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.global_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_rank</span></code></a></p></td>
<td><p>The index of the current process across all nodes and devices.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.global_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.global_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">global_step</span></code></a></p></td>
<td><p>Total training batches seen across all epochs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams_initial"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hparams_initial</span></code></a></p></td>
<td><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.local_rank" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.local_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">local_rank</span></code></a></p></td>
<td><p>The index of the current process within a single node.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.logger" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.logger"><code class="xref py py-obj docutils literal notranslate"><span class="pre">logger</span></code></a></p></td>
<td><p>Reference to the logger object in the Trainer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.model_size" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.model_size"><code class="xref py py-obj docutils literal notranslate"><span class="pre">model_size</span></code></a></p></td>
<td><p>Returns the model size in MegaBytes (MB)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_gpu" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_gpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_gpu</span></code></a></p></td>
<td><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.truncated_bptt_steps" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.truncated_bptt_steps"><code class="xref py py-obj docutils literal notranslate"><span class="pre">truncated_bptt_steps</span></code></a></p></td>
<td><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 76%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>device</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>dtype</strong></p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p><strong>epochs_trained</strong></p></td>
<td></td>
</tr>
<tr class="row-even"><td><p><strong>loaded_optimizer_states_dict</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_module</span></code></a>(name, module)</p></td>
<td><p>Adds a child module to the current module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_to_queue" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_to_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">add_to_queue</span></code></a>(queue)</p></td>
<td><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.all_gather" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.all_gather"><code class="xref py py-obj docutils literal notranslate"><span class="pre">all_gather</span></code></a>(data[, group, sync_grads])</p></td>
<td><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation accelerator agnostic.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.apply" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.apply"><code class="xref py py-obj docutils literal notranslate"><span class="pre">apply</span></code></a>(fn)</p></td>
<td><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.backward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">backward</span></code></a>(loss, optimizer, optimizer_idx, ...)</p></td>
<td><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.bfloat16" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.bfloat16"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bfloat16</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.buffers" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">buffers</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module buffers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.children" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.clip_gradients" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.clip_gradients"><code class="xref py py-obj docutils literal notranslate"><span class="pre">clip_gradients</span></code></a>(optimizer[, ...])</p></td>
<td><p>Handles gradient clipping internally.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_callbacks" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_callbacks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_callbacks</span></code></a>()</p></td>
<td><p>Configure model-specific callbacks.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_gradient_clipping"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_gradient_clipping</span></code></a>(optimizer, ...)</p></td>
<td><p>Perform gradient clipping for the optimizer parameters.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_optimizers</span></code></a>()</p></td>
<td><p>configures optimizers and learning rate schedulers for for model optimization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_sharded_model" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_sharded_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">configure_sharded_model</span></code></a>()</p></td>
<td><p>Hook to create modules in a distributed aware context.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cpu" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cpu</span></code></a>()</p></td>
<td><p>Moves all model parameters and buffers to the CPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cuda"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cuda</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the GPU.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.double" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.double"><code class="xref py py-obj docutils literal notranslate"><span class="pre">double</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.eval" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">eval</span></code></a>()</p></td>
<td><p>Sets the module in evaluation mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.extra_repr" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.extra_repr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">extra_repr</span></code></a>()</p></td>
<td><p>Set the extra representation of the module</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float"><code class="xref py py-obj docutils literal notranslate"><span class="pre">float</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">forward</span></code></a>(*args, **kwargs)</p></td>
<td><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.freeze" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.freeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">freeze</span></code></a>()</p></td>
<td><p>Freeze all params for inference.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_buffer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_buffer</span></code></a>(target)</p></td>
<td><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_extra_state</span></code></a>()</p></td>
<td><p>Returns any extra state to include in the module's state_dict.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_from_queue" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_from_queue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_from_queue</span></code></a>(queue)</p></td>
<td><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_parameter" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_parameter</span></code></a>(target)</p></td>
<td><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_progress_bar_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_progress_bar_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_progress_bar_dict</span></code></a>()</p></td>
<td><p><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5.</span></p>
</div>
</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_submodule" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_submodule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_submodule</span></code></a>(target)</p></td>
<td><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throws an error.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.half" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.half"><code class="xref py py-obj docutils literal notranslate"><span class="pre">half</span></code></a>()</p></td>
<td><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_from_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_from_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code></a>(checkpoint_path[, ...])</p></td>
<td><p>Primary way of loading a model from a checkpoint.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_state_dict</span></code></a>(state_dict[, strict])</p></td>
<td><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log</span></code></a>(name, value[, prog_bar, logger, ...])</p></td>
<td><p>Log a key, value pair.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_dict</span></code></a>(dictionary[, prog_bar, logger, ...])</p></td>
<td><p>Log a dictionary of values at once.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log_grad_norm" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log_grad_norm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">log_grad_norm</span></code></a>(grad_norm_dict)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.lr_schedulers" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.lr_schedulers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lr_schedulers</span></code></a>()</p></td>
<td><p>Returns the learning rate scheduler(s) that are being used during training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.manual_backward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.manual_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">manual_backward</span></code></a>(loss, *args, **kwargs)</p></td>
<td><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.modules" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">modules</span></code></a>()</p></td>
<td><p>Returns an iterator over all modules in the network.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_buffers" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_buffers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_buffers</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_children" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_children"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_children</span></code></a>()</p></td>
<td><p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_modules" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_modules"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_modules</span></code></a>([memo, prefix, remove_duplicate])</p></td>
<td><p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_parameters" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">named_parameters</span></code></a>([prefix, recurse])</p></td>
<td><p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_backward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_backward</span></code></a>()</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_after_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_backward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_backward"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_backward</span></code></a>(loss)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_batch_transfer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_batch_transfer</span></code></a>(batch, dataloader_idx)</p></td>
<td><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code></a>(optimizer, ...)</p></td>
<td><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_before_zero_grad</span></code></a>(optimizer)</p></td>
<td><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_end</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch ends.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_epoch_start</span></code></a>()</p></td>
<td><p>Called when either of train/val/test epoch begins.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_fit_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_fit_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_end</span></code></a>()</p></td>
<td><p>Called at the very end of fit.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_fit_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_fit_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_fit_start</span></code></a>()</p></td>
<td><p>Called at the very beginning of fit.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_hpc_load" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_hpc_load"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_load</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager loads the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_hpc_save" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_hpc_save"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_hpc_save</span></code></a>(checkpoint)</p></td>
<td><p>Hook to do whatever you need right before Slurm manager saves the model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_load_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_load_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_load_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning to restore your model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_post_move_to_device" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_post_move_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_post_move_to_device</span></code></a>()</p></td>
<td><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after <code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the predict loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the predict loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the predict dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_end</span></code></a>()</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_end</span></code></a>(results)</p></td>
<td><p>Called at the end of predicting.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_epoch_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the predict loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_predict_start</span></code></a>()</p></td>
<td><p>Called at the beginning of predicting.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_pretrain_routine_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_pretrain_routine_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_end</span></code></a>()</p></td>
<td><p>Called at the end of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_pretrain_routine_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_pretrain_routine_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_pretrain_routine_start</span></code></a>()</p></td>
<td><p>Called at the beginning of the pretrain routine (between fit and train start).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_save_checkpoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_save_checkpoint</span></code></a>(checkpoint)</p></td>
<td><p>Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_end</span></code></a>(outputs, batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop after the batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the test loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the test dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_end</span></code></a>()</p></td>
<td><p>Called at the end of testing.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_end</span></code></a>()</p></td>
<td><p>Called in the test loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_epoch_start</span></code></a>()</p></td>
<td><p>Called in the test loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the test loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_model_train" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the test loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_test_start</span></code></a>()</p></td>
<td><p>Called at the beginning of testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_end</span></code></a>(outputs, batch, batch_idx)</p></td>
<td><p>Called in the training loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_batch_start</span></code></a>(batch, batch_idx[, unused])</p></td>
<td><p>Called in the training loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the train dataloader.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_end</span></code></a>()</p></td>
<td><p>Called at the end of training before logger experiment is closed.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_end</span></code></a>()</p></td>
<td><p>Called in the training loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_epoch_start</span></code></a>()</p></td>
<td><p>Called in the training loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_train_start</span></code></a>()</p></td>
<td><p>Called at the beginning of training after sanity check.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_val_dataloader</span></code></a>()</p></td>
<td><p>Called before requesting the val dataloader.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_batch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_batch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_end</span></code></a>(outputs, batch, ...)</p></td>
<td><p>Called in the validation loop after the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_batch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_batch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_batch_start</span></code></a>(batch, batch_idx, ...)</p></td>
<td><p>Called in the validation loop before anything happens for that batch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_end</span></code></a>()</p></td>
<td><p>Called at the end of validation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_end</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very end of the epoch.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_epoch_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_epoch_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_epoch_start</span></code></a>()</p></td>
<td><p>Called in the validation loop at the very beginning of the epoch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_model_eval" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_model_eval"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_eval</span></code></a>()</p></td>
<td><p>Sets the model to eval during the val loop.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_model_train" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_model_train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_model_train</span></code></a>()</p></td>
<td><p>Sets the model to train during the val loop.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_start" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_start"><code class="xref py py-obj docutils literal notranslate"><span class="pre">on_validation_start</span></code></a>()</p></td>
<td><p>Called at the beginning of validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_step</span></code></a>(epoch, batch_idx, optimizer)</p></td>
<td><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizer_zero_grad</span></code></a>(epoch, batch_idx, ...)</p></td>
<td><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizers" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizers"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optimizers</span></code></a>([use_pl_optimizer])</p></td>
<td><p>Returns the optimizer(s) that are being used during training.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.parameters" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">parameters</span></code></a>([recurse])</p></td>
<td><p>Returns an iterator over module parameters.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for prediction.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict_step</span></code></a>(batch, batch_idx[, dataloader_idx])</p></td>
<td><p>performs the prediction step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">prepare_data</span></code></a>()</p></td>
<td><p>Use this to download and prepare data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.print" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.print"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print</span></code></a>(*args, **kwargs)</p></td>
<td><p>Prints only from process 0.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_buffer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_buffer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_buffer</span></code></a>(name, tensor[, persistent])</p></td>
<td><p>Adds a buffer to the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_forward_hook" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_forward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_forward_pre_hook" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_forward_pre_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_forward_pre_hook</span></code></a>(hook)</p></td>
<td><p>Registers a forward pre-hook on the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_full_backward_hook" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_full_backward_hook"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_full_backward_hook</span></code></a>(hook)</p></td>
<td><p>Registers a backward hook on the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_module" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_module"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_module</span></code></a>(name, module)</p></td>
<td><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_parameter" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_parameter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">register_parameter</span></code></a>(name, param)</p></td>
<td><p>Adds a parameter to the module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.requires_grad_" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.requires_grad_"><code class="xref py py-obj docutils literal notranslate"><span class="pre">requires_grad_</span></code></a>([requires_grad])</p></td>
<td><p>Change if autograd should record operations on parameters in this module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_hyperparameters</span></code></a>(*args[, ignore, frame, ...])</p></td>
<td><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_extra_state"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_extra_state</span></code></a>(state)</p></td>
<td><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state found within the <cite>state_dict</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_predict_parameters" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_predict_parameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_predict_parameters</span></code></a>(n, num_samples, ...)</p></td>
<td><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">setup</span></code></a>([stage])</p></td>
<td><p>Called at the beginning of fit (train + validate), validate, test, and predict.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.share_memory" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.share_memory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">share_memory</span></code></a>()</p></td>
<td><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">state_dict</span></code></a>([destination, prefix, keep_vars])</p></td>
<td><p>Returns a dictionary containing a whole state of the module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.summarize" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.summarize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">summarize</span></code></a>([mode, max_depth])</p></td>
<td><p>Summarize this LightningModule.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.tbptt_split_batch" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.tbptt_split_batch"><code class="xref py py-obj docutils literal notranslate"><span class="pre">tbptt_split_batch</span></code></a>(batch, split_size)</p></td>
<td><p>When using truncated backpropagation through time, each batch must be split along the time dimension.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.teardown" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.teardown"><code class="xref py py-obj docutils literal notranslate"><span class="pre">teardown</span></code></a>([stage])</p></td>
<td><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for testing.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of a test epoch with the output of all test steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step</span></code></a>(*args, **kwargs)</p></td>
<td><p>Operates on a single batch of data from the test set.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">test_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to</span></code></a>(*args, **kwargs)</p></td>
<td><p>Moves and/or casts the parameters and buffers.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_empty" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_empty"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_empty</span></code></a>(*, device)</p></td>
<td><p>Moves the parameters and buffers to the specified device without copying storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_onnx" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_onnx"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_onnx</span></code></a>(file_path[, input_sample])</p></td>
<td><p>Saves the model in ONNX format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_torchscript" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_torchscript"><code class="xref py py-obj docutils literal notranslate"><span class="pre">to_torchscript</span></code></a>([file_path, method, ...])</p></td>
<td><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.toggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">toggle_optimizer</span></code></a>(optimizer, optimizer_idx)</p></td>
<td><p>Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train</span></code></a>([mode])</p></td>
<td><p>Sets the module in training mode.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">train_dataloader</span></code></a>()</p></td>
<td><p>Implement one or more PyTorch DataLoaders for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the training epoch with the outputs of all training steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step</span></code></a>(train_batch, batch_idx)</p></td>
<td><p>performs the training step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">training_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.transfer_batch_to_device"><code class="xref py py-obj docutils literal notranslate"><span class="pre">transfer_batch_to_device</span></code></a>(batch, device, ...)</p></td>
<td><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom data structure.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.type" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.type"><code class="xref py py-obj docutils literal notranslate"><span class="pre">type</span></code></a>(dst_type)</p></td>
<td><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.unfreeze" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.unfreeze"><code class="xref py py-obj docutils literal notranslate"><span class="pre">unfreeze</span></code></a>()</p></td>
<td><p>Unfreeze all parameters for training.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.untoggle_optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">untoggle_optimizer</span></code></a>(optimizer_idx)</p></td>
<td><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">val_dataloader</span></code></a>()</p></td>
<td><p>Implement one or multiple PyTorch DataLoaders for validation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_epoch_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_epoch_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_epoch_end</span></code></a>(outputs)</p></td>
<td><p>Called at the end of the validation epoch with the outputs of all validation steps.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step</span></code></a>(val_batch, batch_idx)</p></td>
<td><p>performs the validation step</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step_end"><code class="xref py py-obj docutils literal notranslate"><span class="pre">validation_step_end</span></code></a>(*args, **kwargs)</p></td>
<td><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of the batch.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.xpu" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.xpu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">xpu</span></code></a>([device])</p></td>
<td><p>Moves all model parameters and buffers to the XPU.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.zero_grad" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.zero_grad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_grad</span></code></a>([set_to_none])</p></td>
<td><p>Sets gradients of all model parameters to zero.</p></td>
</tr>
</tbody>
</table>
<table class="table">
<colgroup>
<col style="width: 55%" />
<col style="width: 45%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>__call__</strong></p></td>
<td></td>
</tr>
</tbody>
</table>
<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_KEY</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hyper_parameters'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.CHECKPOINT_HYPER_PARAMS_KEY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_NAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_name'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.CHECKPOINT_HYPER_PARAMS_NAME" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE">
<span class="sig-name descname"><span class="pre">CHECKPOINT_HYPER_PARAMS_TYPE</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'hparams_type'</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.CHECKPOINT_HYPER_PARAMS_TYPE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.T_destination">
<span class="sig-name descname"><span class="pre">T_destination</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.T_destination" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of TypeVar(‘T_destination’, bound=<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code>])</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<em>Module</em>) – child module to be added to the module.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_to_queue">
<span class="sig-name descname"><span class="pre">add_to_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_to_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary to the given queue. To avoid issues with memory
sharing, we cast the data to numpy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue to append the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.add_to_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.all_gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows users to call <code class="docutils literal notranslate"><span class="pre">self.all_gather()</span></code> from the LightningModule, thus making the <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> operation
accelerator agnostic. <code class="docutils literal notranslate"><span class="pre">all_gather</span></code> is a function provided by accelerators to gather a tensor from several
distributed processes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>]) – int, float, tensor of shape (batch, …), or a (possibly nested) collection thereof.</p></li>
<li><p><strong>group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><strong>sync_grads</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, …), or if the input was a collection
the output will also be a collection with tensors of this shape.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.allow_zero_length_dataloader_with_multiple_devices">
<span class="sig-name descname"><span class="pre">allow_zero_length_dataloader_with_multiple_devices</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.allow_zero_length_dataloader_with_multiple_devices" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Applies <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>)
as well as self. Typical use includes initializing the parameters of a model
(see also <span class="xref std std-ref">nn-init-doc</span>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> -&gt; None) – function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[ 1.,  1.],</span>
<span class="go">        [ 1.,  1.]])</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.automatic_optimization">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">automatic_optimization</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.automatic_optimization" title="Permalink to this definition">¶</a></dt>
<dd><p>If set to <code class="docutils literal notranslate"><span class="pre">False</span></code> you are responsible for calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, <code class="docutils literal notranslate"><span class="pre">.step()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called to perform backward on the loss returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. Override this hook with your
own implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The loss tensor returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>]) – Current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Index of the current optimizer being used. <code class="docutils literal notranslate"><span class="pre">None</span></code> if using manual optimization.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.bfloat16" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>torch.Tensor</em> – module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a child module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.clip_gradients">
<span class="sig-name descname"><span class="pre">clip_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.clip_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Handles gradient clipping internally.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Do not override this method. If you want to customize gradient clipping, consider
using <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_gradient_clipping" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_gradient_clipping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_gradient_clipping()</span></code></a> method.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_callbacks">
<span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_callbacks" title="Permalink to this definition">¶</a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code>
gets called, the list returned here will be merged with the list of callbacks passed to the Trainer’s
<code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks already
present in the Trainer’s callbacks list, it will take priority and replace them. In addition, Lightning
will make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Certain callback methods like <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_init_start()</span></code>
will never be invoked on the new callbacks returned here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_gradient_clipping">
<span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_gradient_clipping" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The value at which to clip gradients. By default value passed in Trainer
will be available here.</p></li>
<li><p><strong>gradient_clip_algorithm</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – The gradient clipping algorithm to use. By default value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN</span>
<span class="k">def</span> <span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Lightning will handle the gradient clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
            <span class="n">optimizer</span><span class="p">,</span>
            <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
            <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># implement your own custom logic to clip gradients for generator (optimizer_idx=0)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>configures optimizers and learning rate schedulers for for model optimization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_sharded_model">
<span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_sharded_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to create modules in a distributed aware context. This is useful for when using sharded plugins,
where we’d like to shard the model instantly, which is useful for extremely large models which can save
memory and initialization time.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is idempotent.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
different objects. So it should be called before constructing optimizer if the module will live on GPU
while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.current_epoch">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">current_epoch</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.current_epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The current epoch in the Trainer.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.device</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.device" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.double" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">double</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.dtype">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">dtype</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.dtype</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.dtype" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">dtype</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.dump_patches">
<span class="sig-name descname"><span class="pre">dump_patches</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.dump_patches" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows better BC support for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a>. In
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a>, the version number will be saved as in the attribute
<cite>_metadata</cite> of the returned state dict, and thus pickled. <cite>_metadata</cite> is a
dictionary with keys that follow the naming convention of state dict. See
<code class="docutils literal notranslate"><span class="pre">_load_from_state_dict</span></code> on how to use this information in loading.</p>
<p>If new parameters/buffers are added/removed from a module, this number shall
be bumped, and the module’s <cite>_load_from_state_dict</cite> method can compare the
version number and do appropriate changes if the state dict is from before
the change.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.epochs_trained">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">epochs_trained</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.epochs_trained" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in evaluation mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code>.</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.example_input_array">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">example_input_array</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Any</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.example_input_array" title="Permalink to this definition">¶</a></dt>
<dd><p>The example input array is a specification of what the module can consume in the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method.
The return type is interpreted as follows:</p>
<ul class="simple">
<li><p>Single tensor: It is assumed the model takes a single argument, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(model.example_input_array)</span></code></p></li>
<li><p>Tuple: The input array should be interpreted as a sequence of positional arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(*model.example_input_array)</span></code></p></li>
<li><p>Dict: The input array represents named keyword arguments, i.e.,
<code class="docutils literal notranslate"><span class="pre">model.forward(**model.example_input_array)</span></code></p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.extra_repr" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">float</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Your model’s output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the buffer
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not a
buffer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns any extra state to include in the module’s state_dict.
Implement this and a corresponding <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
module’s <cite>state_dict()</cite>.</p>
<p>Note that extra state should be pickleable to ensure working serialization
of the state_dict. We only provide provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Any extra state to store in the module’s state_dict</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_from_queue">
<span class="sig-name descname"><span class="pre">get_from_queue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">queue</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieve the <code class="xref py py-attr docutils literal notranslate"><span class="pre">trainer.callback_metrics</span></code> dictionary from the given queue. To preserve consistency,
we cast back the data to <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>queue</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleQueue</span></code>) – the instance of the queue from where to get the data.</p>
</dd>
</dl>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>DDPSpawnPlugin.get_from_queue</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this method’s functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the Parameter
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Parameter</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_progress_bar_dict">
<span class="sig-name descname"><span class="pre">get_progress_bar_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_progress_bar_dict" title="Permalink to this definition">¶</a></dt>
<dd><div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of
<cite>pytorch_lightning.callbacks.progress.base.get_metrics</cite> and will be removed in v1.7.</p>
</div>
<p>Implement this to override the default items displayed in the progress bar.
By default it includes the average loss value, split index of BPTT (if used)
and the version of the experiment when using a logger.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Epoch 1:   4%|▎         | 40/1095 [00:03&lt;01:37, 10.84it/s, loss=4.501, v_num=10]
</pre></div>
</div>
<p>Here is an example how to override the defaults:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_progress_bar_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># don&#39;t show the version number</span>
    <span class="n">items</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_progress_bar_dict</span><span class="p">()</span>
    <span class="n">items</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;v_num&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">items</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary with the items to be displayed in the progress bar.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_submodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists,
otherwise throws an error.</p>
<p>For example, let’s say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.nn.Module</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the target string references an invalid
path or resolves to something that is not an
<code class="docutils literal notranslate"><span class="pre">nn.Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.global_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.global_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process across all nodes and devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.global_step">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">global_step</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.global_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Total training batches seen across all epochs.</p>
<p>If no Trainer is attached, this propery is 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.half" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">half</span></code> datatype.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">argparse.Namespace</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. It is mutable by the user.
For the frozen set of initial hyperparameters, use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams_initial" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams_initial"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams_initial</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>mutable hyperparameters dicionary</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Union[AttributeDict, dict, Namespace]</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams_initial">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">hparams_initial</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">pytorch_lightning.utilities.parsing.AttributeDict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams_initial" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of hyperparameters saved with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save_hyperparameters()</span></code></a>. These contents are read-only.
Manual updates to the saved hyperparameters can instead be performed through <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.hparams"><code class="xref py py-attr docutils literal notranslate"><span class="pre">hparams</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>immutable initial hyperparameters</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>AttributeDict</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_from_checkpoint">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_from_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint
it stores the arguments passed to <cite>__init__</cite>  in the checkpoint under <cite>hyper_parameters</cite></p>
<p>Any arguments specified through *args and **kwargs will override args stored in <cite>hyper_parameters</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">IO</span></code>]) – Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><strong>map_location</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code>.</p></li>
<li><p><strong>hparams_file</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Optional path to a .yaml file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely won’t need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights don’t have the hyperparameters saved,
use this method to pass in a .yaml file with the hparams you’d like to use.
These will be converted into a <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your model’s <cite>hparams</cite> argument is <code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code>
and .yaml file has hierarchical structure, you need to refactor your model to treat
<cite>hparams</cite> as <code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code>.</p>
</p></li>
<li><p><strong>strict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this module’s state dict. Default: <cite>True</cite>.</p></li>
<li><p><strong>kwargs</strong> – Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.loaded_optimizer_states_dict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">loaded_optimizer_states_dict</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">dict</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.loaded_optimizer_states_dict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.local_rank">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">local_rank</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.local_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>The index of the current process within a single node.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is as follows:</p>
<table class="colwidths-given table" id="id66">
<caption><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">*</span></code> also applies to the test loop</span><a class="headerlink" href="#id66" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>LightningModule Hook</p></th>
<th class="head"><p>on_step</p></th>
<th class="head"><p>on_epoch</p></th>
<th class="head"><p>prog_bar</p></th>
<th class="head"><p>logger</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>training_step</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>training_step_end</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>training_epoch_end</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_step*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-even"><td><p>validation_step_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
<tr class="row-odd"><td><p>validation_epoch_end*</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
<td><p>F</p></td>
<td><p>T</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> – key to log</p></li>
<li><p><strong>value</strong> – value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> – if True logs to the progress bar</p></li>
<li><p><strong>logger</strong> – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> – if True logs at this step. None auto-logs at the training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> – if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> – the ddp group to sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>metric_attribute</strong> – To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><strong>rank_zero_only</strong> – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log_dict">
<span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_reduce_fx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tbptt_pad_token</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_op</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dictionary</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Mapping</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Metric</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]]]]) – key value pairs.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or a dictionary of the former.</p></li>
<li><p><strong>prog_bar</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the progress base</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True logs to the logger</p></li>
<li><p><strong>on_step</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs at this step. None auto-logs for training_step but not validation/test_step</p></li>
<li><p><strong>on_epoch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step</p></li>
<li><p><strong>reduce_fx</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>]) – reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, will not auto detach the graph</p></li>
<li><p><strong>sync_dist</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – the ddp group sync across</p></li>
<li><p><strong>add_dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – if True, appends the index of the current dataloader to
the name (when using multiple). If False, user needs to give unique names for
each dataloader to not mix values</p></li>
<li><p><strong>batch_size</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Current batch_size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>rank_zero_only</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>]) – Whether the value will be logged only on rank 0. This will prevent synchronization which
would produce a deadlock as not all processes would perform this log call.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log_grad_norm">
<span class="sig-name descname"><span class="pre">log_grad_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grad_norm_dict</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.log_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">log_grad_norm</span></code>.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grad_norm_dict</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]) – Dictionary containing current grad norm metrics</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">log_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_norm_dict</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">grad_norm_dict</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.logger">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">logger</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Reference to the logger object in the Trainer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.lr_schedulers">
<span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.lr_schedulers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_optimizers" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.configure_optimizers"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.manual_backward">
<span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.manual_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Call this directly from your <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <span class="xref std std-ref">manual optimization</span> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><strong>*args</strong> – Additional positional arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments to be forwarded to <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.model_size">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">model_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float"><span class="pre">float</span></a></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.model_size" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the model size in MegaBytes (MB)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This property will not return correct value for Deepspeed (stage 3) and fully-sharded training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.float"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>Module</em> – a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_buffers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module buffers, yielding both the
name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_children" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over immediate children modules, yielding both
the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields</dt>
<dd class="field-odd"><p><em>(string, Module)</em> – Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over all modules in the network, yielding
both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memo</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Set</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>]]) – a memo to store the set of modules already added to the result</p></li>
<li><p><strong>prefix</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>) – a prefix that will be added to the name of the module</p></li>
<li><p><strong>remove_duplicate</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – whether to remove the duplicated module instances in the result
or not</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Module)</em> – Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="go">        print(idx, &#39;-&gt;&#39;, m)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.named_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters, yielding both the
name of the parameter as well as the parameter itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<em>str</em>) – prefix to prepend to all parameter names.</p></li>
<li><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">print</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_batch_transfer">
<span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_backward">
<span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) – Loss divided by number of batches for gradient accumulation and scaled if using native AMP.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_batch_transfer">
<span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_batch_transfer" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_batch_transfer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_after_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.transfer_batch_to_device" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_optimizer_step">
<span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>The hook is only called if gradients do not need to be accumulated.
See: <a href="#id51"><span class="problematic" id="id52">:paramref:`~pytorch_lightning.trainer.Trainer.accumulate_grad_batches`</span></a>.</p>
<p>If using native AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – Current optimizer being used.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of the current optimizer being used.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_zero_grad">
<span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_before_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – The optimizer for which grads should be zeroed.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_epoch_end">
<span class="sig-name descname"><span class="pre">on_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch ends.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_epoch_start">
<span class="sig-name descname"><span class="pre">on_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when either of train/val/test epoch begins.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_fit_end">
<span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_fit_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_fit_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_gpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">on_gpu</span></span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_gpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">True</span></code> if this model is currently located on a GPU.</p>
<p>Useful to set flags around the LightningModule for different CPU vs GPU behavior.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_hpc_load">
<span class="sig-name descname"><span class="pre">on_hpc_load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_hpc_load" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager loads the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary with variables from the checkpoint.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_hpc_save">
<span class="sig-name descname"><span class="pre">on_hpc_save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_hpc_save" title="Permalink to this definition">¶</a></dt>
<dd><p>Hook to do whatever you need right before Slurm manager saves the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – A dictionary in which you can save variables to save in a checkpoint.
Contents need to be pickleable.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning to restore your model.
If you saved something with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_save_checkpoint" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_post_move_to_device">
<span class="sig-name descname"><span class="pre">on_post_move_to_device</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_post_move_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the <code class="docutils literal notranslate"><span class="pre">parameter_validation</span></code> decorator after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">to()</span></code> is called. This is a good place to tie weights between
modules after moving them to a device. Can be used when training models with weight sharing properties on
TPU.</p>
<p>Addresses the handling of shared weights on TPU:
<a class="reference external" href="https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks">https://github.com/pytorch/xla/blob/master/TROUBLESHOOTING.md#xla-tensor-quirks</a></p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_post_move_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">weight</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The outputs of predict_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_batch_start">
<span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_dataloader">
<span class="sig-name descname"><span class="pre">on_predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the predict dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_predict_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_end">
<span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_epoch_end">
<span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">results</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_epoch_start">
<span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_model_eval">
<span class="sig-name descname"><span class="pre">on_predict_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the predict loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_start">
<span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_predict_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of predicting.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_pretrain_routine_end">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_pretrain_routine_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_pretrain_routine_start">
<span class="sig-name descname"><span class="pre">on_pretrain_routine_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_pretrain_routine_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of the pretrain routine (between fit and train start).</p>
<ul class="simple">
<li><p>fit</p></li>
<li><p>pretrain_routine start</p></li>
<li><p>pretrain_routine end</p></li>
<li><p>training_start</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_save_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by Lightning when saving a checkpoint to give you a chance to store anything
else you might want to save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – The full checkpoint dictionary before it gets dumped to a file.
Implementations of this hook can insert additional data into this dictionary.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of use cases you don&#39;t need to implement this method</span>
    <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">my_cool_pickable_object</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning saves all aspects of training (epoch, global step, etc…)
including amp scaling.
There is no need for you to store anything about training.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of test_step_end(test_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_batch_start">
<span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_dataloader">
<span class="sig-name descname"><span class="pre">on_test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the test dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_end">
<span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_epoch_start">
<span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_model_eval">
<span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_model_train">
<span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the test loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_start">
<span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_test_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – The outputs of training_step_end(training_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_batch_start">
<span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unused</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>unused</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – Deprecated argument. Will be removed in v1.7.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_dataloader">
<span class="sig-name descname"><span class="pre">on_train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the train dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_train_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, either:</p>
<ol class="arabic simple">
<li><p>Implement <cite>training_epoch_end</cite> in the LightningModule OR</p></li>
<li><p>Cache data across steps on the attribute(s) of the <cite>LightningModule</cite> and access them in this hook</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_start">
<span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_train_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_val_dataloader">
<span class="sig-name descname"><span class="pre">on_val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Called before requesting the val dataloader.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_val_dataloader()</span></code></a> is deprecated and will be removed in v1.7.0.
Please use <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a> directly.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_batch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – The outputs of validation_step_end(validation_step(x))</p></li>
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_batch_start">
<span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_batch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – the index of the dataloader</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_epoch_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_model_eval">
<span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_model_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to eval during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_model_train">
<span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_model_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the model to train during the val loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.on_validation_start" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_step">
<span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_tpu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_native_amp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">using_lbfgs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to adjust the default way the
<code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls each optimizer.
By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example
once per optimizer. This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) won’t be called during the
accumulation phase when <code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers, this indexes into that list.</p></li>
<li><p><strong>optimizer_closure</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[], <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]) – Closure for all optimizers. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
<li><p><strong>on_tpu</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if TPU backward is required</p></li>
<li><p><strong>using_native_amp</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – <code class="docutils literal notranslate"><span class="pre">True</span></code> if using native amp</p></li>
<li><p><strong>using_lbfgs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – True if the matching optimizer is <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.LBFGS</span></code></p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

<span class="c1"># Alternating schedule for optimizer steps (i.e.: GANs)</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">,</span>
                   <span class="n">optimizer_closure</span><span class="p">,</span> <span class="n">on_tpu</span><span class="p">,</span> <span class="n">using_native_amp</span><span class="p">,</span> <span class="n">using_lbfgs</span><span class="p">):</span>
    <span class="c1"># update generator opt every step</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># update discriminator opt every 2 steps</span>
    <span class="k">if</span> <span class="n">optimizer_idx</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># call the closure by itself to run `training_step` + `backward` without an optimizer step</span>
            <span class="n">optimizer_closure</span><span class="p">()</span>

    <span class="c1"># ...</span>
    <span class="c1"># add as many optimizers as you want</span>
</pre></div>
</div>
<p>Here’s another example showing how to use this for more advanced things such as
learning rate warm-up:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># learning rate warm-up</span>
<span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">epoch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">optimizer_idx</span><span class="p">,</span>
    <span class="n">optimizer_closure</span><span class="p">,</span>
    <span class="n">on_tpu</span><span class="p">,</span>
    <span class="n">using_native_amp</span><span class="p">,</span>
    <span class="n">using_lbfgs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># warm up lr</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="mi">500</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span>

    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_zero_grad">
<span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizer_zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Current epoch</p></li>
<li><p><strong>batch_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – Index of current batch</p></li>
<li><p><strong>optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>) – A PyTorch optimizer</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – If you used multiple optimizers this indexes into that list.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance.</span>
<span class="k">def</span> <span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_idx</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for the explanation of the above example.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizers">
<span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_pl_optimizer</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision and
profiling.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an iterator over module parameters.</p>
<p>This is typically passed to an optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<em>bool</em>) – if True, then yields parameters of this module
and all submodules. Otherwise, yields only parameters that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><em>Parameter</em> – module parameter</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">param</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Iterator</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.precision">
<span class="sig-name descname"><span class="pre">precision</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.precision" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_batch_size">
<span class="sig-name descname"><span class="pre">pred_batch_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_batch_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_n">
<span class="sig-name descname"><span class="pre">pred_n</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_n" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_n_jobs">
<span class="sig-name descname"><span class="pre">pred_n_jobs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_n_jobs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_num_samples">
<span class="sig-name descname"><span class="pre">pred_num_samples</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_num_samples" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_roll_size">
<span class="sig-name descname"><span class="pre">pred_roll_size</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.pred_roll_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for prediction.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple prediction dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.predict_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the prediction step</p>
<dl class="simple">
<dt>batch</dt><dd><p>output of Darts’ <code class="xref py py-class docutils literal notranslate"><span class="pre">InferenceDataset</span></code> - tuple of <code class="docutils literal notranslate"><span class="pre">(past_target,</span> <span class="pre">past_covariates,</span>
<span class="pre">historic_future_covariates,</span> <span class="pre">future_covariates,</span> <span class="pre">future_past_covariates,</span> <span class="pre">input_timeseries)</span></code></p>
</dd>
<dt>batch_idx</dt><dd><p>the batch index of the current batch</p>
</dd>
<dt>dataloader_idx</dt><dd><p>the dataloader index</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<a class="reference internal" href="darts.timeseries.html#darts.timeseries.TimeSeries" title="darts.timeseries.TimeSeries"><code class="xref py py-class docutils literal notranslate"><span class="pre">TimeSeries</span></code></a>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this to download and prepare data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <cite>setup</cite> instead)
since this is NOT called on every GPU in DDP/TPU</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> with the trainer flag is deprecated and will be removed in v1.7.0.
Please set <code class="docutils literal notranslate"><span class="pre">prepare_data_per_node</span></code> in LightningDataModule or LightningModule directly instead.</p>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data_per_node">
<span class="sig-name descname"><span class="pre">prepare_data_per_node</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data_per_node" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.print">
<span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.print" title="Permalink to this definition">¶</a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – The thing to print. The same as for Python’s built-in print function.</p></li>
<li><p><strong>**kwargs</strong> – The same as for Python’s built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>This function is deprecated in favor of <code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code> and
the behavior of this function will change in future versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_buffer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a buffer to the module.</p>
<p>This is typically used to register a buffer that should not to be
considered a model parameter. For example, BatchNorm’s <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the module’s state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em><em> or </em><em>None</em>) – buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations
that run on buffers, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
<li><p><strong>persistent</strong> (<em>bool</em>) – whether the buffer is part of this module’s
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_forward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the output. It can modify the input inplace but
it will not have effect on forward since this is called after
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_forward_pre_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.
It should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>The input contains only the positional arguments given to the module.
Keyword arguments won’t be passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.
The hook can modify the input. User can either return a tuple or a
single modified value in the hook. We will wrap the value into a tuple
if a single value is returned(unless that value is already a tuple).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_full_backward_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Registers a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to module
inputs are computed. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Module’s forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_module" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.register_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em><em> or </em><em>None</em>) – parameter to be added to the module. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cuda" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
module’s <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.requires_grad_" title="Permalink to this definition">¶</a></dt>
<dd><p>Change if autograd should record operations on parameters in this
module.</p>
<p>This method sets the parameters’ <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <span class="xref std std-ref">locally-disable-grad-doc</span> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<em>bool</em>) – whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters">
<span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.save_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><strong>ignore</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><strong>frame</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">frame</span></code>]) – a frame object. Default is None</p></li>
<li><p><strong>logger</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code>) – Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_extra_state" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is called from <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_extra_state" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>state</strong> (<em>dict</em>) – Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_predict_parameters">
<span class="sig-name descname"><span class="pre">set_predict_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roll_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.set_predict_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>to be set from TorchForecastingModel before calling trainer.predict() and reset at self.on_predict_end()</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when
you need to build models dynamically or adjust something about them. This hook is called on every process
when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.share_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>~T</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.summarize">
<span class="sig-name descname"><span class="pre">summarize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'top'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.summarize" title="Permalink to this definition">¶</a></dt>
<dd><p>Summarize this LightningModule.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.5: </span>This method was deprecated in v1.5 in favor of <cite>pytorch_lightning.utilities.model_summary.summarize</cite>
and will be removed in v1.7.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – <p>Can be either <code class="docutils literal notranslate"><span class="pre">'top'</span></code> (summarize only direct submodules) or <code class="docutils literal notranslate"><span class="pre">'full'</span></code> (summarize all layers).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version v1.4: </span>This parameter was deprecated in v1.4 in favor of <cite>max_depth</cite> and will be removed in v1.6.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]) – The maximum depth of layer nesting that the summary will include. A value of 0 turns the
layer summary off. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ModelSummary</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The model summary object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.tbptt_split_batch">
<span class="sig-name descname"><span class="pre">tbptt_split_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.tbptt_split_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>When using truncated backpropagation through time, each batch must be split along the
time dimension. Lightning handles this by default, but for custom behavior override
this function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Current batch</p></li>
<li><p><strong>split_size</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The size of the split</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List of batch splits. Each split will be passed to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> to enable truncated
back propagation through time. The default implementation splits root level Tensors and
Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tbptt_split_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">split_size</span><span class="p">):</span>
    <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">time_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">split_size</span><span class="p">):</span>
        <span class="n">batch_split</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">collections</span><span class="o">.</span><span class="n">Sequence</span><span class="p">):</span>
                <span class="n">split_x</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">batch_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                  <span class="n">split_x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">][</span><span class="n">t</span><span class="p">:</span><span class="n">t</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">]</span>
            <span class="n">batch_split</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">split_x</span><span class="p">)</span>
        <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_split</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">splits</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Called in the training loop after
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_batch_start()</span></code>
if <a href="#id53"><span class="problematic" id="id54">:paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps`</span></a> &gt; 0.
Each returned batch split is passed separately to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.teardown">
<span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.teardown" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, predict, or tune.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id55"><span class="problematic" id="id56">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a postive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying testing samples.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_epoch_end">
<span class="sig-name descname"><span class="pre">test_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of a test epoch with the output of all test steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step_end" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step_end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step_end()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="c1"># do something with the outputs of all test batches</span>
    <span class="n">all_test_preds</span> <span class="o">=</span> <span class="n">test_step_outputs</span><span class="o">.</span><span class="n">predictions</span>

    <span class="n">some_result</span> <span class="o">=</span> <span class="n">calc_all_results</span><span class="p">(</span><span class="n">all_test_preds</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">some_result</span><span class="p">)</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each test step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="n">final_value</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">dataloader_outputs</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">test_step_out</span> <span class="ow">in</span> <span class="n">dataloader_outputs</span><span class="p">:</span>
            <span class="c1"># do something</span>
            <span class="n">final_value</span> <span class="o">+=</span> <span class="n">test_step_out</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test dataloaders used).</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="o">...</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to test you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step_end">
<span class="sig-name descname"><span class="pre">test_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when testing with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will operate on only part of the batch.
However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">test_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT test_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with test_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">test_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_results</span><span class="p">):</span>
    <span class="c1"># this out is now the full size of the batch</span>
    <span class="n">all_test_step_outs</span> <span class="o">=</span> <span class="n">output_results</span><span class="o">.</span><span class="n">out</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">all_test_step_outs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a> s. In addition, this method will
only cast the floating point parameters and buffers to <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.dtype" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.dtype"><code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code></a>
(if given). The integral parameters and buffers will be moved
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.device" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ExampleModule</span><span class="p">(</span><span class="n">DeviceDtypeModuleMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">ExampleModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float16</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves the parameters and buffers to the specified device without copying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code>) – The desired device of the parameters
and buffers in this module.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>]) – The path of the file the onnx model should be saved to.</p></li>
<li><p><strong>input_sample</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><strong>**kwargs</strong> – Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">NamedTemporaryFile</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="s1">&#39;.onnx&#39;</span><span class="p">,</span> <span class="n">delete</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">tmpfile</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_torchscript">
<span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.to_torchscript" title="Permalink to this definition">¶</a></dt>
<dd><p>By default compiles the whole model to a <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a> set. If you would like to customize the modules that
are scripted you should override this method. In case you want to return multiple modules, we recommend
using a dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]) – Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><strong>method</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]) – Whether to use TorchScript’s script or trace method. Default: ‘script’</p></li>
<li><p><strong>example_inputs</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]) – An input to be used to do tracing when method is set to ‘trace’.
Default: None (uses <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.example_input_array" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.example_input_array"><code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code></a>)</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments that will be passed to the <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code>
documentation for supported features.</p></li>
</ul>
</div>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(),</span> <span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> 
<span class="gp">... </span>                                    <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)))</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">)</span>  
<span class="go">True</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.toggle_optimizer">
<span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.toggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes sure only the gradients of the current optimizer’s parameters are calculated in the training step
to prevent dangling gradients in multiple-optimizer setup.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.
It works with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.untoggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code>]) – The optimizer to toggle.</p></li>
<li><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to toggle.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the module in training mode.</p>
<p>This has any effect only on certain modules. See documentations of
particular modules for details of their behaviors in training/evaluation
mode, if they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>bool</em>) – whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]], <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A collection of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> specifying training samples.
In the case of multiple dataloaders, please see this <span class="xref std std-ref">page</span>.</p>
</dd>
</dl>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id57"><span class="problematic" id="id58">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_epoch_end">
<span class="sig-name descname"><span class="pre">training_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the training epoch with the outputs of all training steps. Use this in case you
need to do something with all the outputs returned by <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">train_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_batch</span> <span class="ow">in</span> <span class="n">train_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">train_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">training_epoch_end</span><span class="p">(</span><span class="n">train_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>.
If there are multiple optimizers, it is a list containing a list of outputs for each optimizer.
If using <code class="docutils literal notranslate"><span class="pre">truncated_bptt_steps</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, each element is a list of outputs corresponding to the outputs
of each processed split batch.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If this method is not overridden, this won’t be called.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="c1"># do something with all training_step outputs</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">training_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the training step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step_end">
<span class="sig-name descname"><span class="pre">training_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when training with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will operate on only part of the
batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">training_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">training_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <cite>training_step</cite> for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Anything</p>
</dd>
</dl>
<p>When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># softmax uses only a portion of the batch in the denominator</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>If you wish to do something with all the parts of the batch, then use this method to do it:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;pred&quot;</span><span class="p">:</span> <span class="n">out</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">training_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_step_outputs</span><span class="p">):</span>
    <span class="n">gpu_0_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_1_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>
    <span class="n">gpu_n_pred</span> <span class="o">=</span> <span class="n">training_step_outputs</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="s2">&quot;pred&quot;</span><span class="p">]</span>

    <span class="c1"># this softmax now uses the full batch</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">([</span><span class="n">gpu_0_pred</span><span class="p">,</span> <span class="n">gpu_1_pred</span><span class="p">,</span> <span class="n">gpu_n_pred</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.transfer_batch_to_device">
<span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.transfer_batch_to_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Override this hook if your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> returns tensors wrapped in a custom
data structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> or anything that implements <cite>.to(…)</cite></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torchtext.data.batch.Batch</span></code></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, …).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook only runs on single GPU training and DDP (no data-parallel).
Data-Parallel support will come in near future.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – A batch of data that needs to be transferred to a new device.</p></li>
<li><p><strong>device</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">device</span></code>) – The target device as defined in PyTorch.</p></li>
<li><p><strong>dataloader_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><strong>MisconfigurationException</strong> – If using data-parallel, <code class="docutils literal notranslate"><span class="pre">Trainer(strategy='dp')</span></code>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.truncated_bptt_steps">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">truncated_bptt_steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.truncated_bptt_steps" title="Permalink to this definition">¶</a></dt>
<dd><p>Enables <cite>Truncated Backpropagation Through Time</cite> in the Trainer when set to a positive integer.</p>
<p>It represents
the number of times <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> gets called before backpropagation. If this is &gt; 0, the
<a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> receives an additional argument <code class="docutils literal notranslate"><span class="pre">hiddens</span></code> and is expected to return a hidden state.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.type" title="Permalink to this definition">¶</a></dt>
<dd><p>Casts all parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dst_type</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<em>type</em><em> or </em><em>string</em>) – the desired type</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.unfreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.untoggle_optimizer">
<span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.untoggle_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.toggle_optimizer" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p>
<p>This is only called automatically when automatic optimization is enabled and multiple optimizers are used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>optimizer_idx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>) – The index of the optimizer to untoggle.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.use_amp">
<span class="sig-name descname"><span class="pre">use_amp</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.use_amp" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id59"><span class="problematic" id="id60">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Sequence</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>]]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> or a sequence of them specifying validation samples.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_epoch_end">
<span class="sig-name descname"><span class="pre">validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_epoch_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the end of the validation epoch with the outputs of all validation steps.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">val_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>outputs</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>]]]) – List of outputs you defined in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, or if there
are multiple dataloaders, a list containing a list of outputs for each dataloader.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you didn’t define a <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, this won’t be called.</p>
</div>
<p class="rubric">Examples</p>
<p>With a single dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>With multiple dataloaders, <cite>outputs</cite> will be a list of lists. The outer list contains
one entry per dataloader, while the inner list contains the individual outputs of
each validation step for that dataloader.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">dataloader_output_result</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
        <span class="n">dataloader_outs</span> <span class="o">=</span> <span class="n">dataloader_output_result</span><span class="o">.</span><span class="n">dataloader_i_outputs</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;final_metric&quot;</span><span class="p">,</span> <span class="n">final_value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val_batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>performs the validation step</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step_end">
<span class="sig-name descname"><span class="pre">validation_step_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step_end" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this when validating with dp or ddp2 because <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will operate on only part of
the batch. However, this is still optional and only needed for things like softmax or NCE loss.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you later switch to ddp or some other mode, this will still be called
so that you don’t have to change your code.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode</span>
<span class="n">sub_batches</span> <span class="o">=</span> <span class="n">split_batches_for_dp</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="n">batch_parts_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">validation_step</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span> <span class="k">for</span> <span class="n">sub_batch</span> <span class="ow">in</span> <span class="n">sub_batches</span><span class="p">]</span>
<span class="n">validation_step_end</span><span class="p">(</span><span class="n">batch_parts_outputs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch_parts_outputs</strong> – What you return in <a class="reference internal" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step" title="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>
for each batch part.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>], <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>None or anything</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># WITHOUT validation_step_end</span>
<span class="c1"># if used in DP or DDP2, this batch is 1/num_gpus large</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="c1"># --------------</span>
<span class="c1"># with validation_step_end to do softmax over the full batch</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="c1"># batch is 1/num_gpus big</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">validation_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_step_outputs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">val_step_outputs</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>See the <span class="xref std std-ref">advanced/multi_gpu:Multi-GPU training</span> guide for more details.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.xpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<em>int</em><em>, </em><em>optional</em>) – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#darts.models.forecasting.pl_forecasting_module.PLSplitCovariatesModule.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets gradients of all model parameters to zero. See similar function
under <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> for more context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
See <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code> for details.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>



              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="darts.models.forecasting.nbeats.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">N-BEATS</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="darts.models.forecasting.prophet_model.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Facebook Prophet</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020 - 2022, Unit8 SA (Apache 2.0 License).<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.3.2.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>